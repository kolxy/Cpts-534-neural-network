{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7M-nFzShya7"
      },
      "source": [
        "<h1><center> \n",
        "    Neural network design and application\n",
        "</center></h1>\n",
        "\n",
        "<h2><center>CPT_S 434/534, 2022 Spring</center></h2>\n",
        "\n",
        "<h2><center>HW 5: GNNs (100 pts)</center></h2>\n",
        "\n",
        "### Name: *Xinyu Liu*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYACAtOgP16l"
      },
      "source": [
        "## This assignment includes:\n",
        "\n",
        "## Coding in Python (PyTorch) and answering questions: train GNNs (100 points) by [pygcn](https://github.com/tkipf/pygcn)\n",
        "\n",
        "Step 0: Install and configure: python ([Anaconda platform](https://docs.anaconda.com/anaconda/install/) recommended), [Jupyter Notebook](https://jupyter.org/install) and [pytorch](https://pytorch.org/get-started/) \n",
        "\n",
        "**Remark 1.** [Colab](https://colab.research.google.com) is a cloud platform that enables your Jupyter Notebooks (including this .ipynb assignment) to run with different runtime types (hardware acceleration is possible using GPU or TPU). You may also choose Colab to finish assignments (future and this assignments may require extensive computation that may be time-consuming on your laptop). \n",
        "\n",
        "**Remark 2.** If you use Colab, it is still required to convert your .ipynb to .html and submit **BOTH** files to Canvas. See [this page](https://stackoverflow.com/questions/53460051/convert-ipynb-notebook-to-html-in-google-colab) on how to convert to .html\n",
        "\n",
        "Step 1: Read provided code (with git) to download the [pygcn](https://github.com/tkipf/pygcn) from Github\n",
        "\n",
        "Step 2: Read the manual at [pygcn](https://github.com/tkipf/pygcn) and follow the instruction to install the package and train GNNs\n",
        "\n",
        "Step 3: Record and display (show text, no need to plot figures) results to show performance measure convergence (against #epoch)\n",
        "\n",
        "## Submission:\n",
        "\n",
        "* Convert the .ipynb file to .html file (**save the execution outputs** to show your progress: otherwise grading may be affected)\n",
        "    \n",
        "* Upload **both** your .ipynb and .html files to Canvas (**NOT a .ZIP file**)\n",
        "\n",
        "* Deadline: April 17, 11:59 PM, Pacific time.\n",
        "\n",
        "<!-- * Plots should be clear and easy to read. -->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmTrblD8P16m"
      },
      "source": [
        "## Reference\n",
        "\n",
        "* [os.chdir()](https://www.tutorialspoint.com/python/os_chdir.htm): change current working directory in Python\n",
        "\n",
        "* [os.system()](https://docs.python.org/3/library/os.html#os.system): execute terminal commands in Python\n",
        "\n",
        "* In the case the terminal command does not automatically save the execute output (for example, if you run this .ipynb on Colab, then it may be this case): see [how to save terminal command output to a file](https://askubuntu.com/questions/420981/how-do-i-save-terminal-output-to-a-file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRx33wz4P16n"
      },
      "source": [
        "# 1. (Read/run/implement the code, 80 pts) Graph convolutional networks by [pygcn](https://github.com/tkipf/pygcn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOBU3apWP16n",
        "outputId": "94b58118-3d4d-4abc-8ad0-796e1897ad64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'pygcn'...\n",
            "remote: Enumerating objects: 78, done.\u001b[K\n",
            "remote: Total 78 (delta 0), reused 0 (delta 0), pack-reused 78\u001b[K\n",
            "Unpacking objects: 100% (78/78), done.\n"
          ]
        }
      ],
      "source": [
        "# if the code starts with \"!\", it tells jupyter-lab to execute terminal commands\n",
        "# here we use \"git\" to download the code from Github\n",
        "!pwd\n",
        "!git clone https://github.com/tkipf/pygcn.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpT0L2ZpP16o",
        "outputId": "9e3e5846-f882-4f48-b6be-25a0c1a881ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "pygcn  sample_data\n"
          ]
        }
      ],
      "source": [
        "!pwd # display the current working directory in absolute path\n",
        "!ls  # display all contents under this current working directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bWxNEMgP16p",
        "outputId": "e6889216-0ec9-4236-a05d-75409d1be1ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.chdir('pygcn')  # use os.chdir to change the current working directory\n",
        "\n",
        "os.system('pwd')   # use terminal command \"pwd\" to display the absolute path to the current working directory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWI95hi8P16p"
      },
      "source": [
        "## 1.1 (To finish, 15 pts) Install the package following [pygcn@installation](https://github.com/tkipf/pygcn#installation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSSiif-z_g_p",
        "outputId": "2e683e0b-3a01-43a3-d243-ce783be87eec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 100\n",
            "drwxr-xr-x 8 root root  4096 Apr 11 00:20 .\n",
            "drwxr-xr-x 1 root root  4096 Apr 11 00:19 ..\n",
            "drwxr-xr-x 4 root root  4096 Apr 11 00:20 build\n",
            "drwxr-xr-x 3 root root  4096 Apr 11 00:19 data\n",
            "drwxr-xr-x 2 root root  4096 Apr 11 00:20 dist\n",
            "-rw-r--r-- 1 root root 49982 Apr 11 00:19 figure.png\n",
            "drwxr-xr-x 8 root root  4096 Apr 11 00:19 .git\n",
            "-rw-r--r-- 1 root root     7 Apr 11 00:19 .gitignore\n",
            "-rw-r--r-- 1 root root  1071 Apr 11 00:19 LICENCE\n",
            "drwxr-xr-x 2 root root  4096 Apr 11 00:19 pygcn\n",
            "drwxr-xr-x 2 root root  4096 Apr 11 00:20 pygcn.egg-info\n",
            "-rw-r--r-- 1 root root  1366 Apr 11 00:19 README.md\n",
            "-rw-r--r-- 1 root root   553 Apr 11 00:19 setup.py\n"
          ]
        }
      ],
      "source": [
        "# Check content\n",
        "! ls -la"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bH_5IHFFP16q",
        "outputId": "b017c403-1db3-45bb-d7a4-f4ed8b4b0fd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating pygcn.egg-info\n",
            "writing pygcn.egg-info/PKG-INFO\n",
            "writing dependency_links to pygcn.egg-info/dependency_links.txt\n",
            "writing requirements to pygcn.egg-info/requires.txt\n",
            "writing top-level names to pygcn.egg-info/top_level.txt\n",
            "writing manifest file 'pygcn.egg-info/SOURCES.txt'\n",
            "adding license file 'LICENCE'\n",
            "writing manifest file 'pygcn.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/pygcn\n",
            "copying pygcn/utils.py -> build/lib/pygcn\n",
            "copying pygcn/models.py -> build/lib/pygcn\n",
            "copying pygcn/layers.py -> build/lib/pygcn\n",
            "copying pygcn/train.py -> build/lib/pygcn\n",
            "copying pygcn/__init__.py -> build/lib/pygcn\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/pygcn\n",
            "copying build/lib/pygcn/utils.py -> build/bdist.linux-x86_64/egg/pygcn\n",
            "copying build/lib/pygcn/models.py -> build/bdist.linux-x86_64/egg/pygcn\n",
            "copying build/lib/pygcn/layers.py -> build/bdist.linux-x86_64/egg/pygcn\n",
            "copying build/lib/pygcn/train.py -> build/bdist.linux-x86_64/egg/pygcn\n",
            "copying build/lib/pygcn/__init__.py -> build/bdist.linux-x86_64/egg/pygcn\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pygcn/utils.py to utils.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pygcn/models.py to models.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pygcn/layers.py to layers.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pygcn/train.py to train.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pygcn/__init__.py to __init__.cpython-37.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying pygcn.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying pygcn.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying pygcn.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying pygcn.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying pygcn.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating dist\n",
            "creating 'dist/pygcn-0.1-py3.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing pygcn-0.1-py3.7.egg\n",
            "Copying pygcn-0.1-py3.7.egg to /usr/local/lib/python3.7/dist-packages\n",
            "Adding pygcn 0.1 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.7/dist-packages/pygcn-0.1-py3.7.egg\n",
            "Processing dependencies for pygcn==0.1\n",
            "Searching for scipy==1.4.1\n",
            "Best match: scipy 1.4.1\n",
            "Adding scipy 1.4.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for torch==1.10.0+cu111\n",
            "Best match: torch 1.10.0+cu111\n",
            "Adding torch 1.10.0+cu111 to easy-install.pth file\n",
            "Installing convert-caffe2-to-onnx script to /usr/local/bin\n",
            "Installing convert-onnx-to-caffe2 script to /usr/local/bin\n",
            "Installing torchrun script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for numpy==1.21.5\n",
            "Best match: numpy 1.21.5\n",
            "Adding numpy 1.21.5 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.7 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for typing-extensions==3.10.0.2\n",
            "Best match: typing-extensions 3.10.0.2\n",
            "Adding typing-extensions 3.10.0.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Finished processing dependencies for pygcn==0.1\n"
          ]
        }
      ],
      "source": [
        "# Your code goes here\n",
        "# Hint: see how to install the package at https://github.com/tkipf/pygcn#installation\n",
        "\n",
        "! python setup.py install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCTj1qzGP16q"
      },
      "source": [
        "## 1.2 (To finish, 15 pts) Run the training code following [pygcn@usage](https://github.com/tkipf/pygcn#usage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owz2EoZAP16r",
        "outputId": "f3128f90-8785-406e-8b37-f80bea9af610"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading cora dataset...\n",
            "Epoch: 0001 loss_train: 1.9233 acc_train: 0.1571 loss_val: 1.9285 acc_val: 0.1567 time: 0.0802s\n",
            "Epoch: 0002 loss_train: 1.9238 acc_train: 0.1929 loss_val: 1.9176 acc_val: 0.1567 time: 0.0051s\n",
            "Epoch: 0003 loss_train: 1.9023 acc_train: 0.2000 loss_val: 1.9072 acc_val: 0.1567 time: 0.0047s\n",
            "Epoch: 0004 loss_train: 1.8900 acc_train: 0.2000 loss_val: 1.8970 acc_val: 0.1567 time: 0.0047s\n",
            "Epoch: 0005 loss_train: 1.8744 acc_train: 0.2071 loss_val: 1.8872 acc_val: 0.1567 time: 0.0047s\n",
            "Epoch: 0006 loss_train: 1.8840 acc_train: 0.2000 loss_val: 1.8777 acc_val: 0.1567 time: 0.0052s\n",
            "Epoch: 0007 loss_train: 1.8659 acc_train: 0.2000 loss_val: 1.8682 acc_val: 0.1567 time: 0.0070s\n",
            "Epoch: 0008 loss_train: 1.8566 acc_train: 0.2143 loss_val: 1.8587 acc_val: 0.1567 time: 0.0068s\n",
            "Epoch: 0009 loss_train: 1.8466 acc_train: 0.2143 loss_val: 1.8493 acc_val: 0.1567 time: 0.0057s\n",
            "Epoch: 0010 loss_train: 1.8396 acc_train: 0.2357 loss_val: 1.8400 acc_val: 0.1600 time: 0.0049s\n",
            "Epoch: 0011 loss_train: 1.8222 acc_train: 0.2357 loss_val: 1.8308 acc_val: 0.2500 time: 0.0049s\n",
            "Epoch: 0012 loss_train: 1.8148 acc_train: 0.2929 loss_val: 1.8216 acc_val: 0.4133 time: 0.0047s\n",
            "Epoch: 0013 loss_train: 1.8104 acc_train: 0.3357 loss_val: 1.8126 acc_val: 0.4667 time: 0.0048s\n",
            "Epoch: 0014 loss_train: 1.8032 acc_train: 0.3500 loss_val: 1.8037 acc_val: 0.4467 time: 0.0047s\n",
            "Epoch: 0015 loss_train: 1.7938 acc_train: 0.3429 loss_val: 1.7950 acc_val: 0.3800 time: 0.0048s\n",
            "Epoch: 0016 loss_train: 1.7688 acc_train: 0.3286 loss_val: 1.7864 acc_val: 0.3633 time: 0.0048s\n",
            "Epoch: 0017 loss_train: 1.7689 acc_train: 0.3643 loss_val: 1.7780 acc_val: 0.3533 time: 0.0048s\n",
            "Epoch: 0018 loss_train: 1.7528 acc_train: 0.3643 loss_val: 1.7698 acc_val: 0.3533 time: 0.0048s\n",
            "Epoch: 0019 loss_train: 1.7437 acc_train: 0.3357 loss_val: 1.7619 acc_val: 0.3500 time: 0.0049s\n",
            "Epoch: 0020 loss_train: 1.7465 acc_train: 0.3143 loss_val: 1.7544 acc_val: 0.3500 time: 0.0048s\n",
            "Epoch: 0021 loss_train: 1.7630 acc_train: 0.3286 loss_val: 1.7474 acc_val: 0.3500 time: 0.0047s\n",
            "Epoch: 0022 loss_train: 1.7434 acc_train: 0.3214 loss_val: 1.7408 acc_val: 0.3500 time: 0.0048s\n",
            "Epoch: 0023 loss_train: 1.7304 acc_train: 0.3143 loss_val: 1.7346 acc_val: 0.3500 time: 0.0049s\n",
            "Epoch: 0024 loss_train: 1.7240 acc_train: 0.3000 loss_val: 1.7287 acc_val: 0.3500 time: 0.0049s\n",
            "Epoch: 0025 loss_train: 1.7163 acc_train: 0.3357 loss_val: 1.7230 acc_val: 0.3500 time: 0.0047s\n",
            "Epoch: 0026 loss_train: 1.7039 acc_train: 0.3143 loss_val: 1.7174 acc_val: 0.3500 time: 0.0048s\n",
            "Epoch: 0027 loss_train: 1.7147 acc_train: 0.3071 loss_val: 1.7119 acc_val: 0.3500 time: 0.0048s\n",
            "Epoch: 0028 loss_train: 1.6970 acc_train: 0.3071 loss_val: 1.7065 acc_val: 0.3500 time: 0.0057s\n",
            "Epoch: 0029 loss_train: 1.6923 acc_train: 0.3214 loss_val: 1.7011 acc_val: 0.3500 time: 0.0051s\n",
            "Epoch: 0030 loss_train: 1.6661 acc_train: 0.3429 loss_val: 1.6954 acc_val: 0.3500 time: 0.0048s\n",
            "Epoch: 0031 loss_train: 1.6926 acc_train: 0.3429 loss_val: 1.6897 acc_val: 0.3533 time: 0.0048s\n",
            "Epoch: 0032 loss_train: 1.6527 acc_train: 0.3214 loss_val: 1.6840 acc_val: 0.3533 time: 0.0048s\n",
            "Epoch: 0033 loss_train: 1.6456 acc_train: 0.3571 loss_val: 1.6781 acc_val: 0.3533 time: 0.0046s\n",
            "Epoch: 0034 loss_train: 1.6606 acc_train: 0.3357 loss_val: 1.6718 acc_val: 0.3600 time: 0.0045s\n",
            "Epoch: 0035 loss_train: 1.6349 acc_train: 0.3786 loss_val: 1.6652 acc_val: 0.3667 time: 0.0045s\n",
            "Epoch: 0036 loss_train: 1.6621 acc_train: 0.3571 loss_val: 1.6583 acc_val: 0.3667 time: 0.0045s\n",
            "Epoch: 0037 loss_train: 1.6334 acc_train: 0.3786 loss_val: 1.6512 acc_val: 0.3667 time: 0.0047s\n",
            "Epoch: 0038 loss_train: 1.6138 acc_train: 0.4214 loss_val: 1.6435 acc_val: 0.3667 time: 0.0049s\n",
            "Epoch: 0039 loss_train: 1.5913 acc_train: 0.4143 loss_val: 1.6354 acc_val: 0.3733 time: 0.0048s\n",
            "Epoch: 0040 loss_train: 1.5600 acc_train: 0.4214 loss_val: 1.6269 acc_val: 0.3767 time: 0.0047s\n",
            "Epoch: 0041 loss_train: 1.5739 acc_train: 0.4357 loss_val: 1.6180 acc_val: 0.3833 time: 0.0046s\n",
            "Epoch: 0042 loss_train: 1.5608 acc_train: 0.4214 loss_val: 1.6090 acc_val: 0.3867 time: 0.0046s\n",
            "Epoch: 0043 loss_train: 1.5663 acc_train: 0.4071 loss_val: 1.5999 acc_val: 0.3933 time: 0.0046s\n",
            "Epoch: 0044 loss_train: 1.5306 acc_train: 0.4286 loss_val: 1.5905 acc_val: 0.3967 time: 0.0048s\n",
            "Epoch: 0045 loss_train: 1.5059 acc_train: 0.4643 loss_val: 1.5810 acc_val: 0.4100 time: 0.0047s\n",
            "Epoch: 0046 loss_train: 1.4910 acc_train: 0.4286 loss_val: 1.5711 acc_val: 0.4100 time: 0.0047s\n",
            "Epoch: 0047 loss_train: 1.4629 acc_train: 0.4571 loss_val: 1.5608 acc_val: 0.4200 time: 0.0047s\n",
            "Epoch: 0048 loss_train: 1.4740 acc_train: 0.4929 loss_val: 1.5504 acc_val: 0.4200 time: 0.0047s\n",
            "Epoch: 0049 loss_train: 1.4388 acc_train: 0.4857 loss_val: 1.5398 acc_val: 0.4300 time: 0.0046s\n",
            "Epoch: 0050 loss_train: 1.4784 acc_train: 0.4643 loss_val: 1.5292 acc_val: 0.4333 time: 0.0081s\n",
            "Epoch: 0051 loss_train: 1.4348 acc_train: 0.5357 loss_val: 1.5183 acc_val: 0.4533 time: 0.0047s\n",
            "Epoch: 0052 loss_train: 1.4304 acc_train: 0.5143 loss_val: 1.5070 acc_val: 0.4733 time: 0.0047s\n",
            "Epoch: 0053 loss_train: 1.4118 acc_train: 0.5286 loss_val: 1.4952 acc_val: 0.4867 time: 0.0049s\n",
            "Epoch: 0054 loss_train: 1.4036 acc_train: 0.5500 loss_val: 1.4829 acc_val: 0.4967 time: 0.0048s\n",
            "Epoch: 0055 loss_train: 1.3807 acc_train: 0.5357 loss_val: 1.4702 acc_val: 0.5133 time: 0.0047s\n",
            "Epoch: 0056 loss_train: 1.3806 acc_train: 0.5143 loss_val: 1.4573 acc_val: 0.5267 time: 0.0048s\n",
            "Epoch: 0057 loss_train: 1.3447 acc_train: 0.5000 loss_val: 1.4446 acc_val: 0.5400 time: 0.0051s\n",
            "Epoch: 0058 loss_train: 1.3063 acc_train: 0.5786 loss_val: 1.4318 acc_val: 0.5433 time: 0.0046s\n",
            "Epoch: 0059 loss_train: 1.3037 acc_train: 0.5500 loss_val: 1.4192 acc_val: 0.5600 time: 0.0047s\n",
            "Epoch: 0060 loss_train: 1.3010 acc_train: 0.5929 loss_val: 1.4068 acc_val: 0.5633 time: 0.0046s\n",
            "Epoch: 0061 loss_train: 1.3099 acc_train: 0.5500 loss_val: 1.3946 acc_val: 0.5767 time: 0.0046s\n",
            "Epoch: 0062 loss_train: 1.2624 acc_train: 0.6000 loss_val: 1.3825 acc_val: 0.5967 time: 0.0045s\n",
            "Epoch: 0063 loss_train: 1.2578 acc_train: 0.6357 loss_val: 1.3701 acc_val: 0.6067 time: 0.0046s\n",
            "Epoch: 0064 loss_train: 1.2660 acc_train: 0.6000 loss_val: 1.3576 acc_val: 0.6167 time: 0.0046s\n",
            "Epoch: 0065 loss_train: 1.2323 acc_train: 0.5929 loss_val: 1.3451 acc_val: 0.6233 time: 0.0048s\n",
            "Epoch: 0066 loss_train: 1.1852 acc_train: 0.6357 loss_val: 1.3327 acc_val: 0.6300 time: 0.0046s\n",
            "Epoch: 0067 loss_train: 1.1767 acc_train: 0.6214 loss_val: 1.3205 acc_val: 0.6333 time: 0.0045s\n",
            "Epoch: 0068 loss_train: 1.1761 acc_train: 0.6571 loss_val: 1.3084 acc_val: 0.6333 time: 0.0047s\n",
            "Epoch: 0069 loss_train: 1.1684 acc_train: 0.6929 loss_val: 1.2967 acc_val: 0.6367 time: 0.0046s\n",
            "Epoch: 0070 loss_train: 1.1591 acc_train: 0.6500 loss_val: 1.2849 acc_val: 0.6467 time: 0.0050s\n",
            "Epoch: 0071 loss_train: 1.1564 acc_train: 0.6714 loss_val: 1.2734 acc_val: 0.6500 time: 0.0045s\n",
            "Epoch: 0072 loss_train: 1.1330 acc_train: 0.6357 loss_val: 1.2626 acc_val: 0.6533 time: 0.0045s\n",
            "Epoch: 0073 loss_train: 1.1187 acc_train: 0.6643 loss_val: 1.2515 acc_val: 0.6633 time: 0.0065s\n",
            "Epoch: 0074 loss_train: 1.1123 acc_train: 0.7000 loss_val: 1.2402 acc_val: 0.6733 time: 0.0056s\n",
            "Epoch: 0075 loss_train: 1.0846 acc_train: 0.7000 loss_val: 1.2291 acc_val: 0.6867 time: 0.0043s\n",
            "Epoch: 0076 loss_train: 1.1141 acc_train: 0.6929 loss_val: 1.2178 acc_val: 0.7000 time: 0.0043s\n",
            "Epoch: 0077 loss_train: 1.0632 acc_train: 0.7000 loss_val: 1.2065 acc_val: 0.7067 time: 0.0043s\n",
            "Epoch: 0078 loss_train: 1.0270 acc_train: 0.7500 loss_val: 1.1959 acc_val: 0.7200 time: 0.0043s\n",
            "Epoch: 0079 loss_train: 1.0318 acc_train: 0.7429 loss_val: 1.1850 acc_val: 0.7200 time: 0.0042s\n",
            "Epoch: 0080 loss_train: 1.0142 acc_train: 0.7429 loss_val: 1.1745 acc_val: 0.7200 time: 0.0043s\n",
            "Epoch: 0081 loss_train: 0.9866 acc_train: 0.7857 loss_val: 1.1643 acc_val: 0.7267 time: 0.0043s\n",
            "Epoch: 0082 loss_train: 1.0272 acc_train: 0.7643 loss_val: 1.1543 acc_val: 0.7367 time: 0.0042s\n",
            "Epoch: 0083 loss_train: 0.9976 acc_train: 0.7071 loss_val: 1.1444 acc_val: 0.7533 time: 0.0042s\n",
            "Epoch: 0084 loss_train: 1.0148 acc_train: 0.7571 loss_val: 1.1343 acc_val: 0.7533 time: 0.0042s\n",
            "Epoch: 0085 loss_train: 0.9657 acc_train: 0.7429 loss_val: 1.1248 acc_val: 0.7500 time: 0.0044s\n",
            "Epoch: 0086 loss_train: 0.9369 acc_train: 0.7571 loss_val: 1.1157 acc_val: 0.7567 time: 0.0042s\n",
            "Epoch: 0087 loss_train: 0.9692 acc_train: 0.7500 loss_val: 1.1070 acc_val: 0.7567 time: 0.0042s\n",
            "Epoch: 0088 loss_train: 0.9450 acc_train: 0.7714 loss_val: 1.0994 acc_val: 0.7567 time: 0.0042s\n",
            "Epoch: 0089 loss_train: 0.9272 acc_train: 0.7714 loss_val: 1.0921 acc_val: 0.7600 time: 0.0041s\n",
            "Epoch: 0090 loss_train: 0.9457 acc_train: 0.7571 loss_val: 1.0833 acc_val: 0.7633 time: 0.0045s\n",
            "Epoch: 0091 loss_train: 0.9097 acc_train: 0.8071 loss_val: 1.0743 acc_val: 0.7633 time: 0.0043s\n",
            "Epoch: 0092 loss_train: 0.8874 acc_train: 0.8214 loss_val: 1.0648 acc_val: 0.7700 time: 0.0042s\n",
            "Epoch: 0093 loss_train: 0.8617 acc_train: 0.8214 loss_val: 1.0562 acc_val: 0.7800 time: 0.0046s\n",
            "Epoch: 0094 loss_train: 0.8575 acc_train: 0.8357 loss_val: 1.0476 acc_val: 0.7833 time: 0.0058s\n",
            "Epoch: 0095 loss_train: 0.8512 acc_train: 0.8214 loss_val: 1.0398 acc_val: 0.7867 time: 0.0043s\n",
            "Epoch: 0096 loss_train: 0.8409 acc_train: 0.8071 loss_val: 1.0325 acc_val: 0.7867 time: 0.0045s\n",
            "Epoch: 0097 loss_train: 0.8452 acc_train: 0.8357 loss_val: 1.0260 acc_val: 0.7900 time: 0.0043s\n",
            "Epoch: 0098 loss_train: 0.7856 acc_train: 0.8786 loss_val: 1.0197 acc_val: 0.7933 time: 0.0042s\n",
            "Epoch: 0099 loss_train: 0.7916 acc_train: 0.8643 loss_val: 1.0130 acc_val: 0.8000 time: 0.0043s\n",
            "Epoch: 0100 loss_train: 0.8283 acc_train: 0.8071 loss_val: 1.0058 acc_val: 0.8000 time: 0.0042s\n",
            "Epoch: 0101 loss_train: 0.8337 acc_train: 0.8000 loss_val: 0.9988 acc_val: 0.7967 time: 0.0042s\n",
            "Epoch: 0102 loss_train: 0.7643 acc_train: 0.8929 loss_val: 0.9916 acc_val: 0.7967 time: 0.0044s\n",
            "Epoch: 0103 loss_train: 0.7464 acc_train: 0.8714 loss_val: 0.9842 acc_val: 0.7933 time: 0.0044s\n",
            "Epoch: 0104 loss_train: 0.8122 acc_train: 0.8429 loss_val: 0.9769 acc_val: 0.7967 time: 0.0043s\n",
            "Epoch: 0105 loss_train: 0.7709 acc_train: 0.8786 loss_val: 0.9697 acc_val: 0.7933 time: 0.0042s\n",
            "Epoch: 0106 loss_train: 0.7795 acc_train: 0.8500 loss_val: 0.9621 acc_val: 0.7967 time: 0.0042s\n",
            "Epoch: 0107 loss_train: 0.7507 acc_train: 0.8571 loss_val: 0.9550 acc_val: 0.8000 time: 0.0043s\n",
            "Epoch: 0108 loss_train: 0.7359 acc_train: 0.8786 loss_val: 0.9478 acc_val: 0.8100 time: 0.0060s\n",
            "Epoch: 0109 loss_train: 0.7793 acc_train: 0.8214 loss_val: 0.9419 acc_val: 0.8133 time: 0.0046s\n",
            "Epoch: 0110 loss_train: 0.7469 acc_train: 0.8714 loss_val: 0.9371 acc_val: 0.8167 time: 0.0042s\n",
            "Epoch: 0111 loss_train: 0.7294 acc_train: 0.8500 loss_val: 0.9326 acc_val: 0.8133 time: 0.0042s\n",
            "Epoch: 0112 loss_train: 0.7417 acc_train: 0.8571 loss_val: 0.9270 acc_val: 0.8133 time: 0.0042s\n",
            "Epoch: 0113 loss_train: 0.7520 acc_train: 0.8143 loss_val: 0.9199 acc_val: 0.8167 time: 0.0043s\n",
            "Epoch: 0114 loss_train: 0.7287 acc_train: 0.8429 loss_val: 0.9134 acc_val: 0.8167 time: 0.0042s\n",
            "Epoch: 0115 loss_train: 0.7122 acc_train: 0.8714 loss_val: 0.9074 acc_val: 0.8133 time: 0.0045s\n",
            "Epoch: 0116 loss_train: 0.7162 acc_train: 0.8857 loss_val: 0.9023 acc_val: 0.8067 time: 0.0042s\n",
            "Epoch: 0117 loss_train: 0.7286 acc_train: 0.8714 loss_val: 0.8977 acc_val: 0.8000 time: 0.0043s\n",
            "Epoch: 0118 loss_train: 0.6734 acc_train: 0.9071 loss_val: 0.8933 acc_val: 0.8000 time: 0.0052s\n",
            "Epoch: 0119 loss_train: 0.7266 acc_train: 0.8500 loss_val: 0.8889 acc_val: 0.8033 time: 0.0048s\n",
            "Epoch: 0120 loss_train: 0.6971 acc_train: 0.8500 loss_val: 0.8851 acc_val: 0.8033 time: 0.0046s\n",
            "Epoch: 0121 loss_train: 0.7454 acc_train: 0.8643 loss_val: 0.8811 acc_val: 0.8067 time: 0.0052s\n",
            "Epoch: 0122 loss_train: 0.6429 acc_train: 0.8929 loss_val: 0.8764 acc_val: 0.8067 time: 0.0043s\n",
            "Epoch: 0123 loss_train: 0.6732 acc_train: 0.8857 loss_val: 0.8701 acc_val: 0.8067 time: 0.0043s\n",
            "Epoch: 0124 loss_train: 0.6660 acc_train: 0.8714 loss_val: 0.8644 acc_val: 0.8100 time: 0.0043s\n",
            "Epoch: 0125 loss_train: 0.6574 acc_train: 0.8429 loss_val: 0.8598 acc_val: 0.8133 time: 0.0041s\n",
            "Epoch: 0126 loss_train: 0.6399 acc_train: 0.9143 loss_val: 0.8550 acc_val: 0.8133 time: 0.0043s\n",
            "Epoch: 0127 loss_train: 0.6285 acc_train: 0.9000 loss_val: 0.8500 acc_val: 0.8100 time: 0.0042s\n",
            "Epoch: 0128 loss_train: 0.6369 acc_train: 0.8571 loss_val: 0.8462 acc_val: 0.8100 time: 0.0042s\n",
            "Epoch: 0129 loss_train: 0.6188 acc_train: 0.9071 loss_val: 0.8433 acc_val: 0.8133 time: 0.0043s\n",
            "Epoch: 0130 loss_train: 0.6094 acc_train: 0.9214 loss_val: 0.8400 acc_val: 0.8100 time: 0.0046s\n",
            "Epoch: 0131 loss_train: 0.5931 acc_train: 0.9000 loss_val: 0.8359 acc_val: 0.8100 time: 0.0045s\n",
            "Epoch: 0132 loss_train: 0.6098 acc_train: 0.9143 loss_val: 0.8313 acc_val: 0.8133 time: 0.0045s\n",
            "Epoch: 0133 loss_train: 0.6526 acc_train: 0.8929 loss_val: 0.8272 acc_val: 0.8133 time: 0.0045s\n",
            "Epoch: 0134 loss_train: 0.6069 acc_train: 0.9071 loss_val: 0.8238 acc_val: 0.8167 time: 0.0045s\n",
            "Epoch: 0135 loss_train: 0.6012 acc_train: 0.8714 loss_val: 0.8220 acc_val: 0.8200 time: 0.0045s\n",
            "Epoch: 0136 loss_train: 0.6120 acc_train: 0.8714 loss_val: 0.8197 acc_val: 0.8233 time: 0.0041s\n",
            "Epoch: 0137 loss_train: 0.5734 acc_train: 0.9214 loss_val: 0.8168 acc_val: 0.8233 time: 0.0045s\n",
            "Epoch: 0138 loss_train: 0.5628 acc_train: 0.9357 loss_val: 0.8131 acc_val: 0.8233 time: 0.0042s\n",
            "Epoch: 0139 loss_train: 0.5973 acc_train: 0.8857 loss_val: 0.8081 acc_val: 0.8233 time: 0.0045s\n",
            "Epoch: 0140 loss_train: 0.5613 acc_train: 0.9000 loss_val: 0.8032 acc_val: 0.8200 time: 0.0045s\n",
            "Epoch: 0141 loss_train: 0.6107 acc_train: 0.8786 loss_val: 0.7988 acc_val: 0.8233 time: 0.0045s\n",
            "Epoch: 0142 loss_train: 0.5651 acc_train: 0.8857 loss_val: 0.7954 acc_val: 0.8167 time: 0.0045s\n",
            "Epoch: 0143 loss_train: 0.5553 acc_train: 0.9214 loss_val: 0.7934 acc_val: 0.8200 time: 0.0041s\n",
            "Epoch: 0144 loss_train: 0.5892 acc_train: 0.8857 loss_val: 0.7913 acc_val: 0.8167 time: 0.0041s\n",
            "Epoch: 0145 loss_train: 0.5999 acc_train: 0.9214 loss_val: 0.7877 acc_val: 0.8200 time: 0.0041s\n",
            "Epoch: 0146 loss_train: 0.5483 acc_train: 0.8929 loss_val: 0.7846 acc_val: 0.8200 time: 0.0040s\n",
            "Epoch: 0147 loss_train: 0.5284 acc_train: 0.9357 loss_val: 0.7824 acc_val: 0.8233 time: 0.0043s\n",
            "Epoch: 0148 loss_train: 0.5475 acc_train: 0.9286 loss_val: 0.7800 acc_val: 0.8267 time: 0.0053s\n",
            "Epoch: 0149 loss_train: 0.5111 acc_train: 0.9143 loss_val: 0.7779 acc_val: 0.8267 time: 0.0056s\n",
            "Epoch: 0150 loss_train: 0.5683 acc_train: 0.9071 loss_val: 0.7760 acc_val: 0.8267 time: 0.0048s\n",
            "Epoch: 0151 loss_train: 0.5384 acc_train: 0.8929 loss_val: 0.7736 acc_val: 0.8300 time: 0.0047s\n",
            "Epoch: 0152 loss_train: 0.5039 acc_train: 0.9071 loss_val: 0.7709 acc_val: 0.8300 time: 0.0046s\n",
            "Epoch: 0153 loss_train: 0.5214 acc_train: 0.8929 loss_val: 0.7694 acc_val: 0.8233 time: 0.0046s\n",
            "Epoch: 0154 loss_train: 0.5366 acc_train: 0.8857 loss_val: 0.7675 acc_val: 0.8267 time: 0.0043s\n",
            "Epoch: 0155 loss_train: 0.4951 acc_train: 0.9143 loss_val: 0.7654 acc_val: 0.8267 time: 0.0042s\n",
            "Epoch: 0156 loss_train: 0.5276 acc_train: 0.9143 loss_val: 0.7626 acc_val: 0.8267 time: 0.0045s\n",
            "Epoch: 0157 loss_train: 0.5079 acc_train: 0.9071 loss_val: 0.7596 acc_val: 0.8267 time: 0.0044s\n",
            "Epoch: 0158 loss_train: 0.5327 acc_train: 0.9214 loss_val: 0.7575 acc_val: 0.8233 time: 0.0043s\n",
            "Epoch: 0159 loss_train: 0.5328 acc_train: 0.9286 loss_val: 0.7559 acc_val: 0.8233 time: 0.0042s\n",
            "Epoch: 0160 loss_train: 0.5023 acc_train: 0.9000 loss_val: 0.7543 acc_val: 0.8233 time: 0.0080s\n",
            "Epoch: 0161 loss_train: 0.5271 acc_train: 0.9071 loss_val: 0.7530 acc_val: 0.8200 time: 0.0042s\n",
            "Epoch: 0162 loss_train: 0.4996 acc_train: 0.9214 loss_val: 0.7532 acc_val: 0.8233 time: 0.0041s\n",
            "Epoch: 0163 loss_train: 0.5252 acc_train: 0.9286 loss_val: 0.7524 acc_val: 0.8200 time: 0.0045s\n",
            "Epoch: 0164 loss_train: 0.5015 acc_train: 0.9143 loss_val: 0.7506 acc_val: 0.8200 time: 0.0042s\n",
            "Epoch: 0165 loss_train: 0.4440 acc_train: 0.9429 loss_val: 0.7482 acc_val: 0.8200 time: 0.0041s\n",
            "Epoch: 0166 loss_train: 0.4703 acc_train: 0.9357 loss_val: 0.7466 acc_val: 0.8200 time: 0.0041s\n",
            "Epoch: 0167 loss_train: 0.5340 acc_train: 0.9071 loss_val: 0.7447 acc_val: 0.8200 time: 0.0041s\n",
            "Epoch: 0168 loss_train: 0.5094 acc_train: 0.9143 loss_val: 0.7430 acc_val: 0.8200 time: 0.0045s\n",
            "Epoch: 0169 loss_train: 0.5008 acc_train: 0.8929 loss_val: 0.7413 acc_val: 0.8233 time: 0.0042s\n",
            "Epoch: 0170 loss_train: 0.4969 acc_train: 0.9000 loss_val: 0.7380 acc_val: 0.8233 time: 0.0055s\n",
            "Epoch: 0171 loss_train: 0.4533 acc_train: 0.9429 loss_val: 0.7345 acc_val: 0.8233 time: 0.0040s\n",
            "Epoch: 0172 loss_train: 0.4718 acc_train: 0.9500 loss_val: 0.7314 acc_val: 0.8233 time: 0.0041s\n",
            "Epoch: 0173 loss_train: 0.5022 acc_train: 0.9357 loss_val: 0.7281 acc_val: 0.8233 time: 0.0041s\n",
            "Epoch: 0174 loss_train: 0.4580 acc_train: 0.9214 loss_val: 0.7255 acc_val: 0.8267 time: 0.0045s\n",
            "Epoch: 0175 loss_train: 0.4650 acc_train: 0.9429 loss_val: 0.7230 acc_val: 0.8300 time: 0.0041s\n",
            "Epoch: 0176 loss_train: 0.4661 acc_train: 0.9143 loss_val: 0.7207 acc_val: 0.8267 time: 0.0041s\n",
            "Epoch: 0177 loss_train: 0.4724 acc_train: 0.9143 loss_val: 0.7187 acc_val: 0.8300 time: 0.0041s\n",
            "Epoch: 0178 loss_train: 0.5160 acc_train: 0.9000 loss_val: 0.7176 acc_val: 0.8300 time: 0.0040s\n",
            "Epoch: 0179 loss_train: 0.4609 acc_train: 0.9214 loss_val: 0.7175 acc_val: 0.8333 time: 0.0041s\n",
            "Epoch: 0180 loss_train: 0.4422 acc_train: 0.9429 loss_val: 0.7181 acc_val: 0.8300 time: 0.0040s\n",
            "Epoch: 0181 loss_train: 0.4585 acc_train: 0.9143 loss_val: 0.7194 acc_val: 0.8300 time: 0.0042s\n",
            "Epoch: 0182 loss_train: 0.4096 acc_train: 0.9429 loss_val: 0.7211 acc_val: 0.8167 time: 0.0043s\n",
            "Epoch: 0183 loss_train: 0.4656 acc_train: 0.9286 loss_val: 0.7229 acc_val: 0.8133 time: 0.0043s\n",
            "Epoch: 0184 loss_train: 0.4731 acc_train: 0.9429 loss_val: 0.7215 acc_val: 0.8133 time: 0.0043s\n",
            "Epoch: 0185 loss_train: 0.4837 acc_train: 0.9143 loss_val: 0.7176 acc_val: 0.8133 time: 0.0042s\n",
            "Epoch: 0186 loss_train: 0.4336 acc_train: 0.9500 loss_val: 0.7136 acc_val: 0.8200 time: 0.0041s\n",
            "Epoch: 0187 loss_train: 0.5271 acc_train: 0.9143 loss_val: 0.7090 acc_val: 0.8267 time: 0.0042s\n",
            "Epoch: 0188 loss_train: 0.4823 acc_train: 0.9357 loss_val: 0.7072 acc_val: 0.8267 time: 0.0043s\n",
            "Epoch: 0189 loss_train: 0.4362 acc_train: 0.9357 loss_val: 0.7052 acc_val: 0.8300 time: 0.0042s\n",
            "Epoch: 0190 loss_train: 0.4209 acc_train: 0.9429 loss_val: 0.7040 acc_val: 0.8233 time: 0.0042s\n",
            "Epoch: 0191 loss_train: 0.4509 acc_train: 0.9357 loss_val: 0.7029 acc_val: 0.8233 time: 0.0041s\n",
            "Epoch: 0192 loss_train: 0.4326 acc_train: 0.9286 loss_val: 0.7028 acc_val: 0.8233 time: 0.0041s\n",
            "Epoch: 0193 loss_train: 0.4529 acc_train: 0.9500 loss_val: 0.7038 acc_val: 0.8133 time: 0.0043s\n",
            "Epoch: 0194 loss_train: 0.4057 acc_train: 0.9571 loss_val: 0.7054 acc_val: 0.8100 time: 0.0051s\n",
            "Epoch: 0195 loss_train: 0.4566 acc_train: 0.9357 loss_val: 0.7070 acc_val: 0.8100 time: 0.0051s\n",
            "Epoch: 0196 loss_train: 0.3971 acc_train: 0.9571 loss_val: 0.7081 acc_val: 0.8100 time: 0.0047s\n",
            "Epoch: 0197 loss_train: 0.4594 acc_train: 0.9143 loss_val: 0.7086 acc_val: 0.8100 time: 0.0042s\n",
            "Epoch: 0198 loss_train: 0.3873 acc_train: 0.9643 loss_val: 0.7070 acc_val: 0.8100 time: 0.0041s\n",
            "Epoch: 0199 loss_train: 0.4654 acc_train: 0.9214 loss_val: 0.7034 acc_val: 0.8167 time: 0.0041s\n",
            "Epoch: 0200 loss_train: 0.4135 acc_train: 0.9571 loss_val: 0.7000 acc_val: 0.8167 time: 0.0041s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 1.0032s\n",
            "Test set results: loss= 0.7410 accuracy= 0.8300\n"
          ]
        }
      ],
      "source": [
        "# Your code goes here\n",
        "# Hint: see how to run the training code at https://github.com/tkipf/pygcn#usage\n",
        "\n",
        "os.chdir(\"pygcn\")\n",
        "! python train.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vH1hGHmTP16r"
      },
      "source": [
        "## 1.3 (To finish, 50 pts) Tuning the hyperparameters learniing rate from \\{0.1, 0.01, 0.001\\}, number of epochs from \\{100,300\\}, Number of hidden units from \\{8, 32\\}\n",
        "\n",
        "Hint: See [this file from Line 15](https://github.com/tkipf/pygcn/blob/master/pygcn/train.py#L15) for how to use the interface of \"train.py\" to change the hyper parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrbWnbWvP16r",
        "outputId": "c8994a8b-ca11-40ed-c32e-7a96aa519fcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading cora dataset...\n",
            "Epoch: 0001 loss_train: 1.8968 acc_train: 0.2143 loss_val: 1.8021 acc_val: 0.3500 time: 0.0105s\n",
            "Epoch: 0002 loss_train: 1.8081 acc_train: 0.2929 loss_val: 1.7611 acc_val: 0.3500 time: 0.0049s\n",
            "Epoch: 0003 loss_train: 1.7873 acc_train: 0.2929 loss_val: 1.7347 acc_val: 0.3500 time: 0.0047s\n",
            "Epoch: 0004 loss_train: 1.7202 acc_train: 0.2929 loss_val: 1.6977 acc_val: 0.3500 time: 0.0049s\n",
            "Epoch: 0005 loss_train: 1.6701 acc_train: 0.3214 loss_val: 1.6540 acc_val: 0.3767 time: 0.0047s\n",
            "Epoch: 0006 loss_train: 1.6091 acc_train: 0.4429 loss_val: 1.5993 acc_val: 0.4767 time: 0.0048s\n",
            "Epoch: 0007 loss_train: 1.5271 acc_train: 0.5286 loss_val: 1.5177 acc_val: 0.4300 time: 0.0047s\n",
            "Epoch: 0008 loss_train: 1.4603 acc_train: 0.4643 loss_val: 1.4383 acc_val: 0.4700 time: 0.0047s\n",
            "Epoch: 0009 loss_train: 1.3213 acc_train: 0.5357 loss_val: 1.3675 acc_val: 0.5467 time: 0.0047s\n",
            "Epoch: 0010 loss_train: 1.2615 acc_train: 0.6071 loss_val: 1.3002 acc_val: 0.6033 time: 0.0047s\n",
            "Epoch: 0011 loss_train: 1.1802 acc_train: 0.6429 loss_val: 1.2309 acc_val: 0.6233 time: 0.0054s\n",
            "Epoch: 0012 loss_train: 1.1119 acc_train: 0.6143 loss_val: 1.1642 acc_val: 0.6933 time: 0.0053s\n",
            "Epoch: 0013 loss_train: 1.0530 acc_train: 0.6500 loss_val: 1.1073 acc_val: 0.7167 time: 0.0061s\n",
            "Epoch: 0014 loss_train: 0.9196 acc_train: 0.7857 loss_val: 1.0542 acc_val: 0.7433 time: 0.0061s\n",
            "Epoch: 0015 loss_train: 0.8853 acc_train: 0.8000 loss_val: 1.0102 acc_val: 0.7667 time: 0.0052s\n",
            "Epoch: 0016 loss_train: 0.8090 acc_train: 0.8214 loss_val: 0.9651 acc_val: 0.7600 time: 0.0048s\n",
            "Epoch: 0017 loss_train: 0.7531 acc_train: 0.8357 loss_val: 0.9165 acc_val: 0.7600 time: 0.0048s\n",
            "Epoch: 0018 loss_train: 0.7370 acc_train: 0.8000 loss_val: 0.8800 acc_val: 0.7800 time: 0.0047s\n",
            "Epoch: 0019 loss_train: 0.6884 acc_train: 0.8500 loss_val: 0.8484 acc_val: 0.7867 time: 0.0047s\n",
            "Epoch: 0020 loss_train: 0.6763 acc_train: 0.8571 loss_val: 0.8280 acc_val: 0.7867 time: 0.0050s\n",
            "Epoch: 0021 loss_train: 0.6263 acc_train: 0.8143 loss_val: 0.8095 acc_val: 0.7833 time: 0.0050s\n",
            "Epoch: 0022 loss_train: 0.5942 acc_train: 0.8643 loss_val: 0.7782 acc_val: 0.7667 time: 0.0050s\n",
            "Epoch: 0023 loss_train: 0.5596 acc_train: 0.8571 loss_val: 0.7518 acc_val: 0.7767 time: 0.0047s\n",
            "Epoch: 0024 loss_train: 0.5232 acc_train: 0.8643 loss_val: 0.7514 acc_val: 0.7933 time: 0.0048s\n",
            "Epoch: 0025 loss_train: 0.4811 acc_train: 0.8857 loss_val: 0.7251 acc_val: 0.7967 time: 0.0047s\n",
            "Epoch: 0026 loss_train: 0.4834 acc_train: 0.9000 loss_val: 0.7055 acc_val: 0.7833 time: 0.0047s\n",
            "Epoch: 0027 loss_train: 0.4265 acc_train: 0.9000 loss_val: 0.7078 acc_val: 0.7733 time: 0.0047s\n",
            "Epoch: 0028 loss_train: 0.4826 acc_train: 0.9071 loss_val: 0.6942 acc_val: 0.7867 time: 0.0048s\n",
            "Epoch: 0029 loss_train: 0.3835 acc_train: 0.9214 loss_val: 0.7032 acc_val: 0.8100 time: 0.0048s\n",
            "Epoch: 0030 loss_train: 0.4783 acc_train: 0.8929 loss_val: 0.7555 acc_val: 0.7900 time: 0.0048s\n",
            "Epoch: 0031 loss_train: 0.4396 acc_train: 0.9071 loss_val: 0.7077 acc_val: 0.8067 time: 0.0050s\n",
            "Epoch: 0032 loss_train: 0.3742 acc_train: 0.9286 loss_val: 0.6748 acc_val: 0.8067 time: 0.0050s\n",
            "Epoch: 0033 loss_train: 0.4477 acc_train: 0.8929 loss_val: 0.6744 acc_val: 0.7933 time: 0.0045s\n",
            "Epoch: 0034 loss_train: 0.4649 acc_train: 0.8786 loss_val: 0.6667 acc_val: 0.7733 time: 0.0046s\n",
            "Epoch: 0035 loss_train: 0.3412 acc_train: 0.9000 loss_val: 0.6841 acc_val: 0.8000 time: 0.0044s\n",
            "Epoch: 0036 loss_train: 0.4333 acc_train: 0.9143 loss_val: 0.6691 acc_val: 0.8033 time: 0.0045s\n",
            "Epoch: 0037 loss_train: 0.3744 acc_train: 0.9357 loss_val: 0.6539 acc_val: 0.8067 time: 0.0045s\n",
            "Epoch: 0038 loss_train: 0.3846 acc_train: 0.9357 loss_val: 0.6843 acc_val: 0.7867 time: 0.0045s\n",
            "Epoch: 0039 loss_train: 0.4057 acc_train: 0.9071 loss_val: 0.6614 acc_val: 0.8100 time: 0.0045s\n",
            "Epoch: 0040 loss_train: 0.3523 acc_train: 0.9429 loss_val: 0.6880 acc_val: 0.8067 time: 0.0046s\n",
            "Epoch: 0041 loss_train: 0.3506 acc_train: 0.9286 loss_val: 0.7028 acc_val: 0.8000 time: 0.0045s\n",
            "Epoch: 0042 loss_train: 0.3550 acc_train: 0.9571 loss_val: 0.6556 acc_val: 0.8000 time: 0.0045s\n",
            "Epoch: 0043 loss_train: 0.3029 acc_train: 0.9500 loss_val: 0.6229 acc_val: 0.8067 time: 0.0048s\n",
            "Epoch: 0044 loss_train: 0.2789 acc_train: 0.9500 loss_val: 0.6248 acc_val: 0.8000 time: 0.0048s\n",
            "Epoch: 0045 loss_train: 0.2857 acc_train: 0.9429 loss_val: 0.6340 acc_val: 0.8067 time: 0.0046s\n",
            "Epoch: 0046 loss_train: 0.2814 acc_train: 0.9571 loss_val: 0.6539 acc_val: 0.8000 time: 0.0046s\n",
            "Epoch: 0047 loss_train: 0.3039 acc_train: 0.9500 loss_val: 0.6894 acc_val: 0.8000 time: 0.0047s\n",
            "Epoch: 0048 loss_train: 0.2637 acc_train: 0.9857 loss_val: 0.6849 acc_val: 0.7867 time: 0.0045s\n",
            "Epoch: 0049 loss_train: 0.3454 acc_train: 0.9500 loss_val: 0.6374 acc_val: 0.8000 time: 0.0045s\n",
            "Epoch: 0050 loss_train: 0.3255 acc_train: 0.9286 loss_val: 0.6251 acc_val: 0.8000 time: 0.0046s\n",
            "Epoch: 0051 loss_train: 0.3133 acc_train: 0.9429 loss_val: 0.6446 acc_val: 0.7800 time: 0.0049s\n",
            "Epoch: 0052 loss_train: 0.2968 acc_train: 0.9286 loss_val: 0.6525 acc_val: 0.7767 time: 0.0068s\n",
            "Epoch: 0053 loss_train: 0.2891 acc_train: 0.9571 loss_val: 0.6564 acc_val: 0.7900 time: 0.0063s\n",
            "Epoch: 0054 loss_train: 0.2407 acc_train: 0.9857 loss_val: 0.6783 acc_val: 0.7900 time: 0.0047s\n",
            "Epoch: 0055 loss_train: 0.2789 acc_train: 0.9357 loss_val: 0.6736 acc_val: 0.7967 time: 0.0045s\n",
            "Epoch: 0056 loss_train: 0.3149 acc_train: 0.9429 loss_val: 0.6714 acc_val: 0.7967 time: 0.0047s\n",
            "Epoch: 0057 loss_train: 0.2931 acc_train: 0.9571 loss_val: 0.6578 acc_val: 0.7900 time: 0.0047s\n",
            "Epoch: 0058 loss_train: 0.2419 acc_train: 0.9643 loss_val: 0.6523 acc_val: 0.8000 time: 0.0046s\n",
            "Epoch: 0059 loss_train: 0.2639 acc_train: 0.9643 loss_val: 0.6725 acc_val: 0.7900 time: 0.0046s\n",
            "Epoch: 0060 loss_train: 0.2620 acc_train: 0.9429 loss_val: 0.6629 acc_val: 0.8067 time: 0.0049s\n",
            "Epoch: 0061 loss_train: 0.3002 acc_train: 0.9571 loss_val: 0.6418 acc_val: 0.8000 time: 0.0046s\n",
            "Epoch: 0062 loss_train: 0.2700 acc_train: 0.9500 loss_val: 0.6498 acc_val: 0.8000 time: 0.0045s\n",
            "Epoch: 0063 loss_train: 0.2831 acc_train: 0.9571 loss_val: 0.6475 acc_val: 0.8067 time: 0.0046s\n",
            "Epoch: 0064 loss_train: 0.3068 acc_train: 0.9429 loss_val: 0.6440 acc_val: 0.8100 time: 0.0046s\n",
            "Epoch: 0065 loss_train: 0.2648 acc_train: 0.9571 loss_val: 0.6622 acc_val: 0.8067 time: 0.0045s\n",
            "Epoch: 0066 loss_train: 0.2817 acc_train: 0.9429 loss_val: 0.6476 acc_val: 0.8033 time: 0.0046s\n",
            "Epoch: 0067 loss_train: 0.2393 acc_train: 0.9786 loss_val: 0.6453 acc_val: 0.7933 time: 0.0047s\n",
            "Epoch: 0068 loss_train: 0.3117 acc_train: 0.9357 loss_val: 0.6257 acc_val: 0.8033 time: 0.0046s\n",
            "Epoch: 0069 loss_train: 0.2882 acc_train: 0.9571 loss_val: 0.6233 acc_val: 0.8000 time: 0.0046s\n",
            "Epoch: 0070 loss_train: 0.3091 acc_train: 0.9571 loss_val: 0.6404 acc_val: 0.8100 time: 0.0046s\n",
            "Epoch: 0071 loss_train: 0.2428 acc_train: 0.9857 loss_val: 0.6610 acc_val: 0.8133 time: 0.0045s\n",
            "Epoch: 0072 loss_train: 0.2822 acc_train: 0.9571 loss_val: 0.6666 acc_val: 0.8067 time: 0.0046s\n",
            "Epoch: 0073 loss_train: 0.2994 acc_train: 0.9214 loss_val: 0.6535 acc_val: 0.7933 time: 0.0047s\n",
            "Epoch: 0074 loss_train: 0.2433 acc_train: 0.9714 loss_val: 0.6282 acc_val: 0.8167 time: 0.0048s\n",
            "Epoch: 0075 loss_train: 0.2738 acc_train: 0.9429 loss_val: 0.6200 acc_val: 0.8200 time: 0.0043s\n",
            "Epoch: 0076 loss_train: 0.2952 acc_train: 0.9286 loss_val: 0.6152 acc_val: 0.8200 time: 0.0044s\n",
            "Epoch: 0077 loss_train: 0.1986 acc_train: 0.9786 loss_val: 0.6233 acc_val: 0.8033 time: 0.0044s\n",
            "Epoch: 0078 loss_train: 0.2316 acc_train: 0.9643 loss_val: 0.6478 acc_val: 0.8000 time: 0.0044s\n",
            "Epoch: 0079 loss_train: 0.3025 acc_train: 0.9714 loss_val: 0.6416 acc_val: 0.8000 time: 0.0043s\n",
            "Epoch: 0080 loss_train: 0.2156 acc_train: 0.9786 loss_val: 0.6351 acc_val: 0.8200 time: 0.0044s\n",
            "Epoch: 0081 loss_train: 0.2297 acc_train: 0.9643 loss_val: 0.6271 acc_val: 0.8200 time: 0.0043s\n",
            "Epoch: 0082 loss_train: 0.2813 acc_train: 0.9286 loss_val: 0.6161 acc_val: 0.8200 time: 0.0043s\n",
            "Epoch: 0083 loss_train: 0.2679 acc_train: 0.9571 loss_val: 0.6071 acc_val: 0.8267 time: 0.0052s\n",
            "Epoch: 0084 loss_train: 0.2560 acc_train: 0.9500 loss_val: 0.6132 acc_val: 0.8367 time: 0.0051s\n",
            "Epoch: 0085 loss_train: 0.2607 acc_train: 0.9429 loss_val: 0.6221 acc_val: 0.8333 time: 0.0043s\n",
            "Epoch: 0086 loss_train: 0.3026 acc_train: 0.9143 loss_val: 0.6446 acc_val: 0.8100 time: 0.0041s\n",
            "Epoch: 0087 loss_train: 0.2744 acc_train: 0.9500 loss_val: 0.6711 acc_val: 0.7933 time: 0.0042s\n",
            "Epoch: 0088 loss_train: 0.2918 acc_train: 0.9643 loss_val: 0.6578 acc_val: 0.7933 time: 0.0042s\n",
            "Epoch: 0089 loss_train: 0.2957 acc_train: 0.9357 loss_val: 0.6206 acc_val: 0.8133 time: 0.0042s\n",
            "Epoch: 0090 loss_train: 0.2418 acc_train: 0.9643 loss_val: 0.6135 acc_val: 0.8067 time: 0.0042s\n",
            "Epoch: 0091 loss_train: 0.2552 acc_train: 0.9714 loss_val: 0.6195 acc_val: 0.7933 time: 0.0042s\n",
            "Epoch: 0092 loss_train: 0.2922 acc_train: 0.9357 loss_val: 0.6270 acc_val: 0.7900 time: 0.0043s\n",
            "Epoch: 0093 loss_train: 0.2411 acc_train: 0.9643 loss_val: 0.6446 acc_val: 0.7967 time: 0.0042s\n",
            "Epoch: 0094 loss_train: 0.2857 acc_train: 0.9286 loss_val: 0.6448 acc_val: 0.8167 time: 0.0043s\n",
            "Epoch: 0095 loss_train: 0.2511 acc_train: 0.9571 loss_val: 0.6383 acc_val: 0.8100 time: 0.0042s\n",
            "Epoch: 0096 loss_train: 0.2467 acc_train: 0.9714 loss_val: 0.6222 acc_val: 0.8067 time: 0.0071s\n",
            "Epoch: 0097 loss_train: 0.2143 acc_train: 0.9786 loss_val: 0.6276 acc_val: 0.7967 time: 0.0048s\n",
            "Epoch: 0098 loss_train: 0.2745 acc_train: 0.9500 loss_val: 0.6526 acc_val: 0.7800 time: 0.0042s\n",
            "Epoch: 0099 loss_train: 0.2830 acc_train: 0.9714 loss_val: 0.6486 acc_val: 0.8100 time: 0.0042s\n",
            "Epoch: 0100 loss_train: 0.2584 acc_train: 0.9786 loss_val: 0.6511 acc_val: 0.8167 time: 0.0043s\n",
            "Epoch: 0101 loss_train: 0.2520 acc_train: 0.9571 loss_val: 0.6440 acc_val: 0.8067 time: 0.0043s\n",
            "Epoch: 0102 loss_train: 0.2648 acc_train: 0.9429 loss_val: 0.6235 acc_val: 0.8100 time: 0.0042s\n",
            "Epoch: 0103 loss_train: 0.2486 acc_train: 0.9500 loss_val: 0.6421 acc_val: 0.8067 time: 0.0042s\n",
            "Epoch: 0104 loss_train: 0.2442 acc_train: 0.9786 loss_val: 0.6685 acc_val: 0.8000 time: 0.0044s\n",
            "Epoch: 0105 loss_train: 0.2443 acc_train: 0.9643 loss_val: 0.6683 acc_val: 0.7867 time: 0.0044s\n",
            "Epoch: 0106 loss_train: 0.2984 acc_train: 0.9286 loss_val: 0.6452 acc_val: 0.8033 time: 0.0042s\n",
            "Epoch: 0107 loss_train: 0.2015 acc_train: 0.9786 loss_val: 0.6689 acc_val: 0.7933 time: 0.0043s\n",
            "Epoch: 0108 loss_train: 0.2932 acc_train: 0.9071 loss_val: 0.6663 acc_val: 0.8067 time: 0.0042s\n",
            "Epoch: 0109 loss_train: 0.2465 acc_train: 0.9500 loss_val: 0.6517 acc_val: 0.8133 time: 0.0041s\n",
            "Epoch: 0110 loss_train: 0.2264 acc_train: 0.9643 loss_val: 0.6609 acc_val: 0.7900 time: 0.0041s\n",
            "Epoch: 0111 loss_train: 0.2218 acc_train: 0.9643 loss_val: 0.6657 acc_val: 0.7900 time: 0.0042s\n",
            "Epoch: 0112 loss_train: 0.2361 acc_train: 0.9714 loss_val: 0.6533 acc_val: 0.7933 time: 0.0041s\n",
            "Epoch: 0113 loss_train: 0.2419 acc_train: 0.9643 loss_val: 0.6378 acc_val: 0.7933 time: 0.0043s\n",
            "Epoch: 0114 loss_train: 0.2983 acc_train: 0.9357 loss_val: 0.6258 acc_val: 0.8133 time: 0.0041s\n",
            "Epoch: 0115 loss_train: 0.2338 acc_train: 0.9714 loss_val: 0.6269 acc_val: 0.8133 time: 0.0042s\n",
            "Epoch: 0116 loss_train: 0.2550 acc_train: 0.9714 loss_val: 0.6224 acc_val: 0.8133 time: 0.0042s\n",
            "Epoch: 0117 loss_train: 0.2667 acc_train: 0.9429 loss_val: 0.6159 acc_val: 0.8100 time: 0.0042s\n",
            "Epoch: 0118 loss_train: 0.2326 acc_train: 0.9786 loss_val: 0.6152 acc_val: 0.8200 time: 0.0042s\n",
            "Epoch: 0119 loss_train: 0.2567 acc_train: 0.9571 loss_val: 0.6248 acc_val: 0.8033 time: 0.0042s\n",
            "Epoch: 0120 loss_train: 0.2718 acc_train: 0.9714 loss_val: 0.6411 acc_val: 0.8033 time: 0.0042s\n",
            "Epoch: 0121 loss_train: 0.2410 acc_train: 0.9786 loss_val: 0.6632 acc_val: 0.7867 time: 0.0041s\n",
            "Epoch: 0122 loss_train: 0.2566 acc_train: 0.9643 loss_val: 0.6326 acc_val: 0.8000 time: 0.0041s\n",
            "Epoch: 0123 loss_train: 0.2724 acc_train: 0.9357 loss_val: 0.6088 acc_val: 0.8067 time: 0.0043s\n",
            "Epoch: 0124 loss_train: 0.2494 acc_train: 0.9571 loss_val: 0.6143 acc_val: 0.8133 time: 0.0049s\n",
            "Epoch: 0125 loss_train: 0.2392 acc_train: 0.9429 loss_val: 0.6287 acc_val: 0.8100 time: 0.0041s\n",
            "Epoch: 0126 loss_train: 0.2643 acc_train: 0.9500 loss_val: 0.6806 acc_val: 0.8000 time: 0.0041s\n",
            "Epoch: 0127 loss_train: 0.2700 acc_train: 0.9500 loss_val: 0.6539 acc_val: 0.8067 time: 0.0040s\n",
            "Epoch: 0128 loss_train: 0.2273 acc_train: 0.9786 loss_val: 0.6238 acc_val: 0.8100 time: 0.0042s\n",
            "Epoch: 0129 loss_train: 0.2416 acc_train: 0.9643 loss_val: 0.6224 acc_val: 0.8133 time: 0.0041s\n",
            "Epoch: 0130 loss_train: 0.2427 acc_train: 0.9714 loss_val: 0.6135 acc_val: 0.8100 time: 0.0041s\n",
            "Epoch: 0131 loss_train: 0.2322 acc_train: 0.9714 loss_val: 0.6299 acc_val: 0.7833 time: 0.0041s\n",
            "Epoch: 0132 loss_train: 0.2543 acc_train: 0.9429 loss_val: 0.6705 acc_val: 0.7833 time: 0.0040s\n",
            "Epoch: 0133 loss_train: 0.2518 acc_train: 0.9571 loss_val: 0.6563 acc_val: 0.7967 time: 0.0042s\n",
            "Epoch: 0134 loss_train: 0.2694 acc_train: 0.9357 loss_val: 0.6272 acc_val: 0.8133 time: 0.0041s\n",
            "Epoch: 0135 loss_train: 0.2279 acc_train: 0.9857 loss_val: 0.6316 acc_val: 0.8167 time: 0.0041s\n",
            "Epoch: 0136 loss_train: 0.2280 acc_train: 0.9571 loss_val: 0.6269 acc_val: 0.8100 time: 0.0041s\n",
            "Epoch: 0137 loss_train: 0.2185 acc_train: 0.9643 loss_val: 0.6374 acc_val: 0.7933 time: 0.0041s\n",
            "Epoch: 0138 loss_train: 0.2365 acc_train: 0.9643 loss_val: 0.6535 acc_val: 0.7900 time: 0.0041s\n",
            "Epoch: 0139 loss_train: 0.2410 acc_train: 0.9643 loss_val: 0.6357 acc_val: 0.7933 time: 0.0043s\n",
            "Epoch: 0140 loss_train: 0.2991 acc_train: 0.9429 loss_val: 0.6159 acc_val: 0.8067 time: 0.0041s\n",
            "Epoch: 0141 loss_train: 0.2319 acc_train: 0.9357 loss_val: 0.6078 acc_val: 0.8133 time: 0.0041s\n",
            "Epoch: 0142 loss_train: 0.2265 acc_train: 0.9643 loss_val: 0.6217 acc_val: 0.8133 time: 0.0044s\n",
            "Epoch: 0143 loss_train: 0.2455 acc_train: 0.9500 loss_val: 0.6326 acc_val: 0.8133 time: 0.0040s\n",
            "Epoch: 0144 loss_train: 0.2968 acc_train: 0.9429 loss_val: 0.6205 acc_val: 0.8067 time: 0.0041s\n",
            "Epoch: 0145 loss_train: 0.2738 acc_train: 0.9357 loss_val: 0.6170 acc_val: 0.8133 time: 0.0043s\n",
            "Epoch: 0146 loss_train: 0.2688 acc_train: 0.9643 loss_val: 0.6278 acc_val: 0.7933 time: 0.0041s\n",
            "Epoch: 0147 loss_train: 0.2606 acc_train: 0.9357 loss_val: 0.6505 acc_val: 0.7900 time: 0.0041s\n",
            "Epoch: 0148 loss_train: 0.2610 acc_train: 0.9500 loss_val: 0.6386 acc_val: 0.8033 time: 0.0042s\n",
            "Epoch: 0149 loss_train: 0.2549 acc_train: 0.9500 loss_val: 0.6272 acc_val: 0.8200 time: 0.0041s\n",
            "Epoch: 0150 loss_train: 0.2254 acc_train: 0.9857 loss_val: 0.6247 acc_val: 0.8100 time: 0.0041s\n",
            "Epoch: 0151 loss_train: 0.2646 acc_train: 0.9429 loss_val: 0.6195 acc_val: 0.8167 time: 0.0041s\n",
            "Epoch: 0152 loss_train: 0.2240 acc_train: 0.9643 loss_val: 0.6305 acc_val: 0.8033 time: 0.0040s\n",
            "Epoch: 0153 loss_train: 0.2412 acc_train: 0.9571 loss_val: 0.6726 acc_val: 0.7867 time: 0.0040s\n",
            "Epoch: 0154 loss_train: 0.2652 acc_train: 0.9714 loss_val: 0.6538 acc_val: 0.7900 time: 0.0042s\n",
            "Epoch: 0155 loss_train: 0.2393 acc_train: 0.9714 loss_val: 0.6262 acc_val: 0.8067 time: 0.0041s\n",
            "Epoch: 0156 loss_train: 0.2236 acc_train: 0.9571 loss_val: 0.6387 acc_val: 0.7900 time: 0.0040s\n",
            "Epoch: 0157 loss_train: 0.2665 acc_train: 0.9571 loss_val: 0.6176 acc_val: 0.8067 time: 0.0041s\n",
            "Epoch: 0158 loss_train: 0.2718 acc_train: 0.9571 loss_val: 0.6139 acc_val: 0.8133 time: 0.0041s\n",
            "Epoch: 0159 loss_train: 0.2748 acc_train: 0.9571 loss_val: 0.6718 acc_val: 0.8067 time: 0.0043s\n",
            "Epoch: 0160 loss_train: 0.2862 acc_train: 0.9571 loss_val: 0.6413 acc_val: 0.8200 time: 0.0042s\n",
            "Epoch: 0161 loss_train: 0.2524 acc_train: 0.9571 loss_val: 0.6109 acc_val: 0.8067 time: 0.0041s\n",
            "Epoch: 0162 loss_train: 0.2694 acc_train: 0.9214 loss_val: 0.6151 acc_val: 0.8167 time: 0.0040s\n",
            "Epoch: 0163 loss_train: 0.2731 acc_train: 0.9500 loss_val: 0.6163 acc_val: 0.8067 time: 0.0040s\n",
            "Epoch: 0164 loss_train: 0.2541 acc_train: 0.9643 loss_val: 0.6366 acc_val: 0.7967 time: 0.0042s\n",
            "Epoch: 0165 loss_train: 0.2115 acc_train: 0.9500 loss_val: 0.6591 acc_val: 0.7967 time: 0.0043s\n",
            "Epoch: 0166 loss_train: 0.2564 acc_train: 0.9429 loss_val: 0.6513 acc_val: 0.8067 time: 0.0047s\n",
            "Epoch: 0167 loss_train: 0.2181 acc_train: 0.9786 loss_val: 0.6343 acc_val: 0.8167 time: 0.0078s\n",
            "Epoch: 0168 loss_train: 0.2715 acc_train: 0.9500 loss_val: 0.6141 acc_val: 0.8067 time: 0.0043s\n",
            "Epoch: 0169 loss_train: 0.2295 acc_train: 0.9500 loss_val: 0.6055 acc_val: 0.8100 time: 0.0043s\n",
            "Epoch: 0170 loss_train: 0.2753 acc_train: 0.9357 loss_val: 0.6361 acc_val: 0.7900 time: 0.0043s\n",
            "Epoch: 0171 loss_train: 0.2368 acc_train: 0.9571 loss_val: 0.6563 acc_val: 0.7867 time: 0.0045s\n",
            "Epoch: 0172 loss_train: 0.2435 acc_train: 0.9500 loss_val: 0.6545 acc_val: 0.8000 time: 0.0043s\n",
            "Epoch: 0173 loss_train: 0.2706 acc_train: 0.9786 loss_val: 0.6453 acc_val: 0.8133 time: 0.0042s\n",
            "Epoch: 0174 loss_train: 0.2135 acc_train: 0.9500 loss_val: 0.6290 acc_val: 0.8200 time: 0.0044s\n",
            "Epoch: 0175 loss_train: 0.2448 acc_train: 0.9500 loss_val: 0.6245 acc_val: 0.8100 time: 0.0043s\n",
            "Epoch: 0176 loss_train: 0.2529 acc_train: 0.9643 loss_val: 0.6360 acc_val: 0.7967 time: 0.0042s\n",
            "Epoch: 0177 loss_train: 0.2703 acc_train: 0.9714 loss_val: 0.6289 acc_val: 0.7967 time: 0.0043s\n",
            "Epoch: 0178 loss_train: 0.2210 acc_train: 0.9571 loss_val: 0.6257 acc_val: 0.8067 time: 0.0042s\n",
            "Epoch: 0179 loss_train: 0.2488 acc_train: 0.9714 loss_val: 0.6208 acc_val: 0.8167 time: 0.0042s\n",
            "Epoch: 0180 loss_train: 0.2368 acc_train: 0.9786 loss_val: 0.6145 acc_val: 0.8167 time: 0.0043s\n",
            "Epoch: 0181 loss_train: 0.2235 acc_train: 0.9714 loss_val: 0.6091 acc_val: 0.8100 time: 0.0042s\n",
            "Epoch: 0182 loss_train: 0.2399 acc_train: 0.9643 loss_val: 0.6180 acc_val: 0.8100 time: 0.0043s\n",
            "Epoch: 0183 loss_train: 0.2439 acc_train: 0.9500 loss_val: 0.6270 acc_val: 0.7967 time: 0.0043s\n",
            "Epoch: 0184 loss_train: 0.2374 acc_train: 0.9786 loss_val: 0.6196 acc_val: 0.8033 time: 0.0043s\n",
            "Epoch: 0185 loss_train: 0.2582 acc_train: 0.9643 loss_val: 0.6227 acc_val: 0.8033 time: 0.0043s\n",
            "Epoch: 0186 loss_train: 0.2064 acc_train: 0.9857 loss_val: 0.6328 acc_val: 0.8000 time: 0.0042s\n",
            "Epoch: 0187 loss_train: 0.2407 acc_train: 0.9571 loss_val: 0.6208 acc_val: 0.8100 time: 0.0042s\n",
            "Epoch: 0188 loss_train: 0.2146 acc_train: 0.9786 loss_val: 0.6219 acc_val: 0.8167 time: 0.0042s\n",
            "Epoch: 0189 loss_train: 0.2357 acc_train: 0.9714 loss_val: 0.6450 acc_val: 0.8133 time: 0.0044s\n",
            "Epoch: 0190 loss_train: 0.2279 acc_train: 0.9500 loss_val: 0.6599 acc_val: 0.8100 time: 0.0043s\n",
            "Epoch: 0191 loss_train: 0.2283 acc_train: 0.9714 loss_val: 0.6365 acc_val: 0.8133 time: 0.0042s\n",
            "Epoch: 0192 loss_train: 0.2653 acc_train: 0.9429 loss_val: 0.6182 acc_val: 0.8000 time: 0.0042s\n",
            "Epoch: 0193 loss_train: 0.2200 acc_train: 0.9643 loss_val: 0.6101 acc_val: 0.8033 time: 0.0043s\n",
            "Epoch: 0194 loss_train: 0.2083 acc_train: 0.9714 loss_val: 0.6100 acc_val: 0.8133 time: 0.0061s\n",
            "Epoch: 0195 loss_train: 0.2654 acc_train: 0.9286 loss_val: 0.6151 acc_val: 0.8067 time: 0.0047s\n",
            "Epoch: 0196 loss_train: 0.2453 acc_train: 0.9571 loss_val: 0.6267 acc_val: 0.8167 time: 0.0041s\n",
            "Epoch: 0197 loss_train: 0.2702 acc_train: 0.9500 loss_val: 0.6481 acc_val: 0.8067 time: 0.0040s\n",
            "Epoch: 0198 loss_train: 0.2554 acc_train: 0.9571 loss_val: 0.6227 acc_val: 0.8067 time: 0.0040s\n",
            "Epoch: 0199 loss_train: 0.2365 acc_train: 0.9643 loss_val: 0.5965 acc_val: 0.8167 time: 0.0042s\n",
            "Epoch: 0200 loss_train: 0.2652 acc_train: 0.9500 loss_val: 0.6014 acc_val: 0.8100 time: 0.0041s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 0.9133s\n",
            "Test set results: loss= 0.5873 accuracy= 0.8310\n",
            "------------------------------------------ learning rate 0.1 ^ ------------------------------------------\n",
            "Loading cora dataset...\n",
            "Epoch: 0001 loss_train: 2.0119 acc_train: 0.1071 loss_val: 2.0056 acc_val: 0.1267 time: 0.0116s\n",
            "Epoch: 0002 loss_train: 1.9917 acc_train: 0.1214 loss_val: 1.9912 acc_val: 0.1267 time: 0.0052s\n",
            "Epoch: 0003 loss_train: 1.9692 acc_train: 0.1500 loss_val: 1.9773 acc_val: 0.1267 time: 0.0048s\n",
            "Epoch: 0004 loss_train: 1.9711 acc_train: 0.1429 loss_val: 1.9634 acc_val: 0.1267 time: 0.0049s\n",
            "Epoch: 0005 loss_train: 1.9444 acc_train: 0.1929 loss_val: 1.9497 acc_val: 0.1267 time: 0.0050s\n",
            "Epoch: 0006 loss_train: 1.9419 acc_train: 0.1429 loss_val: 1.9365 acc_val: 0.1833 time: 0.0049s\n",
            "Epoch: 0007 loss_train: 1.9291 acc_train: 0.1357 loss_val: 1.9237 acc_val: 0.1567 time: 0.0048s\n",
            "Epoch: 0008 loss_train: 1.9114 acc_train: 0.2143 loss_val: 1.9109 acc_val: 0.1567 time: 0.0049s\n",
            "Epoch: 0009 loss_train: 1.9067 acc_train: 0.2000 loss_val: 1.8985 acc_val: 0.1567 time: 0.0055s\n",
            "Epoch: 0010 loss_train: 1.8738 acc_train: 0.2071 loss_val: 1.8864 acc_val: 0.1567 time: 0.0050s\n",
            "Epoch: 0011 loss_train: 1.8701 acc_train: 0.2071 loss_val: 1.8748 acc_val: 0.1567 time: 0.0049s\n",
            "Epoch: 0012 loss_train: 1.8553 acc_train: 0.2071 loss_val: 1.8636 acc_val: 0.1567 time: 0.0050s\n",
            "Epoch: 0013 loss_train: 1.8480 acc_train: 0.2214 loss_val: 1.8531 acc_val: 0.1567 time: 0.0048s\n",
            "Epoch: 0014 loss_train: 1.8431 acc_train: 0.2000 loss_val: 1.8431 acc_val: 0.1567 time: 0.0049s\n",
            "Epoch: 0015 loss_train: 1.8373 acc_train: 0.1929 loss_val: 1.8335 acc_val: 0.1567 time: 0.0049s\n",
            "Epoch: 0016 loss_train: 1.8373 acc_train: 0.1929 loss_val: 1.8246 acc_val: 0.1567 time: 0.0049s\n",
            "Epoch: 0017 loss_train: 1.8055 acc_train: 0.2214 loss_val: 1.8163 acc_val: 0.1567 time: 0.0048s\n",
            "Epoch: 0018 loss_train: 1.8062 acc_train: 0.2071 loss_val: 1.8082 acc_val: 0.1567 time: 0.0049s\n",
            "Epoch: 0019 loss_train: 1.7739 acc_train: 0.2071 loss_val: 1.8001 acc_val: 0.1567 time: 0.0049s\n",
            "Epoch: 0020 loss_train: 1.8053 acc_train: 0.2071 loss_val: 1.7923 acc_val: 0.1567 time: 0.0049s\n",
            "Epoch: 0021 loss_train: 1.7777 acc_train: 0.2357 loss_val: 1.7845 acc_val: 0.1567 time: 0.0050s\n",
            "Epoch: 0022 loss_train: 1.7726 acc_train: 0.2286 loss_val: 1.7768 acc_val: 0.2800 time: 0.0050s\n",
            "Epoch: 0023 loss_train: 1.7607 acc_train: 0.2929 loss_val: 1.7693 acc_val: 0.3467 time: 0.0049s\n",
            "Epoch: 0024 loss_train: 1.7483 acc_train: 0.2714 loss_val: 1.7617 acc_val: 0.3467 time: 0.0050s\n",
            "Epoch: 0025 loss_train: 1.7433 acc_train: 0.3071 loss_val: 1.7542 acc_val: 0.3500 time: 0.0048s\n",
            "Epoch: 0026 loss_train: 1.7340 acc_train: 0.2857 loss_val: 1.7469 acc_val: 0.3500 time: 0.0049s\n",
            "Epoch: 0027 loss_train: 1.7299 acc_train: 0.3000 loss_val: 1.7396 acc_val: 0.3500 time: 0.0048s\n",
            "Epoch: 0028 loss_train: 1.7386 acc_train: 0.2929 loss_val: 1.7324 acc_val: 0.3500 time: 0.0049s\n",
            "Epoch: 0029 loss_train: 1.7087 acc_train: 0.2929 loss_val: 1.7253 acc_val: 0.3500 time: 0.0050s\n",
            "Epoch: 0030 loss_train: 1.7151 acc_train: 0.2857 loss_val: 1.7183 acc_val: 0.3500 time: 0.0051s\n",
            "Epoch: 0031 loss_train: 1.7244 acc_train: 0.2857 loss_val: 1.7113 acc_val: 0.3500 time: 0.0048s\n",
            "Epoch: 0032 loss_train: 1.6875 acc_train: 0.2929 loss_val: 1.7043 acc_val: 0.3500 time: 0.0048s\n",
            "Epoch: 0033 loss_train: 1.7035 acc_train: 0.2929 loss_val: 1.6974 acc_val: 0.3500 time: 0.0049s\n",
            "Epoch: 0034 loss_train: 1.6806 acc_train: 0.2929 loss_val: 1.6904 acc_val: 0.3500 time: 0.0048s\n",
            "Epoch: 0035 loss_train: 1.6517 acc_train: 0.3071 loss_val: 1.6836 acc_val: 0.3500 time: 0.0049s\n",
            "Epoch: 0036 loss_train: 1.6920 acc_train: 0.3000 loss_val: 1.6767 acc_val: 0.3467 time: 0.0048s\n",
            "Epoch: 0037 loss_train: 1.6515 acc_train: 0.3071 loss_val: 1.6697 acc_val: 0.3467 time: 0.0048s\n",
            "Epoch: 0038 loss_train: 1.6424 acc_train: 0.3214 loss_val: 1.6624 acc_val: 0.3467 time: 0.0049s\n",
            "Epoch: 0039 loss_train: 1.6259 acc_train: 0.3214 loss_val: 1.6548 acc_val: 0.3533 time: 0.0050s\n",
            "Epoch: 0040 loss_train: 1.6170 acc_train: 0.3143 loss_val: 1.6466 acc_val: 0.3633 time: 0.0048s\n",
            "Epoch: 0041 loss_train: 1.6100 acc_train: 0.3143 loss_val: 1.6381 acc_val: 0.3700 time: 0.0050s\n",
            "Epoch: 0042 loss_train: 1.5904 acc_train: 0.3357 loss_val: 1.6292 acc_val: 0.3700 time: 0.0048s\n",
            "Epoch: 0043 loss_train: 1.5826 acc_train: 0.3929 loss_val: 1.6199 acc_val: 0.3733 time: 0.0049s\n",
            "Epoch: 0044 loss_train: 1.5693 acc_train: 0.3643 loss_val: 1.6103 acc_val: 0.3867 time: 0.0048s\n",
            "Epoch: 0045 loss_train: 1.5319 acc_train: 0.3786 loss_val: 1.6003 acc_val: 0.4000 time: 0.0049s\n",
            "Epoch: 0046 loss_train: 1.5328 acc_train: 0.3714 loss_val: 1.5902 acc_val: 0.4033 time: 0.0048s\n",
            "Epoch: 0047 loss_train: 1.5419 acc_train: 0.3500 loss_val: 1.5798 acc_val: 0.4167 time: 0.0050s\n",
            "Epoch: 0048 loss_train: 1.5100 acc_train: 0.3857 loss_val: 1.5690 acc_val: 0.4233 time: 0.0049s\n",
            "Epoch: 0049 loss_train: 1.4935 acc_train: 0.4000 loss_val: 1.5576 acc_val: 0.4267 time: 0.0049s\n",
            "Epoch: 0050 loss_train: 1.4904 acc_train: 0.4000 loss_val: 1.5460 acc_val: 0.4300 time: 0.0068s\n",
            "Epoch: 0051 loss_train: 1.4734 acc_train: 0.4429 loss_val: 1.5341 acc_val: 0.4300 time: 0.0050s\n",
            "Epoch: 0052 loss_train: 1.4508 acc_train: 0.4786 loss_val: 1.5222 acc_val: 0.4367 time: 0.0045s\n",
            "Epoch: 0053 loss_train: 1.4069 acc_train: 0.4714 loss_val: 1.5100 acc_val: 0.4533 time: 0.0044s\n",
            "Epoch: 0054 loss_train: 1.4376 acc_train: 0.5143 loss_val: 1.4976 acc_val: 0.4633 time: 0.0045s\n",
            "Epoch: 0055 loss_train: 1.4093 acc_train: 0.5000 loss_val: 1.4848 acc_val: 0.4667 time: 0.0044s\n",
            "Epoch: 0056 loss_train: 1.3561 acc_train: 0.5500 loss_val: 1.4720 acc_val: 0.4833 time: 0.0044s\n",
            "Epoch: 0057 loss_train: 1.3612 acc_train: 0.5714 loss_val: 1.4592 acc_val: 0.4900 time: 0.0045s\n",
            "Epoch: 0058 loss_train: 1.3415 acc_train: 0.5714 loss_val: 1.4464 acc_val: 0.5000 time: 0.0047s\n",
            "Epoch: 0059 loss_train: 1.3256 acc_train: 0.5857 loss_val: 1.4334 acc_val: 0.5167 time: 0.0045s\n",
            "Epoch: 0060 loss_train: 1.2908 acc_train: 0.6286 loss_val: 1.4212 acc_val: 0.5500 time: 0.0046s\n",
            "Epoch: 0061 loss_train: 1.3120 acc_train: 0.6286 loss_val: 1.4080 acc_val: 0.5700 time: 0.0050s\n",
            "Epoch: 0062 loss_train: 1.3089 acc_train: 0.6286 loss_val: 1.3942 acc_val: 0.5933 time: 0.0069s\n",
            "Epoch: 0063 loss_train: 1.2762 acc_train: 0.6929 loss_val: 1.3807 acc_val: 0.6100 time: 0.0059s\n",
            "Epoch: 0064 loss_train: 1.2529 acc_train: 0.6500 loss_val: 1.3672 acc_val: 0.6400 time: 0.0049s\n",
            "Epoch: 0065 loss_train: 1.2688 acc_train: 0.6857 loss_val: 1.3535 acc_val: 0.6533 time: 0.0044s\n",
            "Epoch: 0066 loss_train: 1.2207 acc_train: 0.7071 loss_val: 1.3396 acc_val: 0.6533 time: 0.0043s\n",
            "Epoch: 0067 loss_train: 1.1992 acc_train: 0.7286 loss_val: 1.3258 acc_val: 0.6700 time: 0.0044s\n",
            "Epoch: 0068 loss_train: 1.1721 acc_train: 0.7143 loss_val: 1.3120 acc_val: 0.6767 time: 0.0044s\n",
            "Epoch: 0069 loss_train: 1.1501 acc_train: 0.7071 loss_val: 1.2985 acc_val: 0.6967 time: 0.0043s\n",
            "Epoch: 0070 loss_train: 1.1207 acc_train: 0.7643 loss_val: 1.2850 acc_val: 0.7067 time: 0.0045s\n",
            "Epoch: 0071 loss_train: 1.1046 acc_train: 0.7857 loss_val: 1.2719 acc_val: 0.7167 time: 0.0044s\n",
            "Epoch: 0072 loss_train: 1.1160 acc_train: 0.7571 loss_val: 1.2587 acc_val: 0.7300 time: 0.0044s\n",
            "Epoch: 0073 loss_train: 1.0850 acc_train: 0.7929 loss_val: 1.2461 acc_val: 0.7400 time: 0.0043s\n",
            "Epoch: 0074 loss_train: 1.0845 acc_train: 0.7786 loss_val: 1.2331 acc_val: 0.7433 time: 0.0043s\n",
            "Epoch: 0075 loss_train: 0.9965 acc_train: 0.8143 loss_val: 1.2196 acc_val: 0.7433 time: 0.0043s\n",
            "Epoch: 0076 loss_train: 1.0635 acc_train: 0.7857 loss_val: 1.2057 acc_val: 0.7467 time: 0.0044s\n",
            "Epoch: 0077 loss_train: 0.9977 acc_train: 0.7929 loss_val: 1.1920 acc_val: 0.7467 time: 0.0044s\n",
            "Epoch: 0078 loss_train: 0.9769 acc_train: 0.8071 loss_val: 1.1787 acc_val: 0.7467 time: 0.0043s\n",
            "Epoch: 0079 loss_train: 1.0027 acc_train: 0.8429 loss_val: 1.1657 acc_val: 0.7500 time: 0.0054s\n",
            "Epoch: 0080 loss_train: 0.9721 acc_train: 0.7786 loss_val: 1.1535 acc_val: 0.7600 time: 0.0050s\n",
            "Epoch: 0081 loss_train: 0.9782 acc_train: 0.8071 loss_val: 1.1418 acc_val: 0.7667 time: 0.0049s\n",
            "Epoch: 0082 loss_train: 0.9800 acc_train: 0.7571 loss_val: 1.1310 acc_val: 0.7700 time: 0.0044s\n",
            "Epoch: 0083 loss_train: 0.9805 acc_train: 0.8071 loss_val: 1.1211 acc_val: 0.7767 time: 0.0043s\n",
            "Epoch: 0084 loss_train: 0.9254 acc_train: 0.8286 loss_val: 1.1113 acc_val: 0.7800 time: 0.0044s\n",
            "Epoch: 0085 loss_train: 0.9044 acc_train: 0.8143 loss_val: 1.1012 acc_val: 0.7900 time: 0.0044s\n",
            "Epoch: 0086 loss_train: 0.8861 acc_train: 0.8357 loss_val: 1.0906 acc_val: 0.7967 time: 0.0044s\n",
            "Epoch: 0087 loss_train: 0.9177 acc_train: 0.8286 loss_val: 1.0803 acc_val: 0.7967 time: 0.0043s\n",
            "Epoch: 0088 loss_train: 0.8701 acc_train: 0.8357 loss_val: 1.0704 acc_val: 0.7933 time: 0.0044s\n",
            "Epoch: 0089 loss_train: 0.8671 acc_train: 0.8000 loss_val: 1.0597 acc_val: 0.7933 time: 0.0044s\n",
            "Epoch: 0090 loss_train: 0.8522 acc_train: 0.8500 loss_val: 1.0491 acc_val: 0.7933 time: 0.0045s\n",
            "Epoch: 0091 loss_train: 0.8582 acc_train: 0.8286 loss_val: 1.0378 acc_val: 0.7900 time: 0.0097s\n",
            "Epoch: 0092 loss_train: 0.8574 acc_train: 0.8071 loss_val: 1.0283 acc_val: 0.7900 time: 0.0059s\n",
            "Epoch: 0093 loss_train: 0.7840 acc_train: 0.8571 loss_val: 1.0194 acc_val: 0.7933 time: 0.0054s\n",
            "Epoch: 0094 loss_train: 0.8028 acc_train: 0.8500 loss_val: 1.0119 acc_val: 0.7933 time: 0.0044s\n",
            "Epoch: 0095 loss_train: 0.8181 acc_train: 0.8214 loss_val: 1.0045 acc_val: 0.7900 time: 0.0061s\n",
            "Epoch: 0096 loss_train: 0.8157 acc_train: 0.8500 loss_val: 0.9964 acc_val: 0.7900 time: 0.0056s\n",
            "Epoch: 0097 loss_train: 0.8059 acc_train: 0.8500 loss_val: 0.9892 acc_val: 0.7900 time: 0.0055s\n",
            "Epoch: 0098 loss_train: 0.7771 acc_train: 0.8643 loss_val: 0.9820 acc_val: 0.7967 time: 0.0042s\n",
            "Epoch: 0099 loss_train: 0.7567 acc_train: 0.8786 loss_val: 0.9751 acc_val: 0.7967 time: 0.0046s\n",
            "Epoch: 0100 loss_train: 0.7496 acc_train: 0.8643 loss_val: 0.9676 acc_val: 0.8033 time: 0.0043s\n",
            "Epoch: 0101 loss_train: 0.7487 acc_train: 0.8429 loss_val: 0.9601 acc_val: 0.8033 time: 0.0044s\n",
            "Epoch: 0102 loss_train: 0.7655 acc_train: 0.8286 loss_val: 0.9532 acc_val: 0.8033 time: 0.0045s\n",
            "Epoch: 0103 loss_train: 0.7512 acc_train: 0.8571 loss_val: 0.9453 acc_val: 0.8000 time: 0.0043s\n",
            "Epoch: 0104 loss_train: 0.6837 acc_train: 0.8786 loss_val: 0.9373 acc_val: 0.7967 time: 0.0042s\n",
            "Epoch: 0105 loss_train: 0.7067 acc_train: 0.8643 loss_val: 0.9297 acc_val: 0.8000 time: 0.0042s\n",
            "Epoch: 0106 loss_train: 0.7346 acc_train: 0.8571 loss_val: 0.9237 acc_val: 0.7933 time: 0.0045s\n",
            "Epoch: 0107 loss_train: 0.7293 acc_train: 0.8357 loss_val: 0.9183 acc_val: 0.7867 time: 0.0049s\n",
            "Epoch: 0108 loss_train: 0.7124 acc_train: 0.8571 loss_val: 0.9124 acc_val: 0.7900 time: 0.0043s\n",
            "Epoch: 0109 loss_train: 0.6955 acc_train: 0.8643 loss_val: 0.9069 acc_val: 0.7900 time: 0.0042s\n",
            "Epoch: 0110 loss_train: 0.7155 acc_train: 0.8571 loss_val: 0.9023 acc_val: 0.7900 time: 0.0043s\n",
            "Epoch: 0111 loss_train: 0.6446 acc_train: 0.8786 loss_val: 0.8976 acc_val: 0.7900 time: 0.0042s\n",
            "Epoch: 0112 loss_train: 0.6381 acc_train: 0.8857 loss_val: 0.8931 acc_val: 0.7900 time: 0.0041s\n",
            "Epoch: 0113 loss_train: 0.6427 acc_train: 0.9000 loss_val: 0.8882 acc_val: 0.7900 time: 0.0045s\n",
            "Epoch: 0114 loss_train: 0.6583 acc_train: 0.8857 loss_val: 0.8838 acc_val: 0.7867 time: 0.0043s\n",
            "Epoch: 0115 loss_train: 0.6531 acc_train: 0.8714 loss_val: 0.8798 acc_val: 0.7900 time: 0.0042s\n",
            "Epoch: 0116 loss_train: 0.6031 acc_train: 0.9071 loss_val: 0.8759 acc_val: 0.7900 time: 0.0042s\n",
            "Epoch: 0117 loss_train: 0.6319 acc_train: 0.9000 loss_val: 0.8714 acc_val: 0.7933 time: 0.0044s\n",
            "Epoch: 0118 loss_train: 0.6046 acc_train: 0.9071 loss_val: 0.8663 acc_val: 0.7967 time: 0.0043s\n",
            "Epoch: 0119 loss_train: 0.6716 acc_train: 0.9000 loss_val: 0.8616 acc_val: 0.7967 time: 0.0043s\n",
            "Epoch: 0120 loss_train: 0.6226 acc_train: 0.8857 loss_val: 0.8570 acc_val: 0.7967 time: 0.0042s\n",
            "Epoch: 0121 loss_train: 0.6611 acc_train: 0.8714 loss_val: 0.8526 acc_val: 0.7967 time: 0.0041s\n",
            "Epoch: 0122 loss_train: 0.5739 acc_train: 0.8929 loss_val: 0.8481 acc_val: 0.7933 time: 0.0043s\n",
            "Epoch: 0123 loss_train: 0.5807 acc_train: 0.9071 loss_val: 0.8437 acc_val: 0.7900 time: 0.0042s\n",
            "Epoch: 0124 loss_train: 0.5861 acc_train: 0.9214 loss_val: 0.8390 acc_val: 0.7933 time: 0.0042s\n",
            "Epoch: 0125 loss_train: 0.5693 acc_train: 0.8857 loss_val: 0.8349 acc_val: 0.7833 time: 0.0042s\n",
            "Epoch: 0126 loss_train: 0.6334 acc_train: 0.8643 loss_val: 0.8316 acc_val: 0.7833 time: 0.0044s\n",
            "Epoch: 0127 loss_train: 0.5949 acc_train: 0.8571 loss_val: 0.8293 acc_val: 0.7833 time: 0.0042s\n",
            "Epoch: 0128 loss_train: 0.5788 acc_train: 0.9214 loss_val: 0.8256 acc_val: 0.7867 time: 0.0048s\n",
            "Epoch: 0129 loss_train: 0.5800 acc_train: 0.8643 loss_val: 0.8217 acc_val: 0.7867 time: 0.0044s\n",
            "Epoch: 0130 loss_train: 0.5509 acc_train: 0.9071 loss_val: 0.8174 acc_val: 0.7833 time: 0.0043s\n",
            "Epoch: 0131 loss_train: 0.5567 acc_train: 0.9071 loss_val: 0.8132 acc_val: 0.7867 time: 0.0042s\n",
            "Epoch: 0132 loss_train: 0.5728 acc_train: 0.9000 loss_val: 0.8088 acc_val: 0.7867 time: 0.0042s\n",
            "Epoch: 0133 loss_train: 0.5770 acc_train: 0.9000 loss_val: 0.8042 acc_val: 0.7867 time: 0.0042s\n",
            "Epoch: 0134 loss_train: 0.5730 acc_train: 0.9000 loss_val: 0.8003 acc_val: 0.8000 time: 0.0043s\n",
            "Epoch: 0135 loss_train: 0.5622 acc_train: 0.9143 loss_val: 0.7967 acc_val: 0.8067 time: 0.0041s\n",
            "Epoch: 0136 loss_train: 0.4883 acc_train: 0.9071 loss_val: 0.7929 acc_val: 0.8033 time: 0.0045s\n",
            "Epoch: 0137 loss_train: 0.5110 acc_train: 0.9143 loss_val: 0.7888 acc_val: 0.8033 time: 0.0043s\n",
            "Epoch: 0138 loss_train: 0.5115 acc_train: 0.9214 loss_val: 0.7849 acc_val: 0.7967 time: 0.0041s\n",
            "Epoch: 0139 loss_train: 0.5905 acc_train: 0.9000 loss_val: 0.7812 acc_val: 0.8067 time: 0.0041s\n",
            "Epoch: 0140 loss_train: 0.5357 acc_train: 0.9000 loss_val: 0.7782 acc_val: 0.8133 time: 0.0040s\n",
            "Epoch: 0141 loss_train: 0.5334 acc_train: 0.9071 loss_val: 0.7766 acc_val: 0.8100 time: 0.0040s\n",
            "Epoch: 0142 loss_train: 0.5271 acc_train: 0.9000 loss_val: 0.7756 acc_val: 0.8100 time: 0.0041s\n",
            "Epoch: 0143 loss_train: 0.5174 acc_train: 0.8857 loss_val: 0.7759 acc_val: 0.8100 time: 0.0042s\n",
            "Epoch: 0144 loss_train: 0.5939 acc_train: 0.9286 loss_val: 0.7773 acc_val: 0.8000 time: 0.0043s\n",
            "Epoch: 0145 loss_train: 0.5024 acc_train: 0.9000 loss_val: 0.7774 acc_val: 0.7967 time: 0.0041s\n",
            "Epoch: 0146 loss_train: 0.5185 acc_train: 0.9286 loss_val: 0.7756 acc_val: 0.7933 time: 0.0041s\n",
            "Epoch: 0147 loss_train: 0.5265 acc_train: 0.8929 loss_val: 0.7727 acc_val: 0.7967 time: 0.0040s\n",
            "Epoch: 0148 loss_train: 0.5244 acc_train: 0.9286 loss_val: 0.7691 acc_val: 0.7967 time: 0.0041s\n",
            "Epoch: 0149 loss_train: 0.5242 acc_train: 0.8929 loss_val: 0.7649 acc_val: 0.7933 time: 0.0041s\n",
            "Epoch: 0150 loss_train: 0.4922 acc_train: 0.9214 loss_val: 0.7614 acc_val: 0.7967 time: 0.0042s\n",
            "Epoch: 0151 loss_train: 0.4830 acc_train: 0.9143 loss_val: 0.7576 acc_val: 0.8033 time: 0.0040s\n",
            "Epoch: 0152 loss_train: 0.5327 acc_train: 0.9071 loss_val: 0.7545 acc_val: 0.8000 time: 0.0040s\n",
            "Epoch: 0153 loss_train: 0.5207 acc_train: 0.9071 loss_val: 0.7521 acc_val: 0.8067 time: 0.0042s\n",
            "Epoch: 0154 loss_train: 0.5076 acc_train: 0.9000 loss_val: 0.7502 acc_val: 0.8100 time: 0.0041s\n",
            "Epoch: 0155 loss_train: 0.5056 acc_train: 0.8929 loss_val: 0.7491 acc_val: 0.8067 time: 0.0041s\n",
            "Epoch: 0156 loss_train: 0.4681 acc_train: 0.9214 loss_val: 0.7479 acc_val: 0.8100 time: 0.0041s\n",
            "Epoch: 0157 loss_train: 0.4919 acc_train: 0.9286 loss_val: 0.7481 acc_val: 0.8100 time: 0.0041s\n",
            "Epoch: 0158 loss_train: 0.4993 acc_train: 0.9214 loss_val: 0.7482 acc_val: 0.8100 time: 0.0041s\n",
            "Epoch: 0159 loss_train: 0.5239 acc_train: 0.8786 loss_val: 0.7479 acc_val: 0.8100 time: 0.0040s\n",
            "Epoch: 0160 loss_train: 0.4868 acc_train: 0.9143 loss_val: 0.7482 acc_val: 0.8100 time: 0.0144s\n",
            "Epoch: 0161 loss_train: 0.4521 acc_train: 0.9571 loss_val: 0.7471 acc_val: 0.8067 time: 0.0042s\n",
            "Epoch: 0162 loss_train: 0.5105 acc_train: 0.8929 loss_val: 0.7450 acc_val: 0.8033 time: 0.0041s\n",
            "Epoch: 0163 loss_train: 0.5140 acc_train: 0.9000 loss_val: 0.7418 acc_val: 0.7967 time: 0.0040s\n",
            "Epoch: 0164 loss_train: 0.4637 acc_train: 0.9429 loss_val: 0.7383 acc_val: 0.7967 time: 0.0040s\n",
            "Epoch: 0165 loss_train: 0.4448 acc_train: 0.9357 loss_val: 0.7346 acc_val: 0.7967 time: 0.0040s\n",
            "Epoch: 0166 loss_train: 0.4632 acc_train: 0.9286 loss_val: 0.7324 acc_val: 0.7967 time: 0.0041s\n",
            "Epoch: 0167 loss_train: 0.4783 acc_train: 0.9000 loss_val: 0.7303 acc_val: 0.7967 time: 0.0041s\n",
            "Epoch: 0168 loss_train: 0.4544 acc_train: 0.9286 loss_val: 0.7287 acc_val: 0.8000 time: 0.0042s\n",
            "Epoch: 0169 loss_train: 0.4703 acc_train: 0.9000 loss_val: 0.7276 acc_val: 0.8000 time: 0.0042s\n",
            "Epoch: 0170 loss_train: 0.4730 acc_train: 0.9214 loss_val: 0.7270 acc_val: 0.8033 time: 0.0041s\n",
            "Epoch: 0171 loss_train: 0.4453 acc_train: 0.9143 loss_val: 0.7266 acc_val: 0.8067 time: 0.0041s\n",
            "Epoch: 0172 loss_train: 0.4497 acc_train: 0.9286 loss_val: 0.7268 acc_val: 0.8067 time: 0.0040s\n",
            "Epoch: 0173 loss_train: 0.4523 acc_train: 0.9286 loss_val: 0.7275 acc_val: 0.8100 time: 0.0040s\n",
            "Epoch: 0174 loss_train: 0.4482 acc_train: 0.9286 loss_val: 0.7274 acc_val: 0.8133 time: 0.0042s\n",
            "Epoch: 0175 loss_train: 0.4595 acc_train: 0.9286 loss_val: 0.7268 acc_val: 0.8167 time: 0.0041s\n",
            "Epoch: 0176 loss_train: 0.4861 acc_train: 0.9286 loss_val: 0.7270 acc_val: 0.8133 time: 0.0041s\n",
            "Epoch: 0177 loss_train: 0.4355 acc_train: 0.9214 loss_val: 0.7260 acc_val: 0.8133 time: 0.0041s\n",
            "Epoch: 0178 loss_train: 0.4313 acc_train: 0.9357 loss_val: 0.7242 acc_val: 0.8133 time: 0.0040s\n",
            "Epoch: 0179 loss_train: 0.4496 acc_train: 0.9429 loss_val: 0.7220 acc_val: 0.8167 time: 0.0040s\n",
            "Epoch: 0180 loss_train: 0.3954 acc_train: 0.9429 loss_val: 0.7200 acc_val: 0.8167 time: 0.0040s\n",
            "Epoch: 0181 loss_train: 0.3732 acc_train: 0.9500 loss_val: 0.7188 acc_val: 0.8100 time: 0.0043s\n",
            "Epoch: 0182 loss_train: 0.4720 acc_train: 0.9214 loss_val: 0.7179 acc_val: 0.8133 time: 0.0043s\n",
            "Epoch: 0183 loss_train: 0.4330 acc_train: 0.9214 loss_val: 0.7172 acc_val: 0.8133 time: 0.0044s\n",
            "Epoch: 0184 loss_train: 0.3912 acc_train: 0.9357 loss_val: 0.7168 acc_val: 0.8067 time: 0.0041s\n",
            "Epoch: 0185 loss_train: 0.4826 acc_train: 0.9286 loss_val: 0.7163 acc_val: 0.8067 time: 0.0041s\n",
            "Epoch: 0186 loss_train: 0.4015 acc_train: 0.9500 loss_val: 0.7148 acc_val: 0.8067 time: 0.0040s\n",
            "Epoch: 0187 loss_train: 0.4525 acc_train: 0.9429 loss_val: 0.7113 acc_val: 0.8100 time: 0.0040s\n",
            "Epoch: 0188 loss_train: 0.4430 acc_train: 0.9214 loss_val: 0.7092 acc_val: 0.8167 time: 0.0041s\n",
            "Epoch: 0189 loss_train: 0.4394 acc_train: 0.9429 loss_val: 0.7082 acc_val: 0.8200 time: 0.0041s\n",
            "Epoch: 0190 loss_train: 0.4242 acc_train: 0.9571 loss_val: 0.7076 acc_val: 0.8167 time: 0.0041s\n",
            "Epoch: 0191 loss_train: 0.4068 acc_train: 0.9357 loss_val: 0.7074 acc_val: 0.8267 time: 0.0041s\n",
            "Epoch: 0192 loss_train: 0.4056 acc_train: 0.9643 loss_val: 0.7072 acc_val: 0.8233 time: 0.0052s\n",
            "Epoch: 0193 loss_train: 0.4221 acc_train: 0.9071 loss_val: 0.7074 acc_val: 0.8167 time: 0.0057s\n",
            "Epoch: 0194 loss_train: 0.3790 acc_train: 0.9357 loss_val: 0.7090 acc_val: 0.8133 time: 0.0044s\n",
            "Epoch: 0195 loss_train: 0.3796 acc_train: 0.9500 loss_val: 0.7090 acc_val: 0.8133 time: 0.0041s\n",
            "Epoch: 0196 loss_train: 0.4123 acc_train: 0.9500 loss_val: 0.7093 acc_val: 0.8133 time: 0.0040s\n",
            "Epoch: 0197 loss_train: 0.4344 acc_train: 0.9357 loss_val: 0.7078 acc_val: 0.8133 time: 0.0040s\n",
            "Epoch: 0198 loss_train: 0.3815 acc_train: 0.9500 loss_val: 0.7049 acc_val: 0.8133 time: 0.0040s\n",
            "Epoch: 0199 loss_train: 0.3844 acc_train: 0.9429 loss_val: 0.7024 acc_val: 0.8167 time: 0.0040s\n",
            "Epoch: 0200 loss_train: 0.4214 acc_train: 0.9214 loss_val: 0.6984 acc_val: 0.8133 time: 0.0041s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 0.9355s\n",
            "Test set results: loss= 0.7214 accuracy= 0.8340\n",
            "------------------------------------------ learning rate 0.01 ^ ------------------------------------------\n",
            "Loading cora dataset...\n",
            "Epoch: 0001 loss_train: 1.9571 acc_train: 0.0714 loss_val: 1.9506 acc_val: 0.0667 time: 0.0104s\n",
            "Epoch: 0002 loss_train: 1.9595 acc_train: 0.0643 loss_val: 1.9492 acc_val: 0.0667 time: 0.0050s\n",
            "Epoch: 0003 loss_train: 1.9545 acc_train: 0.0571 loss_val: 1.9478 acc_val: 0.0633 time: 0.0047s\n",
            "Epoch: 0004 loss_train: 1.9550 acc_train: 0.0500 loss_val: 1.9464 acc_val: 0.0633 time: 0.0049s\n",
            "Epoch: 0005 loss_train: 1.9506 acc_train: 0.0857 loss_val: 1.9450 acc_val: 0.0633 time: 0.0048s\n",
            "Epoch: 0006 loss_train: 1.9525 acc_train: 0.0571 loss_val: 1.9436 acc_val: 0.0633 time: 0.0047s\n",
            "Epoch: 0007 loss_train: 1.9534 acc_train: 0.0714 loss_val: 1.9422 acc_val: 0.0733 time: 0.0047s\n",
            "Epoch: 0008 loss_train: 1.9503 acc_train: 0.0571 loss_val: 1.9409 acc_val: 0.0733 time: 0.0046s\n",
            "Epoch: 0009 loss_train: 1.9518 acc_train: 0.0714 loss_val: 1.9395 acc_val: 0.0733 time: 0.0049s\n",
            "Epoch: 0010 loss_train: 1.9469 acc_train: 0.0857 loss_val: 1.9382 acc_val: 0.0733 time: 0.0046s\n",
            "Epoch: 0011 loss_train: 1.9469 acc_train: 0.0929 loss_val: 1.9368 acc_val: 0.0733 time: 0.0047s\n",
            "Epoch: 0012 loss_train: 1.9518 acc_train: 0.0786 loss_val: 1.9354 acc_val: 0.0767 time: 0.0047s\n",
            "Epoch: 0013 loss_train: 1.9419 acc_train: 0.0929 loss_val: 1.9341 acc_val: 0.0800 time: 0.0053s\n",
            "Epoch: 0014 loss_train: 1.9341 acc_train: 0.0929 loss_val: 1.9328 acc_val: 0.0833 time: 0.0051s\n",
            "Epoch: 0015 loss_train: 1.9431 acc_train: 0.1000 loss_val: 1.9314 acc_val: 0.0833 time: 0.0047s\n",
            "Epoch: 0016 loss_train: 1.9340 acc_train: 0.1286 loss_val: 1.9301 acc_val: 0.0867 time: 0.0050s\n",
            "Epoch: 0017 loss_train: 1.9323 acc_train: 0.1571 loss_val: 1.9288 acc_val: 0.0833 time: 0.0054s\n",
            "Epoch: 0018 loss_train: 1.9336 acc_train: 0.1286 loss_val: 1.9274 acc_val: 0.0867 time: 0.0051s\n",
            "Epoch: 0019 loss_train: 1.9372 acc_train: 0.1429 loss_val: 1.9261 acc_val: 0.0933 time: 0.0048s\n",
            "Epoch: 0020 loss_train: 1.9281 acc_train: 0.1143 loss_val: 1.9247 acc_val: 0.0967 time: 0.0047s\n",
            "Epoch: 0021 loss_train: 1.9323 acc_train: 0.1571 loss_val: 1.9234 acc_val: 0.1067 time: 0.0049s\n",
            "Epoch: 0022 loss_train: 1.9299 acc_train: 0.1357 loss_val: 1.9221 acc_val: 0.1267 time: 0.0047s\n",
            "Epoch: 0023 loss_train: 1.9255 acc_train: 0.1786 loss_val: 1.9208 acc_val: 0.1967 time: 0.0049s\n",
            "Epoch: 0024 loss_train: 1.9288 acc_train: 0.1500 loss_val: 1.9195 acc_val: 0.2333 time: 0.0049s\n",
            "Epoch: 0025 loss_train: 1.9210 acc_train: 0.1857 loss_val: 1.9182 acc_val: 0.2567 time: 0.0049s\n",
            "Epoch: 0026 loss_train: 1.9310 acc_train: 0.1643 loss_val: 1.9169 acc_val: 0.2967 time: 0.0047s\n",
            "Epoch: 0027 loss_train: 1.9246 acc_train: 0.1929 loss_val: 1.9156 acc_val: 0.3300 time: 0.0047s\n",
            "Epoch: 0028 loss_train: 1.9188 acc_train: 0.2429 loss_val: 1.9143 acc_val: 0.3433 time: 0.0048s\n",
            "Epoch: 0029 loss_train: 1.9238 acc_train: 0.1929 loss_val: 1.9130 acc_val: 0.3367 time: 0.0050s\n",
            "Epoch: 0030 loss_train: 1.9201 acc_train: 0.2357 loss_val: 1.9117 acc_val: 0.3467 time: 0.0049s\n",
            "Epoch: 0031 loss_train: 1.9149 acc_train: 0.2500 loss_val: 1.9104 acc_val: 0.3467 time: 0.0048s\n",
            "Epoch: 0032 loss_train: 1.9142 acc_train: 0.2643 loss_val: 1.9092 acc_val: 0.3467 time: 0.0049s\n",
            "Epoch: 0033 loss_train: 1.9116 acc_train: 0.2714 loss_val: 1.9079 acc_val: 0.3500 time: 0.0051s\n",
            "Epoch: 0034 loss_train: 1.9150 acc_train: 0.2857 loss_val: 1.9066 acc_val: 0.3500 time: 0.0048s\n",
            "Epoch: 0035 loss_train: 1.9102 acc_train: 0.2857 loss_val: 1.9054 acc_val: 0.3500 time: 0.0047s\n",
            "Epoch: 0036 loss_train: 1.9171 acc_train: 0.2571 loss_val: 1.9041 acc_val: 0.3500 time: 0.0047s\n",
            "Epoch: 0037 loss_train: 1.9134 acc_train: 0.2929 loss_val: 1.9029 acc_val: 0.3500 time: 0.0050s\n",
            "Epoch: 0038 loss_train: 1.9069 acc_train: 0.2786 loss_val: 1.9016 acc_val: 0.3500 time: 0.0046s\n",
            "Epoch: 0039 loss_train: 1.9070 acc_train: 0.2714 loss_val: 1.9004 acc_val: 0.3500 time: 0.0044s\n",
            "Epoch: 0040 loss_train: 1.9084 acc_train: 0.2929 loss_val: 1.8992 acc_val: 0.3500 time: 0.0045s\n",
            "Epoch: 0041 loss_train: 1.8975 acc_train: 0.3000 loss_val: 1.8979 acc_val: 0.3500 time: 0.0045s\n",
            "Epoch: 0042 loss_train: 1.9079 acc_train: 0.2786 loss_val: 1.8967 acc_val: 0.3500 time: 0.0044s\n",
            "Epoch: 0043 loss_train: 1.8998 acc_train: 0.3071 loss_val: 1.8955 acc_val: 0.3500 time: 0.0043s\n",
            "Epoch: 0044 loss_train: 1.8963 acc_train: 0.2857 loss_val: 1.8943 acc_val: 0.3500 time: 0.0045s\n",
            "Epoch: 0045 loss_train: 1.8999 acc_train: 0.3000 loss_val: 1.8930 acc_val: 0.3500 time: 0.0044s\n",
            "Epoch: 0046 loss_train: 1.8999 acc_train: 0.2929 loss_val: 1.8918 acc_val: 0.3500 time: 0.0048s\n",
            "Epoch: 0047 loss_train: 1.8876 acc_train: 0.2929 loss_val: 1.8906 acc_val: 0.3500 time: 0.0044s\n",
            "Epoch: 0048 loss_train: 1.8929 acc_train: 0.2857 loss_val: 1.8894 acc_val: 0.3500 time: 0.0044s\n",
            "Epoch: 0049 loss_train: 1.8857 acc_train: 0.3000 loss_val: 1.8882 acc_val: 0.3500 time: 0.0044s\n",
            "Epoch: 0050 loss_train: 1.8956 acc_train: 0.2857 loss_val: 1.8870 acc_val: 0.3500 time: 0.0045s\n",
            "Epoch: 0051 loss_train: 1.8905 acc_train: 0.2929 loss_val: 1.8858 acc_val: 0.3500 time: 0.0043s\n",
            "Epoch: 0052 loss_train: 1.8912 acc_train: 0.2929 loss_val: 1.8846 acc_val: 0.3500 time: 0.0043s\n",
            "Epoch: 0053 loss_train: 1.8874 acc_train: 0.2857 loss_val: 1.8834 acc_val: 0.3500 time: 0.0043s\n",
            "Epoch: 0054 loss_train: 1.8859 acc_train: 0.2857 loss_val: 1.8823 acc_val: 0.3500 time: 0.0043s\n",
            "Epoch: 0055 loss_train: 1.8832 acc_train: 0.3000 loss_val: 1.8811 acc_val: 0.3500 time: 0.0046s\n",
            "Epoch: 0056 loss_train: 1.8843 acc_train: 0.2929 loss_val: 1.8800 acc_val: 0.3500 time: 0.0044s\n",
            "Epoch: 0057 loss_train: 1.8791 acc_train: 0.2929 loss_val: 1.8788 acc_val: 0.3500 time: 0.0044s\n",
            "Epoch: 0058 loss_train: 1.8784 acc_train: 0.2929 loss_val: 1.8777 acc_val: 0.3500 time: 0.0044s\n",
            "Epoch: 0059 loss_train: 1.8779 acc_train: 0.2929 loss_val: 1.8765 acc_val: 0.3500 time: 0.0065s\n",
            "Epoch: 0060 loss_train: 1.8847 acc_train: 0.2929 loss_val: 1.8754 acc_val: 0.3500 time: 0.0058s\n",
            "Epoch: 0061 loss_train: 1.8761 acc_train: 0.2857 loss_val: 1.8742 acc_val: 0.3500 time: 0.0044s\n",
            "Epoch: 0062 loss_train: 1.8772 acc_train: 0.2929 loss_val: 1.8731 acc_val: 0.3500 time: 0.0044s\n",
            "Epoch: 0063 loss_train: 1.8822 acc_train: 0.2929 loss_val: 1.8720 acc_val: 0.3500 time: 0.0046s\n",
            "Epoch: 0064 loss_train: 1.8790 acc_train: 0.2929 loss_val: 1.8708 acc_val: 0.3500 time: 0.0043s\n",
            "Epoch: 0065 loss_train: 1.8825 acc_train: 0.2929 loss_val: 1.8697 acc_val: 0.3500 time: 0.0044s\n",
            "Epoch: 0066 loss_train: 1.8671 acc_train: 0.2929 loss_val: 1.8686 acc_val: 0.3500 time: 0.0044s\n",
            "Epoch: 0067 loss_train: 1.8691 acc_train: 0.2929 loss_val: 1.8675 acc_val: 0.3500 time: 0.0045s\n",
            "Epoch: 0068 loss_train: 1.8647 acc_train: 0.2929 loss_val: 1.8664 acc_val: 0.3500 time: 0.0047s\n",
            "Epoch: 0069 loss_train: 1.8729 acc_train: 0.2929 loss_val: 1.8652 acc_val: 0.3500 time: 0.0045s\n",
            "Epoch: 0070 loss_train: 1.8708 acc_train: 0.2929 loss_val: 1.8641 acc_val: 0.3500 time: 0.0044s\n",
            "Epoch: 0071 loss_train: 1.8761 acc_train: 0.2929 loss_val: 1.8630 acc_val: 0.3500 time: 0.0044s\n",
            "Epoch: 0072 loss_train: 1.8588 acc_train: 0.2929 loss_val: 1.8620 acc_val: 0.3500 time: 0.0045s\n",
            "Epoch: 0073 loss_train: 1.8715 acc_train: 0.2929 loss_val: 1.8609 acc_val: 0.3500 time: 0.0043s\n",
            "Epoch: 0074 loss_train: 1.8690 acc_train: 0.2929 loss_val: 1.8598 acc_val: 0.3500 time: 0.0044s\n",
            "Epoch: 0075 loss_train: 1.8650 acc_train: 0.2929 loss_val: 1.8587 acc_val: 0.3500 time: 0.0044s\n",
            "Epoch: 0076 loss_train: 1.8566 acc_train: 0.2929 loss_val: 1.8576 acc_val: 0.3500 time: 0.0046s\n",
            "Epoch: 0077 loss_train: 1.8534 acc_train: 0.2929 loss_val: 1.8566 acc_val: 0.3500 time: 0.0045s\n",
            "Epoch: 0078 loss_train: 1.8594 acc_train: 0.2929 loss_val: 1.8555 acc_val: 0.3500 time: 0.0044s\n",
            "Epoch: 0079 loss_train: 1.8617 acc_train: 0.2929 loss_val: 1.8544 acc_val: 0.3500 time: 0.0044s\n",
            "Epoch: 0080 loss_train: 1.8576 acc_train: 0.2929 loss_val: 1.8533 acc_val: 0.3500 time: 0.0044s\n",
            "Epoch: 0081 loss_train: 1.8607 acc_train: 0.2929 loss_val: 1.8522 acc_val: 0.3500 time: 0.0051s\n",
            "Epoch: 0082 loss_train: 1.8546 acc_train: 0.2929 loss_val: 1.8512 acc_val: 0.3500 time: 0.0042s\n",
            "Epoch: 0083 loss_train: 1.8509 acc_train: 0.2929 loss_val: 1.8501 acc_val: 0.3500 time: 0.0042s\n",
            "Epoch: 0084 loss_train: 1.8561 acc_train: 0.2929 loss_val: 1.8491 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0085 loss_train: 1.8477 acc_train: 0.2929 loss_val: 1.8481 acc_val: 0.3500 time: 0.0042s\n",
            "Epoch: 0086 loss_train: 1.8485 acc_train: 0.2929 loss_val: 1.8470 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0087 loss_train: 1.8556 acc_train: 0.3000 loss_val: 1.8460 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0088 loss_train: 1.8615 acc_train: 0.2929 loss_val: 1.8449 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0089 loss_train: 1.8475 acc_train: 0.2929 loss_val: 1.8439 acc_val: 0.3500 time: 0.0042s\n",
            "Epoch: 0090 loss_train: 1.8467 acc_train: 0.2929 loss_val: 1.8429 acc_val: 0.3500 time: 0.0047s\n",
            "Epoch: 0091 loss_train: 1.8413 acc_train: 0.2929 loss_val: 1.8419 acc_val: 0.3500 time: 0.0044s\n",
            "Epoch: 0092 loss_train: 1.8471 acc_train: 0.2929 loss_val: 1.8408 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0093 loss_train: 1.8388 acc_train: 0.2929 loss_val: 1.8398 acc_val: 0.3500 time: 0.0042s\n",
            "Epoch: 0094 loss_train: 1.8388 acc_train: 0.2929 loss_val: 1.8388 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0095 loss_train: 1.8359 acc_train: 0.2929 loss_val: 1.8378 acc_val: 0.3500 time: 0.0043s\n",
            "Epoch: 0096 loss_train: 1.8294 acc_train: 0.2929 loss_val: 1.8367 acc_val: 0.3500 time: 0.0042s\n",
            "Epoch: 0097 loss_train: 1.8357 acc_train: 0.2929 loss_val: 1.8357 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0098 loss_train: 1.8376 acc_train: 0.2929 loss_val: 1.8346 acc_val: 0.3500 time: 0.0042s\n",
            "Epoch: 0099 loss_train: 1.8235 acc_train: 0.2929 loss_val: 1.8336 acc_val: 0.3500 time: 0.0042s\n",
            "Epoch: 0100 loss_train: 1.8333 acc_train: 0.2929 loss_val: 1.8326 acc_val: 0.3500 time: 0.0043s\n",
            "Epoch: 0101 loss_train: 1.8358 acc_train: 0.2929 loss_val: 1.8315 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0102 loss_train: 1.8224 acc_train: 0.2929 loss_val: 1.8305 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0103 loss_train: 1.8243 acc_train: 0.2929 loss_val: 1.8294 acc_val: 0.3500 time: 0.0043s\n",
            "Epoch: 0104 loss_train: 1.8419 acc_train: 0.2929 loss_val: 1.8284 acc_val: 0.3500 time: 0.0047s\n",
            "Epoch: 0105 loss_train: 1.8359 acc_train: 0.2929 loss_val: 1.8274 acc_val: 0.3500 time: 0.0042s\n",
            "Epoch: 0106 loss_train: 1.8269 acc_train: 0.2929 loss_val: 1.8264 acc_val: 0.3500 time: 0.0042s\n",
            "Epoch: 0107 loss_train: 1.8256 acc_train: 0.2929 loss_val: 1.8254 acc_val: 0.3500 time: 0.0043s\n",
            "Epoch: 0108 loss_train: 1.8236 acc_train: 0.2929 loss_val: 1.8243 acc_val: 0.3500 time: 0.0042s\n",
            "Epoch: 0109 loss_train: 1.8247 acc_train: 0.2929 loss_val: 1.8233 acc_val: 0.3500 time: 0.0043s\n",
            "Epoch: 0110 loss_train: 1.8190 acc_train: 0.2929 loss_val: 1.8223 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0111 loss_train: 1.8296 acc_train: 0.2929 loss_val: 1.8213 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0112 loss_train: 1.8203 acc_train: 0.3000 loss_val: 1.8202 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0113 loss_train: 1.8100 acc_train: 0.2929 loss_val: 1.8192 acc_val: 0.3500 time: 0.0043s\n",
            "Epoch: 0114 loss_train: 1.8238 acc_train: 0.2929 loss_val: 1.8182 acc_val: 0.3500 time: 0.0042s\n",
            "Epoch: 0115 loss_train: 1.8191 acc_train: 0.2929 loss_val: 1.8172 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0116 loss_train: 1.8335 acc_train: 0.2929 loss_val: 1.8162 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0117 loss_train: 1.8131 acc_train: 0.2929 loss_val: 1.8152 acc_val: 0.3500 time: 0.0042s\n",
            "Epoch: 0118 loss_train: 1.8115 acc_train: 0.2929 loss_val: 1.8142 acc_val: 0.3500 time: 0.0043s\n",
            "Epoch: 0119 loss_train: 1.8123 acc_train: 0.2929 loss_val: 1.8132 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0120 loss_train: 1.8174 acc_train: 0.2929 loss_val: 1.8122 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0121 loss_train: 1.8129 acc_train: 0.2929 loss_val: 1.8112 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0122 loss_train: 1.8109 acc_train: 0.2929 loss_val: 1.8102 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0123 loss_train: 1.8229 acc_train: 0.2929 loss_val: 1.8092 acc_val: 0.3500 time: 0.0049s\n",
            "Epoch: 0124 loss_train: 1.8060 acc_train: 0.2929 loss_val: 1.8082 acc_val: 0.3500 time: 0.0065s\n",
            "Epoch: 0125 loss_train: 1.8081 acc_train: 0.2929 loss_val: 1.8073 acc_val: 0.3500 time: 0.0055s\n",
            "Epoch: 0126 loss_train: 1.8095 acc_train: 0.2929 loss_val: 1.8063 acc_val: 0.3500 time: 0.0062s\n",
            "Epoch: 0127 loss_train: 1.8188 acc_train: 0.2929 loss_val: 1.8053 acc_val: 0.3500 time: 0.0068s\n",
            "Epoch: 0128 loss_train: 1.8051 acc_train: 0.2929 loss_val: 1.8044 acc_val: 0.3500 time: 0.0047s\n",
            "Epoch: 0129 loss_train: 1.8081 acc_train: 0.2929 loss_val: 1.8034 acc_val: 0.3500 time: 0.0052s\n",
            "Epoch: 0130 loss_train: 1.8025 acc_train: 0.2929 loss_val: 1.8024 acc_val: 0.3500 time: 0.0052s\n",
            "Epoch: 0131 loss_train: 1.8107 acc_train: 0.2929 loss_val: 1.8015 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0132 loss_train: 1.8039 acc_train: 0.2929 loss_val: 1.8005 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0133 loss_train: 1.7937 acc_train: 0.2929 loss_val: 1.7996 acc_val: 0.3500 time: 0.0042s\n",
            "Epoch: 0134 loss_train: 1.7995 acc_train: 0.3000 loss_val: 1.7986 acc_val: 0.3500 time: 0.0046s\n",
            "Epoch: 0135 loss_train: 1.7982 acc_train: 0.2929 loss_val: 1.7977 acc_val: 0.3500 time: 0.0044s\n",
            "Epoch: 0136 loss_train: 1.7960 acc_train: 0.2929 loss_val: 1.7968 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0137 loss_train: 1.8025 acc_train: 0.2929 loss_val: 1.7958 acc_val: 0.3500 time: 0.0042s\n",
            "Epoch: 0138 loss_train: 1.7905 acc_train: 0.2929 loss_val: 1.7949 acc_val: 0.3500 time: 0.0042s\n",
            "Epoch: 0139 loss_train: 1.7932 acc_train: 0.2929 loss_val: 1.7940 acc_val: 0.3500 time: 0.0044s\n",
            "Epoch: 0140 loss_train: 1.7859 acc_train: 0.2929 loss_val: 1.7930 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0141 loss_train: 1.7905 acc_train: 0.2929 loss_val: 1.7921 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0142 loss_train: 1.7833 acc_train: 0.2929 loss_val: 1.7912 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0143 loss_train: 1.7947 acc_train: 0.2929 loss_val: 1.7902 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0144 loss_train: 1.7796 acc_train: 0.2929 loss_val: 1.7893 acc_val: 0.3500 time: 0.0042s\n",
            "Epoch: 0145 loss_train: 1.8012 acc_train: 0.2929 loss_val: 1.7884 acc_val: 0.3500 time: 0.0052s\n",
            "Epoch: 0146 loss_train: 1.7767 acc_train: 0.2929 loss_val: 1.7874 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0147 loss_train: 1.7851 acc_train: 0.2929 loss_val: 1.7865 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0148 loss_train: 1.7864 acc_train: 0.3143 loss_val: 1.7856 acc_val: 0.3500 time: 0.0044s\n",
            "Epoch: 0149 loss_train: 1.7888 acc_train: 0.3000 loss_val: 1.7847 acc_val: 0.3500 time: 0.0044s\n",
            "Epoch: 0150 loss_train: 1.7927 acc_train: 0.2929 loss_val: 1.7838 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0151 loss_train: 1.7837 acc_train: 0.2929 loss_val: 1.7829 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0152 loss_train: 1.7836 acc_train: 0.2929 loss_val: 1.7820 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0153 loss_train: 1.7763 acc_train: 0.2929 loss_val: 1.7812 acc_val: 0.3500 time: 0.0042s\n",
            "Epoch: 0154 loss_train: 1.7886 acc_train: 0.2929 loss_val: 1.7803 acc_val: 0.3500 time: 0.0040s\n",
            "Epoch: 0155 loss_train: 1.7744 acc_train: 0.2929 loss_val: 1.7794 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0156 loss_train: 1.7811 acc_train: 0.2929 loss_val: 1.7786 acc_val: 0.3500 time: 0.0040s\n",
            "Epoch: 0157 loss_train: 1.7638 acc_train: 0.3000 loss_val: 1.7777 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0158 loss_train: 1.7715 acc_train: 0.2929 loss_val: 1.7768 acc_val: 0.3500 time: 0.0042s\n",
            "Epoch: 0159 loss_train: 1.7714 acc_train: 0.2929 loss_val: 1.7760 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0160 loss_train: 1.7686 acc_train: 0.2929 loss_val: 1.7751 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0161 loss_train: 1.7562 acc_train: 0.3071 loss_val: 1.7742 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0162 loss_train: 1.7623 acc_train: 0.3000 loss_val: 1.7734 acc_val: 0.3500 time: 0.0042s\n",
            "Epoch: 0163 loss_train: 1.7698 acc_train: 0.2929 loss_val: 1.7725 acc_val: 0.3500 time: 0.0042s\n",
            "Epoch: 0164 loss_train: 1.7643 acc_train: 0.2929 loss_val: 1.7716 acc_val: 0.3500 time: 0.0040s\n",
            "Epoch: 0165 loss_train: 1.7495 acc_train: 0.2929 loss_val: 1.7708 acc_val: 0.3500 time: 0.0040s\n",
            "Epoch: 0166 loss_train: 1.7665 acc_train: 0.2929 loss_val: 1.7699 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0167 loss_train: 1.7682 acc_train: 0.2929 loss_val: 1.7690 acc_val: 0.3500 time: 0.0043s\n",
            "Epoch: 0168 loss_train: 1.7619 acc_train: 0.3071 loss_val: 1.7682 acc_val: 0.3500 time: 0.0040s\n",
            "Epoch: 0169 loss_train: 1.7733 acc_train: 0.2929 loss_val: 1.7673 acc_val: 0.3500 time: 0.0043s\n",
            "Epoch: 0170 loss_train: 1.7737 acc_train: 0.3071 loss_val: 1.7665 acc_val: 0.3500 time: 0.0042s\n",
            "Epoch: 0171 loss_train: 1.7540 acc_train: 0.3000 loss_val: 1.7657 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0172 loss_train: 1.7647 acc_train: 0.2929 loss_val: 1.7649 acc_val: 0.3500 time: 0.0043s\n",
            "Epoch: 0173 loss_train: 1.7773 acc_train: 0.3000 loss_val: 1.7641 acc_val: 0.3500 time: 0.0061s\n",
            "Epoch: 0174 loss_train: 1.7612 acc_train: 0.2929 loss_val: 1.7633 acc_val: 0.3500 time: 0.0045s\n",
            "Epoch: 0175 loss_train: 1.7578 acc_train: 0.3071 loss_val: 1.7625 acc_val: 0.3500 time: 0.0043s\n",
            "Epoch: 0176 loss_train: 1.7436 acc_train: 0.2929 loss_val: 1.7617 acc_val: 0.3500 time: 0.0043s\n",
            "Epoch: 0177 loss_train: 1.7539 acc_train: 0.3000 loss_val: 1.7609 acc_val: 0.3500 time: 0.0040s\n",
            "Epoch: 0178 loss_train: 1.7571 acc_train: 0.3000 loss_val: 1.7601 acc_val: 0.3500 time: 0.0040s\n",
            "Epoch: 0179 loss_train: 1.7569 acc_train: 0.3000 loss_val: 1.7593 acc_val: 0.3500 time: 0.0040s\n",
            "Epoch: 0180 loss_train: 1.7480 acc_train: 0.2929 loss_val: 1.7585 acc_val: 0.3500 time: 0.0040s\n",
            "Epoch: 0181 loss_train: 1.7414 acc_train: 0.2929 loss_val: 1.7578 acc_val: 0.3500 time: 0.0042s\n",
            "Epoch: 0182 loss_train: 1.7494 acc_train: 0.2929 loss_val: 1.7570 acc_val: 0.3500 time: 0.0040s\n",
            "Epoch: 0183 loss_train: 1.7504 acc_train: 0.3071 loss_val: 1.7562 acc_val: 0.3500 time: 0.0040s\n",
            "Epoch: 0184 loss_train: 1.7471 acc_train: 0.3000 loss_val: 1.7554 acc_val: 0.3500 time: 0.0040s\n",
            "Epoch: 0185 loss_train: 1.7504 acc_train: 0.3000 loss_val: 1.7546 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0186 loss_train: 1.7298 acc_train: 0.3071 loss_val: 1.7538 acc_val: 0.3500 time: 0.0042s\n",
            "Epoch: 0187 loss_train: 1.7598 acc_train: 0.3000 loss_val: 1.7530 acc_val: 0.3500 time: 0.0040s\n",
            "Epoch: 0188 loss_train: 1.7636 acc_train: 0.2857 loss_val: 1.7522 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0189 loss_train: 1.7584 acc_train: 0.2929 loss_val: 1.7514 acc_val: 0.3500 time: 0.0040s\n",
            "Epoch: 0190 loss_train: 1.7352 acc_train: 0.3071 loss_val: 1.7506 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0191 loss_train: 1.7305 acc_train: 0.3071 loss_val: 1.7498 acc_val: 0.3500 time: 0.0043s\n",
            "Epoch: 0192 loss_train: 1.7274 acc_train: 0.3000 loss_val: 1.7490 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0193 loss_train: 1.7342 acc_train: 0.3000 loss_val: 1.7482 acc_val: 0.3500 time: 0.0040s\n",
            "Epoch: 0194 loss_train: 1.7303 acc_train: 0.3000 loss_val: 1.7474 acc_val: 0.3500 time: 0.0040s\n",
            "Epoch: 0195 loss_train: 1.7498 acc_train: 0.2929 loss_val: 1.7466 acc_val: 0.3500 time: 0.0041s\n",
            "Epoch: 0196 loss_train: 1.7280 acc_train: 0.3071 loss_val: 1.7457 acc_val: 0.3500 time: 0.0042s\n",
            "Epoch: 0197 loss_train: 1.7236 acc_train: 0.2929 loss_val: 1.7449 acc_val: 0.3500 time: 0.0042s\n",
            "Epoch: 0198 loss_train: 1.7399 acc_train: 0.2929 loss_val: 1.7441 acc_val: 0.3500 time: 0.0039s\n",
            "Epoch: 0199 loss_train: 1.7336 acc_train: 0.3071 loss_val: 1.7433 acc_val: 0.3500 time: 0.0040s\n",
            "Epoch: 0200 loss_train: 1.7289 acc_train: 0.3000 loss_val: 1.7425 acc_val: 0.3500 time: 0.0040s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 0.9054s\n",
            "Test set results: loss= 1.8231 accuracy= 0.3090\n",
            "------------------------------------------ learning rate 0.001 ^ ------------------------------------------\n",
            "Loading cora dataset...\n",
            "Epoch: 0001 loss_train: 1.9879 acc_train: 0.0857 loss_val: 2.0000 acc_val: 0.1533 time: 0.0109s\n",
            "Epoch: 0002 loss_train: 1.9820 acc_train: 0.1786 loss_val: 1.9881 acc_val: 0.1567 time: 0.0050s\n",
            "Epoch: 0003 loss_train: 1.9667 acc_train: 0.2000 loss_val: 1.9766 acc_val: 0.1567 time: 0.0047s\n",
            "Epoch: 0004 loss_train: 1.9610 acc_train: 0.2000 loss_val: 1.9655 acc_val: 0.1567 time: 0.0047s\n",
            "Epoch: 0005 loss_train: 1.9452 acc_train: 0.2000 loss_val: 1.9545 acc_val: 0.1567 time: 0.0048s\n",
            "Epoch: 0006 loss_train: 1.9345 acc_train: 0.2000 loss_val: 1.9440 acc_val: 0.1567 time: 0.0047s\n",
            "Epoch: 0007 loss_train: 1.9228 acc_train: 0.2000 loss_val: 1.9338 acc_val: 0.1567 time: 0.0048s\n",
            "Epoch: 0008 loss_train: 1.9075 acc_train: 0.2000 loss_val: 1.9238 acc_val: 0.1567 time: 0.0049s\n",
            "Epoch: 0009 loss_train: 1.9110 acc_train: 0.2000 loss_val: 1.9136 acc_val: 0.1567 time: 0.0047s\n",
            "Epoch: 0010 loss_train: 1.8988 acc_train: 0.2000 loss_val: 1.9033 acc_val: 0.1567 time: 0.0048s\n",
            "Epoch: 0011 loss_train: 1.8898 acc_train: 0.2000 loss_val: 1.8929 acc_val: 0.1567 time: 0.0047s\n",
            "Epoch: 0012 loss_train: 1.8728 acc_train: 0.2000 loss_val: 1.8823 acc_val: 0.1567 time: 0.0047s\n",
            "Epoch: 0013 loss_train: 1.8587 acc_train: 0.2000 loss_val: 1.8717 acc_val: 0.1567 time: 0.0047s\n",
            "Epoch: 0014 loss_train: 1.8497 acc_train: 0.2071 loss_val: 1.8612 acc_val: 0.1567 time: 0.0047s\n",
            "Epoch: 0015 loss_train: 1.8463 acc_train: 0.2071 loss_val: 1.8506 acc_val: 0.1567 time: 0.0047s\n",
            "Epoch: 0016 loss_train: 1.8496 acc_train: 0.2071 loss_val: 1.8403 acc_val: 0.1567 time: 0.0049s\n",
            "Epoch: 0017 loss_train: 1.8191 acc_train: 0.2071 loss_val: 1.8299 acc_val: 0.1567 time: 0.0050s\n",
            "Epoch: 0018 loss_train: 1.8006 acc_train: 0.2857 loss_val: 1.8193 acc_val: 0.1567 time: 0.0048s\n",
            "Epoch: 0019 loss_train: 1.7890 acc_train: 0.3286 loss_val: 1.8086 acc_val: 0.2067 time: 0.0047s\n",
            "Epoch: 0020 loss_train: 1.8115 acc_train: 0.2500 loss_val: 1.7981 acc_val: 0.3567 time: 0.0047s\n",
            "Epoch: 0021 loss_train: 1.7747 acc_train: 0.3429 loss_val: 1.7877 acc_val: 0.4267 time: 0.0049s\n",
            "Epoch: 0022 loss_train: 1.7754 acc_train: 0.3143 loss_val: 1.7776 acc_val: 0.4633 time: 0.0047s\n",
            "Epoch: 0023 loss_train: 1.7678 acc_train: 0.3429 loss_val: 1.7677 acc_val: 0.4567 time: 0.0047s\n",
            "Epoch: 0024 loss_train: 1.7572 acc_train: 0.3500 loss_val: 1.7582 acc_val: 0.4033 time: 0.0047s\n",
            "Epoch: 0025 loss_train: 1.7452 acc_train: 0.3286 loss_val: 1.7489 acc_val: 0.3767 time: 0.0047s\n",
            "Epoch: 0026 loss_train: 1.7156 acc_train: 0.4000 loss_val: 1.7399 acc_val: 0.3667 time: 0.0047s\n",
            "Epoch: 0027 loss_train: 1.7184 acc_train: 0.3500 loss_val: 1.7310 acc_val: 0.3667 time: 0.0048s\n",
            "Epoch: 0028 loss_train: 1.7210 acc_train: 0.3929 loss_val: 1.7225 acc_val: 0.3633 time: 0.0049s\n",
            "Epoch: 0029 loss_train: 1.6903 acc_train: 0.3643 loss_val: 1.7142 acc_val: 0.3633 time: 0.0047s\n",
            "Epoch: 0030 loss_train: 1.7082 acc_train: 0.3286 loss_val: 1.7061 acc_val: 0.3600 time: 0.0047s\n",
            "Epoch: 0031 loss_train: 1.7010 acc_train: 0.3500 loss_val: 1.6981 acc_val: 0.3533 time: 0.0047s\n",
            "Epoch: 0032 loss_train: 1.6664 acc_train: 0.3571 loss_val: 1.6902 acc_val: 0.3567 time: 0.0047s\n",
            "Epoch: 0033 loss_train: 1.6788 acc_train: 0.3857 loss_val: 1.6823 acc_val: 0.3600 time: 0.0047s\n",
            "Epoch: 0034 loss_train: 1.6513 acc_train: 0.3643 loss_val: 1.6744 acc_val: 0.3633 time: 0.0047s\n",
            "Epoch: 0035 loss_train: 1.6369 acc_train: 0.3429 loss_val: 1.6661 acc_val: 0.3600 time: 0.0047s\n",
            "Epoch: 0036 loss_train: 1.6730 acc_train: 0.3643 loss_val: 1.6578 acc_val: 0.3600 time: 0.0048s\n",
            "Epoch: 0037 loss_train: 1.6248 acc_train: 0.3714 loss_val: 1.6499 acc_val: 0.3633 time: 0.0048s\n",
            "Epoch: 0038 loss_train: 1.6277 acc_train: 0.3714 loss_val: 1.6420 acc_val: 0.3633 time: 0.0048s\n",
            "Epoch: 0039 loss_train: 1.6013 acc_train: 0.4000 loss_val: 1.6340 acc_val: 0.3733 time: 0.0047s\n",
            "Epoch: 0040 loss_train: 1.5777 acc_train: 0.4143 loss_val: 1.6257 acc_val: 0.3767 time: 0.0047s\n",
            "Epoch: 0041 loss_train: 1.5744 acc_train: 0.4000 loss_val: 1.6171 acc_val: 0.3867 time: 0.0047s\n",
            "Epoch: 0042 loss_train: 1.5453 acc_train: 0.4714 loss_val: 1.6084 acc_val: 0.4267 time: 0.0049s\n",
            "Epoch: 0043 loss_train: 1.5565 acc_train: 0.4714 loss_val: 1.5995 acc_val: 0.4633 time: 0.0047s\n",
            "Epoch: 0044 loss_train: 1.5327 acc_train: 0.4714 loss_val: 1.5905 acc_val: 0.4833 time: 0.0047s\n",
            "Epoch: 0045 loss_train: 1.4979 acc_train: 0.5214 loss_val: 1.5807 acc_val: 0.4967 time: 0.0047s\n",
            "Epoch: 0046 loss_train: 1.4881 acc_train: 0.5357 loss_val: 1.5702 acc_val: 0.5133 time: 0.0048s\n",
            "Epoch: 0047 loss_train: 1.4725 acc_train: 0.5357 loss_val: 1.5589 acc_val: 0.5400 time: 0.0048s\n",
            "Epoch: 0048 loss_train: 1.4642 acc_train: 0.5357 loss_val: 1.5471 acc_val: 0.5433 time: 0.0047s\n",
            "Epoch: 0049 loss_train: 1.4403 acc_train: 0.5571 loss_val: 1.5346 acc_val: 0.5433 time: 0.0048s\n",
            "Epoch: 0050 loss_train: 1.4215 acc_train: 0.5714 loss_val: 1.5222 acc_val: 0.5467 time: 0.0064s\n",
            "Epoch: 0051 loss_train: 1.4160 acc_train: 0.5786 loss_val: 1.5099 acc_val: 0.5600 time: 0.0054s\n",
            "Epoch: 0052 loss_train: 1.4257 acc_train: 0.5357 loss_val: 1.4978 acc_val: 0.5733 time: 0.0044s\n",
            "Epoch: 0053 loss_train: 1.3619 acc_train: 0.5786 loss_val: 1.4858 acc_val: 0.5900 time: 0.0044s\n",
            "Epoch: 0054 loss_train: 1.3652 acc_train: 0.5929 loss_val: 1.4737 acc_val: 0.5933 time: 0.0043s\n",
            "Epoch: 0055 loss_train: 1.3667 acc_train: 0.6286 loss_val: 1.4611 acc_val: 0.6033 time: 0.0043s\n",
            "Epoch: 0056 loss_train: 1.2986 acc_train: 0.6286 loss_val: 1.4481 acc_val: 0.6067 time: 0.0043s\n",
            "Epoch: 0057 loss_train: 1.3007 acc_train: 0.6571 loss_val: 1.4347 acc_val: 0.6133 time: 0.0043s\n",
            "Epoch: 0058 loss_train: 1.2940 acc_train: 0.6357 loss_val: 1.4211 acc_val: 0.6167 time: 0.0045s\n",
            "Epoch: 0059 loss_train: 1.2931 acc_train: 0.6571 loss_val: 1.4070 acc_val: 0.6200 time: 0.0043s\n",
            "Epoch: 0060 loss_train: 1.2461 acc_train: 0.6571 loss_val: 1.3932 acc_val: 0.6200 time: 0.0052s\n",
            "Epoch: 0061 loss_train: 1.2665 acc_train: 0.6857 loss_val: 1.3790 acc_val: 0.6167 time: 0.0043s\n",
            "Epoch: 0062 loss_train: 1.2874 acc_train: 0.6000 loss_val: 1.3650 acc_val: 0.6200 time: 0.0043s\n",
            "Epoch: 0063 loss_train: 1.2274 acc_train: 0.6714 loss_val: 1.3512 acc_val: 0.6233 time: 0.0043s\n",
            "Epoch: 0064 loss_train: 1.2160 acc_train: 0.6929 loss_val: 1.3378 acc_val: 0.6367 time: 0.0043s\n",
            "Epoch: 0065 loss_train: 1.1774 acc_train: 0.6714 loss_val: 1.3245 acc_val: 0.6433 time: 0.0043s\n",
            "Epoch: 0066 loss_train: 1.1645 acc_train: 0.7143 loss_val: 1.3115 acc_val: 0.6467 time: 0.0042s\n",
            "Epoch: 0067 loss_train: 1.1455 acc_train: 0.6643 loss_val: 1.2986 acc_val: 0.6633 time: 0.0043s\n",
            "Epoch: 0068 loss_train: 1.1141 acc_train: 0.7357 loss_val: 1.2854 acc_val: 0.6700 time: 0.0043s\n",
            "Epoch: 0069 loss_train: 1.1206 acc_train: 0.7286 loss_val: 1.2725 acc_val: 0.6700 time: 0.0042s\n",
            "Epoch: 0070 loss_train: 1.0962 acc_train: 0.7143 loss_val: 1.2599 acc_val: 0.6833 time: 0.0043s\n",
            "Epoch: 0071 loss_train: 1.0499 acc_train: 0.7357 loss_val: 1.2482 acc_val: 0.7067 time: 0.0045s\n",
            "Epoch: 0072 loss_train: 1.1032 acc_train: 0.7571 loss_val: 1.2364 acc_val: 0.7167 time: 0.0042s\n",
            "Epoch: 0073 loss_train: 1.0388 acc_train: 0.7571 loss_val: 1.2250 acc_val: 0.7400 time: 0.0043s\n",
            "Epoch: 0074 loss_train: 1.0474 acc_train: 0.7357 loss_val: 1.2129 acc_val: 0.7400 time: 0.0043s\n",
            "Epoch: 0075 loss_train: 1.0104 acc_train: 0.7786 loss_val: 1.2000 acc_val: 0.7533 time: 0.0043s\n",
            "Epoch: 0076 loss_train: 1.0036 acc_train: 0.7429 loss_val: 1.1864 acc_val: 0.7533 time: 0.0042s\n",
            "Epoch: 0077 loss_train: 0.9954 acc_train: 0.8071 loss_val: 1.1724 acc_val: 0.7567 time: 0.0043s\n",
            "Epoch: 0078 loss_train: 0.9536 acc_train: 0.8000 loss_val: 1.1591 acc_val: 0.7600 time: 0.0050s\n",
            "Epoch: 0079 loss_train: 0.9977 acc_train: 0.8000 loss_val: 1.1462 acc_val: 0.7600 time: 0.0052s\n",
            "Epoch: 0080 loss_train: 0.9221 acc_train: 0.8071 loss_val: 1.1343 acc_val: 0.7600 time: 0.0045s\n",
            "Epoch: 0081 loss_train: 0.9485 acc_train: 0.7929 loss_val: 1.1230 acc_val: 0.7633 time: 0.0043s\n",
            "Epoch: 0082 loss_train: 0.9380 acc_train: 0.8000 loss_val: 1.1127 acc_val: 0.7733 time: 0.0043s\n",
            "Epoch: 0083 loss_train: 0.8869 acc_train: 0.8286 loss_val: 1.1028 acc_val: 0.7767 time: 0.0043s\n",
            "Epoch: 0084 loss_train: 0.9243 acc_train: 0.8500 loss_val: 1.0931 acc_val: 0.7767 time: 0.0043s\n",
            "Epoch: 0085 loss_train: 0.9093 acc_train: 0.8071 loss_val: 1.0833 acc_val: 0.7767 time: 0.0042s\n",
            "Epoch: 0086 loss_train: 0.8912 acc_train: 0.8071 loss_val: 1.0731 acc_val: 0.7800 time: 0.0044s\n",
            "Epoch: 0087 loss_train: 0.9073 acc_train: 0.8000 loss_val: 1.0642 acc_val: 0.7833 time: 0.0042s\n",
            "Epoch: 0088 loss_train: 0.8650 acc_train: 0.8429 loss_val: 1.0558 acc_val: 0.7800 time: 0.0042s\n",
            "Epoch: 0089 loss_train: 0.8497 acc_train: 0.8143 loss_val: 1.0471 acc_val: 0.7800 time: 0.0042s\n",
            "Epoch: 0090 loss_train: 0.8241 acc_train: 0.8143 loss_val: 1.0381 acc_val: 0.7867 time: 0.0046s\n",
            "Epoch: 0091 loss_train: 0.8303 acc_train: 0.8500 loss_val: 1.0281 acc_val: 0.7867 time: 0.0043s\n",
            "Epoch: 0092 loss_train: 0.7851 acc_train: 0.8357 loss_val: 1.0186 acc_val: 0.7867 time: 0.0044s\n",
            "Epoch: 0093 loss_train: 0.8062 acc_train: 0.8500 loss_val: 1.0098 acc_val: 0.7900 time: 0.0045s\n",
            "Epoch: 0094 loss_train: 0.8101 acc_train: 0.8143 loss_val: 1.0011 acc_val: 0.7933 time: 0.0050s\n",
            "Epoch: 0095 loss_train: 0.8301 acc_train: 0.8286 loss_val: 0.9924 acc_val: 0.7867 time: 0.0042s\n",
            "Epoch: 0096 loss_train: 0.7909 acc_train: 0.8571 loss_val: 0.9838 acc_val: 0.7900 time: 0.0041s\n",
            "Epoch: 0097 loss_train: 0.7981 acc_train: 0.8786 loss_val: 0.9769 acc_val: 0.7900 time: 0.0043s\n",
            "Epoch: 0098 loss_train: 0.7324 acc_train: 0.8857 loss_val: 0.9703 acc_val: 0.7833 time: 0.0043s\n",
            "Epoch: 0099 loss_train: 0.7334 acc_train: 0.8571 loss_val: 0.9639 acc_val: 0.7833 time: 0.0042s\n",
            "Epoch: 0100 loss_train: 0.7515 acc_train: 0.8643 loss_val: 0.9566 acc_val: 0.7833 time: 0.0045s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 0.4713s\n",
            "Test set results: loss= 1.0632 accuracy= 0.7690\n",
            "------------------------------------------ epoch 100 ^ ------------------------------------------\n",
            "Loading cora dataset...\n",
            "Epoch: 0001 loss_train: 1.9223 acc_train: 0.1500 loss_val: 1.9013 acc_val: 0.3500 time: 0.0104s\n",
            "Epoch: 0002 loss_train: 1.9172 acc_train: 0.2857 loss_val: 1.8907 acc_val: 0.3500 time: 0.0050s\n",
            "Epoch: 0003 loss_train: 1.9022 acc_train: 0.3000 loss_val: 1.8803 acc_val: 0.3500 time: 0.0047s\n",
            "Epoch: 0004 loss_train: 1.8890 acc_train: 0.2929 loss_val: 1.8698 acc_val: 0.3500 time: 0.0047s\n",
            "Epoch: 0005 loss_train: 1.8756 acc_train: 0.2929 loss_val: 1.8592 acc_val: 0.3500 time: 0.0047s\n",
            "Epoch: 0006 loss_train: 1.8726 acc_train: 0.2929 loss_val: 1.8490 acc_val: 0.3500 time: 0.0048s\n",
            "Epoch: 0007 loss_train: 1.8650 acc_train: 0.2929 loss_val: 1.8392 acc_val: 0.3500 time: 0.0050s\n",
            "Epoch: 0008 loss_train: 1.8506 acc_train: 0.2929 loss_val: 1.8295 acc_val: 0.3500 time: 0.0047s\n",
            "Epoch: 0009 loss_train: 1.8393 acc_train: 0.2929 loss_val: 1.8200 acc_val: 0.3500 time: 0.0047s\n",
            "Epoch: 0010 loss_train: 1.8258 acc_train: 0.2929 loss_val: 1.8110 acc_val: 0.3500 time: 0.0047s\n",
            "Epoch: 0011 loss_train: 1.8217 acc_train: 0.2929 loss_val: 1.8025 acc_val: 0.3500 time: 0.0047s\n",
            "Epoch: 0012 loss_train: 1.7995 acc_train: 0.2929 loss_val: 1.7944 acc_val: 0.3500 time: 0.0047s\n",
            "Epoch: 0013 loss_train: 1.8012 acc_train: 0.2929 loss_val: 1.7868 acc_val: 0.3500 time: 0.0047s\n",
            "Epoch: 0014 loss_train: 1.7931 acc_train: 0.2929 loss_val: 1.7796 acc_val: 0.3500 time: 0.0047s\n",
            "Epoch: 0015 loss_train: 1.7761 acc_train: 0.2929 loss_val: 1.7728 acc_val: 0.3500 time: 0.0047s\n",
            "Epoch: 0016 loss_train: 1.7727 acc_train: 0.2929 loss_val: 1.7664 acc_val: 0.3500 time: 0.0048s\n",
            "Epoch: 0017 loss_train: 1.7523 acc_train: 0.2929 loss_val: 1.7604 acc_val: 0.3500 time: 0.0050s\n",
            "Epoch: 0018 loss_train: 1.7570 acc_train: 0.2929 loss_val: 1.7544 acc_val: 0.3500 time: 0.0050s\n",
            "Epoch: 0019 loss_train: 1.7594 acc_train: 0.2929 loss_val: 1.7486 acc_val: 0.3500 time: 0.0050s\n",
            "Epoch: 0020 loss_train: 1.7378 acc_train: 0.2929 loss_val: 1.7427 acc_val: 0.3500 time: 0.0050s\n",
            "Epoch: 0021 loss_train: 1.7489 acc_train: 0.2929 loss_val: 1.7368 acc_val: 0.3500 time: 0.0051s\n",
            "Epoch: 0022 loss_train: 1.7374 acc_train: 0.3000 loss_val: 1.7311 acc_val: 0.3500 time: 0.0049s\n",
            "Epoch: 0023 loss_train: 1.7346 acc_train: 0.3071 loss_val: 1.7253 acc_val: 0.3500 time: 0.0049s\n",
            "Epoch: 0024 loss_train: 1.7034 acc_train: 0.3071 loss_val: 1.7194 acc_val: 0.3500 time: 0.0050s\n",
            "Epoch: 0025 loss_train: 1.7292 acc_train: 0.2929 loss_val: 1.7135 acc_val: 0.3500 time: 0.0049s\n",
            "Epoch: 0026 loss_train: 1.6968 acc_train: 0.3000 loss_val: 1.7074 acc_val: 0.3500 time: 0.0051s\n",
            "Epoch: 0027 loss_train: 1.6938 acc_train: 0.3000 loss_val: 1.7008 acc_val: 0.3500 time: 0.0052s\n",
            "Epoch: 0028 loss_train: 1.6896 acc_train: 0.3143 loss_val: 1.6941 acc_val: 0.3500 time: 0.0048s\n",
            "Epoch: 0029 loss_train: 1.6510 acc_train: 0.3214 loss_val: 1.6872 acc_val: 0.3500 time: 0.0048s\n",
            "Epoch: 0030 loss_train: 1.6716 acc_train: 0.3143 loss_val: 1.6801 acc_val: 0.3500 time: 0.0048s\n",
            "Epoch: 0031 loss_train: 1.6538 acc_train: 0.3429 loss_val: 1.6731 acc_val: 0.3533 time: 0.0049s\n",
            "Epoch: 0032 loss_train: 1.6232 acc_train: 0.3429 loss_val: 1.6660 acc_val: 0.3567 time: 0.0048s\n",
            "Epoch: 0033 loss_train: 1.6393 acc_train: 0.3571 loss_val: 1.6587 acc_val: 0.3633 time: 0.0050s\n",
            "Epoch: 0034 loss_train: 1.6368 acc_train: 0.3286 loss_val: 1.6511 acc_val: 0.3633 time: 0.0048s\n",
            "Epoch: 0035 loss_train: 1.6035 acc_train: 0.3786 loss_val: 1.6433 acc_val: 0.3667 time: 0.0049s\n",
            "Epoch: 0036 loss_train: 1.6339 acc_train: 0.3786 loss_val: 1.6354 acc_val: 0.3700 time: 0.0049s\n",
            "Epoch: 0037 loss_train: 1.5994 acc_train: 0.3786 loss_val: 1.6273 acc_val: 0.3733 time: 0.0048s\n",
            "Epoch: 0038 loss_train: 1.5773 acc_train: 0.4000 loss_val: 1.6191 acc_val: 0.3800 time: 0.0048s\n",
            "Epoch: 0039 loss_train: 1.5607 acc_train: 0.4000 loss_val: 1.6103 acc_val: 0.3867 time: 0.0048s\n",
            "Epoch: 0040 loss_train: 1.5526 acc_train: 0.4357 loss_val: 1.6011 acc_val: 0.3933 time: 0.0047s\n",
            "Epoch: 0041 loss_train: 1.5538 acc_train: 0.4214 loss_val: 1.5914 acc_val: 0.4067 time: 0.0050s\n",
            "Epoch: 0042 loss_train: 1.5121 acc_train: 0.4643 loss_val: 1.5811 acc_val: 0.4200 time: 0.0049s\n",
            "Epoch: 0043 loss_train: 1.5039 acc_train: 0.4571 loss_val: 1.5705 acc_val: 0.4233 time: 0.0048s\n",
            "Epoch: 0044 loss_train: 1.4941 acc_train: 0.4357 loss_val: 1.5598 acc_val: 0.4300 time: 0.0048s\n",
            "Epoch: 0045 loss_train: 1.4871 acc_train: 0.4857 loss_val: 1.5491 acc_val: 0.4433 time: 0.0048s\n",
            "Epoch: 0046 loss_train: 1.4441 acc_train: 0.5143 loss_val: 1.5382 acc_val: 0.4500 time: 0.0046s\n",
            "Epoch: 0047 loss_train: 1.4465 acc_train: 0.4714 loss_val: 1.5274 acc_val: 0.4600 time: 0.0044s\n",
            "Epoch: 0048 loss_train: 1.4350 acc_train: 0.5000 loss_val: 1.5164 acc_val: 0.4800 time: 0.0062s\n",
            "Epoch: 0049 loss_train: 1.4184 acc_train: 0.5214 loss_val: 1.5050 acc_val: 0.4900 time: 0.0054s\n",
            "Epoch: 0050 loss_train: 1.4264 acc_train: 0.5429 loss_val: 1.4931 acc_val: 0.5033 time: 0.0044s\n",
            "Epoch: 0051 loss_train: 1.4156 acc_train: 0.5286 loss_val: 1.4811 acc_val: 0.5067 time: 0.0044s\n",
            "Epoch: 0052 loss_train: 1.3904 acc_train: 0.5786 loss_val: 1.4688 acc_val: 0.5067 time: 0.0042s\n",
            "Epoch: 0053 loss_train: 1.3395 acc_train: 0.5571 loss_val: 1.4565 acc_val: 0.5167 time: 0.0044s\n",
            "Epoch: 0054 loss_train: 1.3279 acc_train: 0.5500 loss_val: 1.4442 acc_val: 0.5167 time: 0.0043s\n",
            "Epoch: 0055 loss_train: 1.3334 acc_train: 0.5500 loss_val: 1.4319 acc_val: 0.5300 time: 0.0042s\n",
            "Epoch: 0056 loss_train: 1.3510 acc_train: 0.5643 loss_val: 1.4194 acc_val: 0.5333 time: 0.0044s\n",
            "Epoch: 0057 loss_train: 1.2789 acc_train: 0.5929 loss_val: 1.4074 acc_val: 0.5400 time: 0.0046s\n",
            "Epoch: 0058 loss_train: 1.2744 acc_train: 0.5929 loss_val: 1.3957 acc_val: 0.5467 time: 0.0044s\n",
            "Epoch: 0059 loss_train: 1.2847 acc_train: 0.6214 loss_val: 1.3841 acc_val: 0.5600 time: 0.0043s\n",
            "Epoch: 0060 loss_train: 1.2460 acc_train: 0.6071 loss_val: 1.3729 acc_val: 0.5733 time: 0.0043s\n",
            "Epoch: 0061 loss_train: 1.2747 acc_train: 0.5786 loss_val: 1.3615 acc_val: 0.5900 time: 0.0043s\n",
            "Epoch: 0062 loss_train: 1.2260 acc_train: 0.6143 loss_val: 1.3504 acc_val: 0.6033 time: 0.0044s\n",
            "Epoch: 0063 loss_train: 1.2002 acc_train: 0.6500 loss_val: 1.3394 acc_val: 0.6133 time: 0.0044s\n",
            "Epoch: 0064 loss_train: 1.2081 acc_train: 0.6643 loss_val: 1.3284 acc_val: 0.6200 time: 0.0043s\n",
            "Epoch: 0065 loss_train: 1.1957 acc_train: 0.7286 loss_val: 1.3174 acc_val: 0.6333 time: 0.0042s\n",
            "Epoch: 0066 loss_train: 1.1686 acc_train: 0.6857 loss_val: 1.3064 acc_val: 0.6433 time: 0.0044s\n",
            "Epoch: 0067 loss_train: 1.1506 acc_train: 0.7357 loss_val: 1.2956 acc_val: 0.6567 time: 0.0044s\n",
            "Epoch: 0068 loss_train: 1.1591 acc_train: 0.7214 loss_val: 1.2846 acc_val: 0.6700 time: 0.0044s\n",
            "Epoch: 0069 loss_train: 1.1320 acc_train: 0.7286 loss_val: 1.2732 acc_val: 0.6733 time: 0.0044s\n",
            "Epoch: 0070 loss_train: 1.1484 acc_train: 0.7071 loss_val: 1.2617 acc_val: 0.6767 time: 0.0045s\n",
            "Epoch: 0071 loss_train: 1.1271 acc_train: 0.7214 loss_val: 1.2507 acc_val: 0.6767 time: 0.0043s\n",
            "Epoch: 0072 loss_train: 1.0921 acc_train: 0.7143 loss_val: 1.2399 acc_val: 0.6833 time: 0.0043s\n",
            "Epoch: 0073 loss_train: 1.1129 acc_train: 0.7071 loss_val: 1.2292 acc_val: 0.6933 time: 0.0045s\n",
            "Epoch: 0074 loss_train: 1.0723 acc_train: 0.7643 loss_val: 1.2188 acc_val: 0.7033 time: 0.0043s\n",
            "Epoch: 0075 loss_train: 1.0590 acc_train: 0.7643 loss_val: 1.2088 acc_val: 0.7167 time: 0.0043s\n",
            "Epoch: 0076 loss_train: 1.0532 acc_train: 0.7857 loss_val: 1.1988 acc_val: 0.7233 time: 0.0043s\n",
            "Epoch: 0077 loss_train: 1.0318 acc_train: 0.7857 loss_val: 1.1883 acc_val: 0.7267 time: 0.0043s\n",
            "Epoch: 0078 loss_train: 0.9924 acc_train: 0.7500 loss_val: 1.1786 acc_val: 0.7300 time: 0.0043s\n",
            "Epoch: 0079 loss_train: 1.0147 acc_train: 0.7643 loss_val: 1.1681 acc_val: 0.7333 time: 0.0050s\n",
            "Epoch: 0080 loss_train: 0.9892 acc_train: 0.8143 loss_val: 1.1583 acc_val: 0.7333 time: 0.0043s\n",
            "Epoch: 0081 loss_train: 0.9758 acc_train: 0.7786 loss_val: 1.1490 acc_val: 0.7433 time: 0.0043s\n",
            "Epoch: 0082 loss_train: 1.0251 acc_train: 0.7714 loss_val: 1.1402 acc_val: 0.7533 time: 0.0042s\n",
            "Epoch: 0083 loss_train: 0.9265 acc_train: 0.8000 loss_val: 1.1314 acc_val: 0.7500 time: 0.0042s\n",
            "Epoch: 0084 loss_train: 0.9690 acc_train: 0.7929 loss_val: 1.1225 acc_val: 0.7500 time: 0.0042s\n",
            "Epoch: 0085 loss_train: 0.9320 acc_train: 0.8214 loss_val: 1.1143 acc_val: 0.7533 time: 0.0047s\n",
            "Epoch: 0086 loss_train: 0.9766 acc_train: 0.7429 loss_val: 1.1067 acc_val: 0.7567 time: 0.0043s\n",
            "Epoch: 0087 loss_train: 0.9428 acc_train: 0.8143 loss_val: 1.0996 acc_val: 0.7633 time: 0.0043s\n",
            "Epoch: 0088 loss_train: 0.9026 acc_train: 0.8143 loss_val: 1.0925 acc_val: 0.7700 time: 0.0042s\n",
            "Epoch: 0089 loss_train: 0.8936 acc_train: 0.8429 loss_val: 1.0857 acc_val: 0.7667 time: 0.0042s\n",
            "Epoch: 0090 loss_train: 0.9267 acc_train: 0.8071 loss_val: 1.0783 acc_val: 0.7667 time: 0.0044s\n",
            "Epoch: 0091 loss_train: 0.8727 acc_train: 0.8286 loss_val: 1.0703 acc_val: 0.7700 time: 0.0042s\n",
            "Epoch: 0092 loss_train: 0.8872 acc_train: 0.8143 loss_val: 1.0616 acc_val: 0.7800 time: 0.0042s\n",
            "Epoch: 0093 loss_train: 0.8417 acc_train: 0.8214 loss_val: 1.0535 acc_val: 0.7800 time: 0.0043s\n",
            "Epoch: 0094 loss_train: 0.8666 acc_train: 0.8357 loss_val: 1.0450 acc_val: 0.7833 time: 0.0057s\n",
            "Epoch: 0095 loss_train: 0.8526 acc_train: 0.8000 loss_val: 1.0364 acc_val: 0.7767 time: 0.0056s\n",
            "Epoch: 0096 loss_train: 0.8059 acc_train: 0.8286 loss_val: 1.0285 acc_val: 0.7800 time: 0.0049s\n",
            "Epoch: 0097 loss_train: 0.7980 acc_train: 0.8500 loss_val: 1.0209 acc_val: 0.7800 time: 0.0041s\n",
            "Epoch: 0098 loss_train: 0.8074 acc_train: 0.8143 loss_val: 1.0134 acc_val: 0.7867 time: 0.0042s\n",
            "Epoch: 0099 loss_train: 0.8339 acc_train: 0.8643 loss_val: 1.0063 acc_val: 0.7900 time: 0.0042s\n",
            "Epoch: 0100 loss_train: 0.7847 acc_train: 0.8571 loss_val: 0.9995 acc_val: 0.7933 time: 0.0041s\n",
            "Epoch: 0101 loss_train: 0.8004 acc_train: 0.8286 loss_val: 0.9936 acc_val: 0.8000 time: 0.0040s\n",
            "Epoch: 0102 loss_train: 0.7741 acc_train: 0.8357 loss_val: 0.9881 acc_val: 0.7967 time: 0.0052s\n",
            "Epoch: 0103 loss_train: 0.8105 acc_train: 0.8571 loss_val: 0.9826 acc_val: 0.8067 time: 0.0041s\n",
            "Epoch: 0104 loss_train: 0.7867 acc_train: 0.8357 loss_val: 0.9772 acc_val: 0.8033 time: 0.0041s\n",
            "Epoch: 0105 loss_train: 0.7867 acc_train: 0.8429 loss_val: 0.9720 acc_val: 0.8067 time: 0.0041s\n",
            "Epoch: 0106 loss_train: 0.8082 acc_train: 0.8214 loss_val: 0.9664 acc_val: 0.8067 time: 0.0040s\n",
            "Epoch: 0107 loss_train: 0.7198 acc_train: 0.8643 loss_val: 0.9608 acc_val: 0.8067 time: 0.0041s\n",
            "Epoch: 0108 loss_train: 0.7475 acc_train: 0.8571 loss_val: 0.9559 acc_val: 0.8067 time: 0.0042s\n",
            "Epoch: 0109 loss_train: 0.7865 acc_train: 0.8429 loss_val: 0.9516 acc_val: 0.8033 time: 0.0040s\n",
            "Epoch: 0110 loss_train: 0.7135 acc_train: 0.8571 loss_val: 0.9475 acc_val: 0.8000 time: 0.0042s\n",
            "Epoch: 0111 loss_train: 0.7339 acc_train: 0.8643 loss_val: 0.9428 acc_val: 0.8000 time: 0.0040s\n",
            "Epoch: 0112 loss_train: 0.7426 acc_train: 0.8429 loss_val: 0.9375 acc_val: 0.7967 time: 0.0040s\n",
            "Epoch: 0113 loss_train: 0.7524 acc_train: 0.8500 loss_val: 0.9309 acc_val: 0.7933 time: 0.0040s\n",
            "Epoch: 0114 loss_train: 0.7610 acc_train: 0.8357 loss_val: 0.9245 acc_val: 0.7933 time: 0.0040s\n",
            "Epoch: 0115 loss_train: 0.6985 acc_train: 0.8571 loss_val: 0.9185 acc_val: 0.8000 time: 0.0039s\n",
            "Epoch: 0116 loss_train: 0.7069 acc_train: 0.8714 loss_val: 0.9132 acc_val: 0.8033 time: 0.0046s\n",
            "Epoch: 0117 loss_train: 0.7165 acc_train: 0.8214 loss_val: 0.9087 acc_val: 0.8067 time: 0.0040s\n",
            "Epoch: 0118 loss_train: 0.6682 acc_train: 0.8929 loss_val: 0.9044 acc_val: 0.8067 time: 0.0041s\n",
            "Epoch: 0119 loss_train: 0.6847 acc_train: 0.8929 loss_val: 0.9007 acc_val: 0.8067 time: 0.0040s\n",
            "Epoch: 0120 loss_train: 0.6595 acc_train: 0.8786 loss_val: 0.8977 acc_val: 0.8067 time: 0.0040s\n",
            "Epoch: 0121 loss_train: 0.7109 acc_train: 0.9000 loss_val: 0.8950 acc_val: 0.8067 time: 0.0040s\n",
            "Epoch: 0122 loss_train: 0.6692 acc_train: 0.8929 loss_val: 0.8921 acc_val: 0.8000 time: 0.0040s\n",
            "Epoch: 0123 loss_train: 0.6948 acc_train: 0.8286 loss_val: 0.8886 acc_val: 0.8000 time: 0.0041s\n",
            "Epoch: 0124 loss_train: 0.6869 acc_train: 0.8643 loss_val: 0.8848 acc_val: 0.8000 time: 0.0040s\n",
            "Epoch: 0125 loss_train: 0.6635 acc_train: 0.8857 loss_val: 0.8806 acc_val: 0.8067 time: 0.0040s\n",
            "Epoch: 0126 loss_train: 0.6856 acc_train: 0.8571 loss_val: 0.8765 acc_val: 0.8100 time: 0.0041s\n",
            "Epoch: 0127 loss_train: 0.6489 acc_train: 0.8714 loss_val: 0.8720 acc_val: 0.8133 time: 0.0041s\n",
            "Epoch: 0128 loss_train: 0.6400 acc_train: 0.8857 loss_val: 0.8680 acc_val: 0.8133 time: 0.0044s\n",
            "Epoch: 0129 loss_train: 0.6247 acc_train: 0.8857 loss_val: 0.8642 acc_val: 0.8133 time: 0.0040s\n",
            "Epoch: 0130 loss_train: 0.6307 acc_train: 0.8571 loss_val: 0.8605 acc_val: 0.8167 time: 0.0041s\n",
            "Epoch: 0131 loss_train: 0.6085 acc_train: 0.8857 loss_val: 0.8573 acc_val: 0.8133 time: 0.0040s\n",
            "Epoch: 0132 loss_train: 0.6868 acc_train: 0.8500 loss_val: 0.8541 acc_val: 0.8200 time: 0.0041s\n",
            "Epoch: 0133 loss_train: 0.6933 acc_train: 0.8929 loss_val: 0.8518 acc_val: 0.8200 time: 0.0041s\n",
            "Epoch: 0134 loss_train: 0.6100 acc_train: 0.8714 loss_val: 0.8495 acc_val: 0.8233 time: 0.0040s\n",
            "Epoch: 0135 loss_train: 0.6173 acc_train: 0.8929 loss_val: 0.8475 acc_val: 0.8133 time: 0.0040s\n",
            "Epoch: 0136 loss_train: 0.5849 acc_train: 0.8857 loss_val: 0.8451 acc_val: 0.8133 time: 0.0040s\n",
            "Epoch: 0137 loss_train: 0.5933 acc_train: 0.9071 loss_val: 0.8423 acc_val: 0.8100 time: 0.0042s\n",
            "Epoch: 0138 loss_train: 0.6103 acc_train: 0.9143 loss_val: 0.8383 acc_val: 0.8133 time: 0.0041s\n",
            "Epoch: 0139 loss_train: 0.6221 acc_train: 0.8714 loss_val: 0.8337 acc_val: 0.8067 time: 0.0041s\n",
            "Epoch: 0140 loss_train: 0.6090 acc_train: 0.8786 loss_val: 0.8298 acc_val: 0.8067 time: 0.0043s\n",
            "Epoch: 0141 loss_train: 0.6147 acc_train: 0.8857 loss_val: 0.8265 acc_val: 0.8100 time: 0.0040s\n",
            "Epoch: 0142 loss_train: 0.5705 acc_train: 0.8714 loss_val: 0.8234 acc_val: 0.8067 time: 0.0040s\n",
            "Epoch: 0143 loss_train: 0.6019 acc_train: 0.9214 loss_val: 0.8210 acc_val: 0.8033 time: 0.0041s\n",
            "Epoch: 0144 loss_train: 0.5937 acc_train: 0.9071 loss_val: 0.8181 acc_val: 0.8033 time: 0.0040s\n",
            "Epoch: 0145 loss_train: 0.6110 acc_train: 0.8857 loss_val: 0.8147 acc_val: 0.8067 time: 0.0040s\n",
            "Epoch: 0146 loss_train: 0.5824 acc_train: 0.8786 loss_val: 0.8117 acc_val: 0.8133 time: 0.0040s\n",
            "Epoch: 0147 loss_train: 0.5728 acc_train: 0.8786 loss_val: 0.8092 acc_val: 0.8133 time: 0.0043s\n",
            "Epoch: 0148 loss_train: 0.5877 acc_train: 0.9000 loss_val: 0.8071 acc_val: 0.8100 time: 0.0055s\n",
            "Epoch: 0149 loss_train: 0.5290 acc_train: 0.9143 loss_val: 0.8050 acc_val: 0.8067 time: 0.0056s\n",
            "Epoch: 0150 loss_train: 0.5849 acc_train: 0.9143 loss_val: 0.8029 acc_val: 0.8033 time: 0.0048s\n",
            "Epoch: 0151 loss_train: 0.5906 acc_train: 0.8929 loss_val: 0.8006 acc_val: 0.8067 time: 0.0041s\n",
            "Epoch: 0152 loss_train: 0.5301 acc_train: 0.8929 loss_val: 0.7980 acc_val: 0.8067 time: 0.0040s\n",
            "Epoch: 0153 loss_train: 0.5479 acc_train: 0.8857 loss_val: 0.7960 acc_val: 0.8067 time: 0.0040s\n",
            "Epoch: 0154 loss_train: 0.5561 acc_train: 0.8857 loss_val: 0.7941 acc_val: 0.8067 time: 0.0040s\n",
            "Epoch: 0155 loss_train: 0.5703 acc_train: 0.9071 loss_val: 0.7919 acc_val: 0.8067 time: 0.0040s\n",
            "Epoch: 0156 loss_train: 0.5293 acc_train: 0.9071 loss_val: 0.7893 acc_val: 0.8067 time: 0.0046s\n",
            "Epoch: 0157 loss_train: 0.5561 acc_train: 0.9286 loss_val: 0.7865 acc_val: 0.8033 time: 0.0049s\n",
            "Epoch: 0158 loss_train: 0.5468 acc_train: 0.9143 loss_val: 0.7839 acc_val: 0.8033 time: 0.0043s\n",
            "Epoch: 0159 loss_train: 0.5487 acc_train: 0.9143 loss_val: 0.7816 acc_val: 0.8033 time: 0.0044s\n",
            "Epoch: 0160 loss_train: 0.5390 acc_train: 0.9000 loss_val: 0.7793 acc_val: 0.8000 time: 0.0041s\n",
            "Epoch: 0161 loss_train: 0.5520 acc_train: 0.8857 loss_val: 0.7775 acc_val: 0.7967 time: 0.0040s\n",
            "Epoch: 0162 loss_train: 0.5829 acc_train: 0.8714 loss_val: 0.7758 acc_val: 0.7967 time: 0.0039s\n",
            "Epoch: 0163 loss_train: 0.5514 acc_train: 0.9000 loss_val: 0.7737 acc_val: 0.7967 time: 0.0043s\n",
            "Epoch: 0164 loss_train: 0.5202 acc_train: 0.9143 loss_val: 0.7717 acc_val: 0.8067 time: 0.0069s\n",
            "Epoch: 0165 loss_train: 0.5469 acc_train: 0.9000 loss_val: 0.7701 acc_val: 0.8067 time: 0.0040s\n",
            "Epoch: 0166 loss_train: 0.5022 acc_train: 0.9500 loss_val: 0.7684 acc_val: 0.8067 time: 0.0040s\n",
            "Epoch: 0167 loss_train: 0.5253 acc_train: 0.9214 loss_val: 0.7666 acc_val: 0.8033 time: 0.0040s\n",
            "Epoch: 0168 loss_train: 0.5388 acc_train: 0.8857 loss_val: 0.7643 acc_val: 0.8067 time: 0.0041s\n",
            "Epoch: 0169 loss_train: 0.5055 acc_train: 0.9357 loss_val: 0.7621 acc_val: 0.8067 time: 0.0039s\n",
            "Epoch: 0170 loss_train: 0.5053 acc_train: 0.9143 loss_val: 0.7593 acc_val: 0.8067 time: 0.0040s\n",
            "Epoch: 0171 loss_train: 0.5287 acc_train: 0.9214 loss_val: 0.7570 acc_val: 0.8000 time: 0.0040s\n",
            "Epoch: 0172 loss_train: 0.5426 acc_train: 0.8929 loss_val: 0.7543 acc_val: 0.8000 time: 0.0041s\n",
            "Epoch: 0173 loss_train: 0.5131 acc_train: 0.9000 loss_val: 0.7518 acc_val: 0.8000 time: 0.0040s\n",
            "Epoch: 0174 loss_train: 0.5052 acc_train: 0.8929 loss_val: 0.7495 acc_val: 0.8067 time: 0.0040s\n",
            "Epoch: 0175 loss_train: 0.4172 acc_train: 0.9357 loss_val: 0.7474 acc_val: 0.8100 time: 0.0041s\n",
            "Epoch: 0176 loss_train: 0.4526 acc_train: 0.9357 loss_val: 0.7458 acc_val: 0.8067 time: 0.0040s\n",
            "Epoch: 0177 loss_train: 0.5061 acc_train: 0.9357 loss_val: 0.7431 acc_val: 0.8100 time: 0.0040s\n",
            "Epoch: 0178 loss_train: 0.4914 acc_train: 0.9071 loss_val: 0.7406 acc_val: 0.8067 time: 0.0039s\n",
            "Epoch: 0179 loss_train: 0.5019 acc_train: 0.9357 loss_val: 0.7384 acc_val: 0.8067 time: 0.0039s\n",
            "Epoch: 0180 loss_train: 0.5126 acc_train: 0.9214 loss_val: 0.7355 acc_val: 0.8100 time: 0.0039s\n",
            "Epoch: 0181 loss_train: 0.5250 acc_train: 0.9000 loss_val: 0.7327 acc_val: 0.8067 time: 0.0039s\n",
            "Epoch: 0182 loss_train: 0.4833 acc_train: 0.9071 loss_val: 0.7303 acc_val: 0.8100 time: 0.0040s\n",
            "Epoch: 0183 loss_train: 0.4462 acc_train: 0.9286 loss_val: 0.7289 acc_val: 0.8100 time: 0.0040s\n",
            "Epoch: 0184 loss_train: 0.4797 acc_train: 0.9429 loss_val: 0.7283 acc_val: 0.8133 time: 0.0052s\n",
            "Epoch: 0185 loss_train: 0.4981 acc_train: 0.9357 loss_val: 0.7278 acc_val: 0.8100 time: 0.0043s\n",
            "Epoch: 0186 loss_train: 0.4627 acc_train: 0.9214 loss_val: 0.7270 acc_val: 0.8133 time: 0.0040s\n",
            "Epoch: 0187 loss_train: 0.5390 acc_train: 0.9000 loss_val: 0.7259 acc_val: 0.8200 time: 0.0043s\n",
            "Epoch: 0188 loss_train: 0.4906 acc_train: 0.9071 loss_val: 0.7252 acc_val: 0.8200 time: 0.0040s\n",
            "Epoch: 0189 loss_train: 0.4597 acc_train: 0.9214 loss_val: 0.7237 acc_val: 0.8100 time: 0.0040s\n",
            "Epoch: 0190 loss_train: 0.5042 acc_train: 0.9000 loss_val: 0.7229 acc_val: 0.8133 time: 0.0040s\n",
            "Epoch: 0191 loss_train: 0.5147 acc_train: 0.9286 loss_val: 0.7223 acc_val: 0.8133 time: 0.0039s\n",
            "Epoch: 0192 loss_train: 0.4531 acc_train: 0.9357 loss_val: 0.7216 acc_val: 0.8167 time: 0.0040s\n",
            "Epoch: 0193 loss_train: 0.4518 acc_train: 0.9286 loss_val: 0.7213 acc_val: 0.8133 time: 0.0040s\n",
            "Epoch: 0194 loss_train: 0.4492 acc_train: 0.9286 loss_val: 0.7199 acc_val: 0.8133 time: 0.0040s\n",
            "Epoch: 0195 loss_train: 0.4500 acc_train: 0.9286 loss_val: 0.7187 acc_val: 0.8133 time: 0.0039s\n",
            "Epoch: 0196 loss_train: 0.4612 acc_train: 0.9286 loss_val: 0.7180 acc_val: 0.8067 time: 0.0039s\n",
            "Epoch: 0197 loss_train: 0.4360 acc_train: 0.9357 loss_val: 0.7164 acc_val: 0.8067 time: 0.0040s\n",
            "Epoch: 0198 loss_train: 0.4763 acc_train: 0.9286 loss_val: 0.7130 acc_val: 0.8100 time: 0.0039s\n",
            "Epoch: 0199 loss_train: 0.4158 acc_train: 0.9571 loss_val: 0.7100 acc_val: 0.8133 time: 0.0049s\n",
            "Epoch: 0200 loss_train: 0.4555 acc_train: 0.9214 loss_val: 0.7084 acc_val: 0.8233 time: 0.0048s\n",
            "Epoch: 0201 loss_train: 0.4299 acc_train: 0.9357 loss_val: 0.7070 acc_val: 0.8200 time: 0.0042s\n",
            "Epoch: 0202 loss_train: 0.4281 acc_train: 0.9357 loss_val: 0.7061 acc_val: 0.8300 time: 0.0040s\n",
            "Epoch: 0203 loss_train: 0.4453 acc_train: 0.9286 loss_val: 0.7051 acc_val: 0.8300 time: 0.0040s\n",
            "Epoch: 0204 loss_train: 0.4289 acc_train: 0.9357 loss_val: 0.7057 acc_val: 0.8167 time: 0.0040s\n",
            "Epoch: 0205 loss_train: 0.4697 acc_train: 0.9286 loss_val: 0.7075 acc_val: 0.8200 time: 0.0040s\n",
            "Epoch: 0206 loss_train: 0.4377 acc_train: 0.9286 loss_val: 0.7097 acc_val: 0.8133 time: 0.0039s\n",
            "Epoch: 0207 loss_train: 0.4016 acc_train: 0.9714 loss_val: 0.7114 acc_val: 0.8167 time: 0.0039s\n",
            "Epoch: 0208 loss_train: 0.4135 acc_train: 0.9500 loss_val: 0.7108 acc_val: 0.8167 time: 0.0039s\n",
            "Epoch: 0209 loss_train: 0.4413 acc_train: 0.9143 loss_val: 0.7085 acc_val: 0.8167 time: 0.0040s\n",
            "Epoch: 0210 loss_train: 0.3977 acc_train: 0.9286 loss_val: 0.7067 acc_val: 0.8100 time: 0.0041s\n",
            "Epoch: 0211 loss_train: 0.4610 acc_train: 0.9571 loss_val: 0.7034 acc_val: 0.8067 time: 0.0042s\n",
            "Epoch: 0212 loss_train: 0.4017 acc_train: 0.9571 loss_val: 0.6987 acc_val: 0.8067 time: 0.0040s\n",
            "Epoch: 0213 loss_train: 0.4254 acc_train: 0.9429 loss_val: 0.6952 acc_val: 0.8067 time: 0.0040s\n",
            "Epoch: 0214 loss_train: 0.4447 acc_train: 0.9214 loss_val: 0.6924 acc_val: 0.8133 time: 0.0041s\n",
            "Epoch: 0215 loss_train: 0.4030 acc_train: 0.9429 loss_val: 0.6902 acc_val: 0.8233 time: 0.0040s\n",
            "Epoch: 0216 loss_train: 0.3589 acc_train: 0.9357 loss_val: 0.6884 acc_val: 0.8233 time: 0.0039s\n",
            "Epoch: 0217 loss_train: 0.3939 acc_train: 0.9643 loss_val: 0.6878 acc_val: 0.8167 time: 0.0040s\n",
            "Epoch: 0218 loss_train: 0.3814 acc_train: 0.9429 loss_val: 0.6878 acc_val: 0.8267 time: 0.0040s\n",
            "Epoch: 0219 loss_train: 0.3996 acc_train: 0.9500 loss_val: 0.6881 acc_val: 0.8200 time: 0.0040s\n",
            "Epoch: 0220 loss_train: 0.4013 acc_train: 0.9429 loss_val: 0.6888 acc_val: 0.8100 time: 0.0040s\n",
            "Epoch: 0221 loss_train: 0.4166 acc_train: 0.9643 loss_val: 0.6889 acc_val: 0.8133 time: 0.0040s\n",
            "Epoch: 0222 loss_train: 0.4112 acc_train: 0.9429 loss_val: 0.6905 acc_val: 0.8167 time: 0.0040s\n",
            "Epoch: 0223 loss_train: 0.4297 acc_train: 0.9429 loss_val: 0.6904 acc_val: 0.8167 time: 0.0039s\n",
            "Epoch: 0224 loss_train: 0.4400 acc_train: 0.9500 loss_val: 0.6888 acc_val: 0.8100 time: 0.0040s\n",
            "Epoch: 0225 loss_train: 0.4145 acc_train: 0.9143 loss_val: 0.6867 acc_val: 0.8100 time: 0.0040s\n",
            "Epoch: 0226 loss_train: 0.4371 acc_train: 0.9286 loss_val: 0.6849 acc_val: 0.8167 time: 0.0039s\n",
            "Epoch: 0227 loss_train: 0.3875 acc_train: 0.9286 loss_val: 0.6825 acc_val: 0.8200 time: 0.0039s\n",
            "Epoch: 0228 loss_train: 0.4110 acc_train: 0.9286 loss_val: 0.6806 acc_val: 0.8133 time: 0.0039s\n",
            "Epoch: 0229 loss_train: 0.3672 acc_train: 0.9429 loss_val: 0.6790 acc_val: 0.8167 time: 0.0041s\n",
            "Epoch: 0230 loss_train: 0.3629 acc_train: 0.9571 loss_val: 0.6795 acc_val: 0.8167 time: 0.0040s\n",
            "Epoch: 0231 loss_train: 0.3509 acc_train: 0.9786 loss_val: 0.6816 acc_val: 0.8100 time: 0.0040s\n",
            "Epoch: 0232 loss_train: 0.3628 acc_train: 0.9500 loss_val: 0.6838 acc_val: 0.8067 time: 0.0041s\n",
            "Epoch: 0233 loss_train: 0.3738 acc_train: 0.9500 loss_val: 0.6820 acc_val: 0.8067 time: 0.0042s\n",
            "Epoch: 0234 loss_train: 0.4204 acc_train: 0.9286 loss_val: 0.6773 acc_val: 0.8133 time: 0.0040s\n",
            "Epoch: 0235 loss_train: 0.4144 acc_train: 0.9286 loss_val: 0.6730 acc_val: 0.8167 time: 0.0039s\n",
            "Epoch: 0236 loss_train: 0.3555 acc_train: 0.9429 loss_val: 0.6712 acc_val: 0.8200 time: 0.0051s\n",
            "Epoch: 0237 loss_train: 0.3972 acc_train: 0.9357 loss_val: 0.6706 acc_val: 0.8233 time: 0.0056s\n",
            "Epoch: 0238 loss_train: 0.3638 acc_train: 0.9429 loss_val: 0.6711 acc_val: 0.8167 time: 0.0044s\n",
            "Epoch: 0239 loss_train: 0.4000 acc_train: 0.9357 loss_val: 0.6728 acc_val: 0.8233 time: 0.0039s\n",
            "Epoch: 0240 loss_train: 0.4163 acc_train: 0.9286 loss_val: 0.6748 acc_val: 0.8167 time: 0.0039s\n",
            "Epoch: 0241 loss_train: 0.4155 acc_train: 0.9286 loss_val: 0.6765 acc_val: 0.8167 time: 0.0039s\n",
            "Epoch: 0242 loss_train: 0.3606 acc_train: 0.9643 loss_val: 0.6776 acc_val: 0.8200 time: 0.0040s\n",
            "Epoch: 0243 loss_train: 0.3119 acc_train: 0.9857 loss_val: 0.6772 acc_val: 0.8233 time: 0.0040s\n",
            "Epoch: 0244 loss_train: 0.3483 acc_train: 0.9571 loss_val: 0.6767 acc_val: 0.8200 time: 0.0040s\n",
            "Epoch: 0245 loss_train: 0.3830 acc_train: 0.9143 loss_val: 0.6751 acc_val: 0.8167 time: 0.0040s\n",
            "Epoch: 0246 loss_train: 0.4169 acc_train: 0.9500 loss_val: 0.6723 acc_val: 0.8167 time: 0.0041s\n",
            "Epoch: 0247 loss_train: 0.3627 acc_train: 0.9643 loss_val: 0.6696 acc_val: 0.8200 time: 0.0041s\n",
            "Epoch: 0248 loss_train: 0.3421 acc_train: 0.9429 loss_val: 0.6670 acc_val: 0.8133 time: 0.0041s\n",
            "Epoch: 0249 loss_train: 0.3726 acc_train: 0.9286 loss_val: 0.6649 acc_val: 0.8133 time: 0.0042s\n",
            "Epoch: 0250 loss_train: 0.3612 acc_train: 0.9357 loss_val: 0.6641 acc_val: 0.8133 time: 0.0042s\n",
            "Epoch: 0251 loss_train: 0.3841 acc_train: 0.9429 loss_val: 0.6644 acc_val: 0.8300 time: 0.0041s\n",
            "Epoch: 0252 loss_train: 0.3880 acc_train: 0.9429 loss_val: 0.6652 acc_val: 0.8233 time: 0.0039s\n",
            "Epoch: 0253 loss_train: 0.4061 acc_train: 0.9286 loss_val: 0.6667 acc_val: 0.8200 time: 0.0039s\n",
            "Epoch: 0254 loss_train: 0.3529 acc_train: 0.9286 loss_val: 0.6696 acc_val: 0.8133 time: 0.0039s\n",
            "Epoch: 0255 loss_train: 0.3993 acc_train: 0.9143 loss_val: 0.6737 acc_val: 0.8133 time: 0.0039s\n",
            "Epoch: 0256 loss_train: 0.4098 acc_train: 0.9571 loss_val: 0.6765 acc_val: 0.8133 time: 0.0040s\n",
            "Epoch: 0257 loss_train: 0.4069 acc_train: 0.9429 loss_val: 0.6781 acc_val: 0.8167 time: 0.0042s\n",
            "Epoch: 0258 loss_train: 0.3990 acc_train: 0.9214 loss_val: 0.6762 acc_val: 0.8133 time: 0.0040s\n",
            "Epoch: 0259 loss_train: 0.3838 acc_train: 0.9429 loss_val: 0.6718 acc_val: 0.8100 time: 0.0040s\n",
            "Epoch: 0260 loss_train: 0.3890 acc_train: 0.9143 loss_val: 0.6683 acc_val: 0.8133 time: 0.0045s\n",
            "Epoch: 0261 loss_train: 0.3450 acc_train: 0.9571 loss_val: 0.6657 acc_val: 0.8100 time: 0.0040s\n",
            "Epoch: 0262 loss_train: 0.3146 acc_train: 0.9714 loss_val: 0.6628 acc_val: 0.8200 time: 0.0039s\n",
            "Epoch: 0263 loss_train: 0.3468 acc_train: 0.9643 loss_val: 0.6611 acc_val: 0.8233 time: 0.0040s\n",
            "Epoch: 0264 loss_train: 0.3652 acc_train: 0.9286 loss_val: 0.6599 acc_val: 0.8233 time: 0.0039s\n",
            "Epoch: 0265 loss_train: 0.3393 acc_train: 0.9714 loss_val: 0.6592 acc_val: 0.8233 time: 0.0039s\n",
            "Epoch: 0266 loss_train: 0.3333 acc_train: 0.9500 loss_val: 0.6587 acc_val: 0.8267 time: 0.0041s\n",
            "Epoch: 0267 loss_train: 0.3148 acc_train: 0.9857 loss_val: 0.6589 acc_val: 0.8267 time: 0.0039s\n",
            "Epoch: 0268 loss_train: 0.3678 acc_train: 0.9357 loss_val: 0.6587 acc_val: 0.8233 time: 0.0039s\n",
            "Epoch: 0269 loss_train: 0.3959 acc_train: 0.9357 loss_val: 0.6596 acc_val: 0.8233 time: 0.0040s\n",
            "Epoch: 0270 loss_train: 0.3342 acc_train: 0.9643 loss_val: 0.6603 acc_val: 0.8200 time: 0.0040s\n",
            "Epoch: 0271 loss_train: 0.3246 acc_train: 0.9643 loss_val: 0.6607 acc_val: 0.8133 time: 0.0039s\n",
            "Epoch: 0272 loss_train: 0.3559 acc_train: 0.9357 loss_val: 0.6617 acc_val: 0.8133 time: 0.0040s\n",
            "Epoch: 0273 loss_train: 0.3047 acc_train: 0.9714 loss_val: 0.6624 acc_val: 0.8133 time: 0.0040s\n",
            "Epoch: 0274 loss_train: 0.3227 acc_train: 0.9714 loss_val: 0.6635 acc_val: 0.8100 time: 0.0039s\n",
            "Epoch: 0275 loss_train: 0.3803 acc_train: 0.9357 loss_val: 0.6635 acc_val: 0.8133 time: 0.0045s\n",
            "Epoch: 0276 loss_train: 0.4002 acc_train: 0.9500 loss_val: 0.6629 acc_val: 0.8133 time: 0.0049s\n",
            "Epoch: 0277 loss_train: 0.3399 acc_train: 0.9714 loss_val: 0.6616 acc_val: 0.8133 time: 0.0040s\n",
            "Epoch: 0278 loss_train: 0.3570 acc_train: 0.9714 loss_val: 0.6609 acc_val: 0.8167 time: 0.0040s\n",
            "Epoch: 0279 loss_train: 0.4005 acc_train: 0.9214 loss_val: 0.6601 acc_val: 0.8100 time: 0.0041s\n",
            "Epoch: 0280 loss_train: 0.3323 acc_train: 0.9643 loss_val: 0.6589 acc_val: 0.8100 time: 0.0040s\n",
            "Epoch: 0281 loss_train: 0.3552 acc_train: 0.9429 loss_val: 0.6583 acc_val: 0.8100 time: 0.0039s\n",
            "Epoch: 0282 loss_train: 0.3649 acc_train: 0.9357 loss_val: 0.6570 acc_val: 0.8133 time: 0.0041s\n",
            "Epoch: 0283 loss_train: 0.3588 acc_train: 0.9500 loss_val: 0.6550 acc_val: 0.8167 time: 0.0040s\n",
            "Epoch: 0284 loss_train: 0.3982 acc_train: 0.9214 loss_val: 0.6536 acc_val: 0.8233 time: 0.0041s\n",
            "Epoch: 0285 loss_train: 0.3604 acc_train: 0.9571 loss_val: 0.6522 acc_val: 0.8300 time: 0.0041s\n",
            "Epoch: 0286 loss_train: 0.3869 acc_train: 0.9214 loss_val: 0.6510 acc_val: 0.8267 time: 0.0040s\n",
            "Epoch: 0287 loss_train: 0.3134 acc_train: 0.9643 loss_val: 0.6506 acc_val: 0.8200 time: 0.0039s\n",
            "Epoch: 0288 loss_train: 0.3557 acc_train: 0.9429 loss_val: 0.6499 acc_val: 0.8200 time: 0.0040s\n",
            "Epoch: 0289 loss_train: 0.3507 acc_train: 0.9643 loss_val: 0.6498 acc_val: 0.8167 time: 0.0040s\n",
            "Epoch: 0290 loss_train: 0.3488 acc_train: 0.9571 loss_val: 0.6502 acc_val: 0.8167 time: 0.0040s\n",
            "Epoch: 0291 loss_train: 0.3545 acc_train: 0.9429 loss_val: 0.6517 acc_val: 0.8067 time: 0.0040s\n",
            "Epoch: 0292 loss_train: 0.3627 acc_train: 0.9571 loss_val: 0.6540 acc_val: 0.8133 time: 0.0042s\n",
            "Epoch: 0293 loss_train: 0.3638 acc_train: 0.9643 loss_val: 0.6555 acc_val: 0.8100 time: 0.0040s\n",
            "Epoch: 0294 loss_train: 0.3029 acc_train: 0.9500 loss_val: 0.6576 acc_val: 0.8100 time: 0.0040s\n",
            "Epoch: 0295 loss_train: 0.3336 acc_train: 0.9571 loss_val: 0.6588 acc_val: 0.8100 time: 0.0040s\n",
            "Epoch: 0296 loss_train: 0.3675 acc_train: 0.9500 loss_val: 0.6573 acc_val: 0.8100 time: 0.0040s\n",
            "Epoch: 0297 loss_train: 0.3134 acc_train: 0.9643 loss_val: 0.6549 acc_val: 0.8167 time: 0.0041s\n",
            "Epoch: 0298 loss_train: 0.3409 acc_train: 0.9357 loss_val: 0.6525 acc_val: 0.8167 time: 0.0039s\n",
            "Epoch: 0299 loss_train: 0.3416 acc_train: 0.9643 loss_val: 0.6501 acc_val: 0.8300 time: 0.0040s\n",
            "Epoch: 0300 loss_train: 0.3375 acc_train: 0.9571 loss_val: 0.6488 acc_val: 0.8300 time: 0.0041s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 1.3078s\n",
            "Test set results: loss= 0.6684 accuracy= 0.8400\n",
            "------------------------------------------ epoch 300 ^ ------------------------------------------\n",
            "Loading cora dataset...\n",
            "Epoch: 0001 loss_train: 2.0014 acc_train: 0.1357 loss_val: 2.0038 acc_val: 0.1567 time: 0.0105s\n",
            "Epoch: 0002 loss_train: 1.9795 acc_train: 0.2071 loss_val: 1.9884 acc_val: 0.1567 time: 0.0049s\n",
            "Epoch: 0003 loss_train: 1.9600 acc_train: 0.1857 loss_val: 1.9736 acc_val: 0.1567 time: 0.0046s\n",
            "Epoch: 0004 loss_train: 1.9394 acc_train: 0.2000 loss_val: 1.9593 acc_val: 0.1567 time: 0.0046s\n",
            "Epoch: 0005 loss_train: 1.9356 acc_train: 0.2000 loss_val: 1.9453 acc_val: 0.1567 time: 0.0049s\n",
            "Epoch: 0006 loss_train: 1.9308 acc_train: 0.2000 loss_val: 1.9321 acc_val: 0.1567 time: 0.0049s\n",
            "Epoch: 0007 loss_train: 1.9020 acc_train: 0.2000 loss_val: 1.9193 acc_val: 0.1567 time: 0.0047s\n",
            "Epoch: 0008 loss_train: 1.9042 acc_train: 0.2000 loss_val: 1.9069 acc_val: 0.1567 time: 0.0048s\n",
            "Epoch: 0009 loss_train: 1.8804 acc_train: 0.1929 loss_val: 1.8951 acc_val: 0.1567 time: 0.0047s\n",
            "Epoch: 0010 loss_train: 1.8758 acc_train: 0.2000 loss_val: 1.8837 acc_val: 0.1567 time: 0.0048s\n",
            "Epoch: 0011 loss_train: 1.8553 acc_train: 0.2143 loss_val: 1.8725 acc_val: 0.1567 time: 0.0048s\n",
            "Epoch: 0012 loss_train: 1.8445 acc_train: 0.2071 loss_val: 1.8614 acc_val: 0.1567 time: 0.0047s\n",
            "Epoch: 0013 loss_train: 1.8429 acc_train: 0.2286 loss_val: 1.8507 acc_val: 0.1567 time: 0.0047s\n",
            "Epoch: 0014 loss_train: 1.8373 acc_train: 0.2571 loss_val: 1.8405 acc_val: 0.1600 time: 0.0047s\n",
            "Epoch: 0015 loss_train: 1.8293 acc_train: 0.2929 loss_val: 1.8308 acc_val: 0.2800 time: 0.0047s\n",
            "Epoch: 0016 loss_train: 1.8272 acc_train: 0.3357 loss_val: 1.8215 acc_val: 0.4133 time: 0.0048s\n",
            "Epoch: 0017 loss_train: 1.8031 acc_train: 0.3071 loss_val: 1.8127 acc_val: 0.4600 time: 0.0047s\n",
            "Epoch: 0018 loss_train: 1.7641 acc_train: 0.4000 loss_val: 1.8041 acc_val: 0.4600 time: 0.0047s\n",
            "Epoch: 0019 loss_train: 1.7819 acc_train: 0.3500 loss_val: 1.7959 acc_val: 0.4067 time: 0.0054s\n",
            "Epoch: 0020 loss_train: 1.7940 acc_train: 0.3429 loss_val: 1.7881 acc_val: 0.3867 time: 0.0047s\n",
            "Epoch: 0021 loss_train: 1.7569 acc_train: 0.4143 loss_val: 1.7807 acc_val: 0.3667 time: 0.0047s\n",
            "Epoch: 0022 loss_train: 1.7420 acc_train: 0.3714 loss_val: 1.7735 acc_val: 0.3633 time: 0.0047s\n",
            "Epoch: 0023 loss_train: 1.7672 acc_train: 0.3000 loss_val: 1.7666 acc_val: 0.3600 time: 0.0047s\n",
            "Epoch: 0024 loss_train: 1.7342 acc_train: 0.3500 loss_val: 1.7599 acc_val: 0.3500 time: 0.0050s\n",
            "Epoch: 0025 loss_train: 1.7285 acc_train: 0.3429 loss_val: 1.7535 acc_val: 0.3500 time: 0.0047s\n",
            "Epoch: 0026 loss_train: 1.7395 acc_train: 0.3357 loss_val: 1.7473 acc_val: 0.3500 time: 0.0047s\n",
            "Epoch: 0027 loss_train: 1.7226 acc_train: 0.3500 loss_val: 1.7413 acc_val: 0.3500 time: 0.0046s\n",
            "Epoch: 0028 loss_train: 1.7343 acc_train: 0.3286 loss_val: 1.7356 acc_val: 0.3500 time: 0.0046s\n",
            "Epoch: 0029 loss_train: 1.7381 acc_train: 0.3429 loss_val: 1.7301 acc_val: 0.3500 time: 0.0046s\n",
            "Epoch: 0030 loss_train: 1.7333 acc_train: 0.3571 loss_val: 1.7249 acc_val: 0.3500 time: 0.0047s\n",
            "Epoch: 0031 loss_train: 1.6834 acc_train: 0.3214 loss_val: 1.7197 acc_val: 0.3500 time: 0.0046s\n",
            "Epoch: 0032 loss_train: 1.6946 acc_train: 0.3357 loss_val: 1.7146 acc_val: 0.3500 time: 0.0048s\n",
            "Epoch: 0033 loss_train: 1.7199 acc_train: 0.3714 loss_val: 1.7095 acc_val: 0.3500 time: 0.0046s\n",
            "Epoch: 0034 loss_train: 1.6958 acc_train: 0.3357 loss_val: 1.7047 acc_val: 0.3500 time: 0.0046s\n",
            "Epoch: 0035 loss_train: 1.6752 acc_train: 0.3786 loss_val: 1.6998 acc_val: 0.3600 time: 0.0047s\n",
            "Epoch: 0036 loss_train: 1.6664 acc_train: 0.3643 loss_val: 1.6950 acc_val: 0.3633 time: 0.0046s\n",
            "Epoch: 0037 loss_train: 1.6817 acc_train: 0.3571 loss_val: 1.6901 acc_val: 0.3633 time: 0.0046s\n",
            "Epoch: 0038 loss_train: 1.6803 acc_train: 0.4143 loss_val: 1.6852 acc_val: 0.3633 time: 0.0047s\n",
            "Epoch: 0039 loss_train: 1.6831 acc_train: 0.3429 loss_val: 1.6803 acc_val: 0.3633 time: 0.0047s\n",
            "Epoch: 0040 loss_train: 1.6222 acc_train: 0.3786 loss_val: 1.6754 acc_val: 0.3667 time: 0.0049s\n",
            "Epoch: 0041 loss_train: 1.6738 acc_train: 0.3429 loss_val: 1.6707 acc_val: 0.3733 time: 0.0046s\n",
            "Epoch: 0042 loss_train: 1.6630 acc_train: 0.3643 loss_val: 1.6659 acc_val: 0.3733 time: 0.0046s\n",
            "Epoch: 0043 loss_train: 1.6497 acc_train: 0.3929 loss_val: 1.6613 acc_val: 0.3800 time: 0.0047s\n",
            "Epoch: 0044 loss_train: 1.6340 acc_train: 0.4071 loss_val: 1.6568 acc_val: 0.3933 time: 0.0046s\n",
            "Epoch: 0045 loss_train: 1.5828 acc_train: 0.4357 loss_val: 1.6522 acc_val: 0.4067 time: 0.0050s\n",
            "Epoch: 0046 loss_train: 1.6008 acc_train: 0.3929 loss_val: 1.6475 acc_val: 0.4100 time: 0.0046s\n",
            "Epoch: 0047 loss_train: 1.5905 acc_train: 0.4071 loss_val: 1.6426 acc_val: 0.4100 time: 0.0046s\n",
            "Epoch: 0048 loss_train: 1.6078 acc_train: 0.4071 loss_val: 1.6376 acc_val: 0.4167 time: 0.0046s\n",
            "Epoch: 0049 loss_train: 1.5862 acc_train: 0.4571 loss_val: 1.6325 acc_val: 0.4200 time: 0.0046s\n",
            "Epoch: 0050 loss_train: 1.5692 acc_train: 0.4357 loss_val: 1.6273 acc_val: 0.4233 time: 0.0046s\n",
            "Epoch: 0051 loss_train: 1.5498 acc_train: 0.4643 loss_val: 1.6220 acc_val: 0.4267 time: 0.0047s\n",
            "Epoch: 0052 loss_train: 1.5138 acc_train: 0.4429 loss_val: 1.6164 acc_val: 0.4267 time: 0.0047s\n",
            "Epoch: 0053 loss_train: 1.5633 acc_train: 0.4357 loss_val: 1.6107 acc_val: 0.4267 time: 0.0049s\n",
            "Epoch: 0054 loss_train: 1.5414 acc_train: 0.4357 loss_val: 1.6049 acc_val: 0.4333 time: 0.0050s\n",
            "Epoch: 0055 loss_train: 1.5549 acc_train: 0.4786 loss_val: 1.5989 acc_val: 0.4333 time: 0.0047s\n",
            "Epoch: 0056 loss_train: 1.5327 acc_train: 0.4286 loss_val: 1.5927 acc_val: 0.4367 time: 0.0047s\n",
            "Epoch: 0057 loss_train: 1.5079 acc_train: 0.4714 loss_val: 1.5864 acc_val: 0.4400 time: 0.0046s\n",
            "Epoch: 0058 loss_train: 1.4950 acc_train: 0.4357 loss_val: 1.5799 acc_val: 0.4400 time: 0.0047s\n",
            "Epoch: 0059 loss_train: 1.5097 acc_train: 0.4857 loss_val: 1.5733 acc_val: 0.4367 time: 0.0043s\n",
            "Epoch: 0060 loss_train: 1.5227 acc_train: 0.4643 loss_val: 1.5668 acc_val: 0.4433 time: 0.0043s\n",
            "Epoch: 0061 loss_train: 1.4900 acc_train: 0.4786 loss_val: 1.5603 acc_val: 0.4533 time: 0.0045s\n",
            "Epoch: 0062 loss_train: 1.4659 acc_train: 0.4857 loss_val: 1.5539 acc_val: 0.4600 time: 0.0081s\n",
            "Epoch: 0063 loss_train: 1.4549 acc_train: 0.5000 loss_val: 1.5474 acc_val: 0.4700 time: 0.0049s\n",
            "Epoch: 0064 loss_train: 1.4566 acc_train: 0.4357 loss_val: 1.5407 acc_val: 0.4700 time: 0.0043s\n",
            "Epoch: 0065 loss_train: 1.4684 acc_train: 0.4929 loss_val: 1.5339 acc_val: 0.4700 time: 0.0047s\n",
            "Epoch: 0066 loss_train: 1.4498 acc_train: 0.5214 loss_val: 1.5267 acc_val: 0.4700 time: 0.0043s\n",
            "Epoch: 0067 loss_train: 1.3891 acc_train: 0.5071 loss_val: 1.5196 acc_val: 0.4867 time: 0.0043s\n",
            "Epoch: 0068 loss_train: 1.3609 acc_train: 0.5357 loss_val: 1.5123 acc_val: 0.4967 time: 0.0043s\n",
            "Epoch: 0069 loss_train: 1.4048 acc_train: 0.5429 loss_val: 1.5047 acc_val: 0.4967 time: 0.0042s\n",
            "Epoch: 0070 loss_train: 1.4196 acc_train: 0.4571 loss_val: 1.4970 acc_val: 0.5000 time: 0.0044s\n",
            "Epoch: 0071 loss_train: 1.3966 acc_train: 0.4857 loss_val: 1.4890 acc_val: 0.5000 time: 0.0042s\n",
            "Epoch: 0072 loss_train: 1.4228 acc_train: 0.4786 loss_val: 1.4816 acc_val: 0.5033 time: 0.0043s\n",
            "Epoch: 0073 loss_train: 1.3776 acc_train: 0.5214 loss_val: 1.4743 acc_val: 0.5100 time: 0.0043s\n",
            "Epoch: 0074 loss_train: 1.4327 acc_train: 0.5071 loss_val: 1.4670 acc_val: 0.5133 time: 0.0042s\n",
            "Epoch: 0075 loss_train: 1.3846 acc_train: 0.5429 loss_val: 1.4599 acc_val: 0.5167 time: 0.0042s\n",
            "Epoch: 0076 loss_train: 1.3724 acc_train: 0.5286 loss_val: 1.4529 acc_val: 0.5300 time: 0.0043s\n",
            "Epoch: 0077 loss_train: 1.3306 acc_train: 0.5214 loss_val: 1.4460 acc_val: 0.5300 time: 0.0042s\n",
            "Epoch: 0078 loss_train: 1.3191 acc_train: 0.6071 loss_val: 1.4390 acc_val: 0.5400 time: 0.0042s\n",
            "Epoch: 0079 loss_train: 1.3394 acc_train: 0.5643 loss_val: 1.4319 acc_val: 0.5433 time: 0.0042s\n",
            "Epoch: 0080 loss_train: 1.3251 acc_train: 0.5214 loss_val: 1.4249 acc_val: 0.5433 time: 0.0042s\n",
            "Epoch: 0081 loss_train: 1.2849 acc_train: 0.6214 loss_val: 1.4176 acc_val: 0.5500 time: 0.0041s\n",
            "Epoch: 0082 loss_train: 1.3300 acc_train: 0.5571 loss_val: 1.4101 acc_val: 0.5500 time: 0.0042s\n",
            "Epoch: 0083 loss_train: 1.2937 acc_train: 0.5500 loss_val: 1.4026 acc_val: 0.5533 time: 0.0041s\n",
            "Epoch: 0084 loss_train: 1.2166 acc_train: 0.5571 loss_val: 1.3953 acc_val: 0.5600 time: 0.0043s\n",
            "Epoch: 0085 loss_train: 1.2290 acc_train: 0.6000 loss_val: 1.3883 acc_val: 0.5700 time: 0.0041s\n",
            "Epoch: 0086 loss_train: 1.2677 acc_train: 0.5643 loss_val: 1.3811 acc_val: 0.5700 time: 0.0041s\n",
            "Epoch: 0087 loss_train: 1.2722 acc_train: 0.5643 loss_val: 1.3739 acc_val: 0.5700 time: 0.0041s\n",
            "Epoch: 0088 loss_train: 1.2349 acc_train: 0.5857 loss_val: 1.3667 acc_val: 0.5700 time: 0.0042s\n",
            "Epoch: 0089 loss_train: 1.2767 acc_train: 0.5643 loss_val: 1.3595 acc_val: 0.5700 time: 0.0042s\n",
            "Epoch: 0090 loss_train: 1.1945 acc_train: 0.6071 loss_val: 1.3522 acc_val: 0.5700 time: 0.0043s\n",
            "Epoch: 0091 loss_train: 1.2006 acc_train: 0.6000 loss_val: 1.3447 acc_val: 0.5767 time: 0.0042s\n",
            "Epoch: 0092 loss_train: 1.2327 acc_train: 0.6500 loss_val: 1.3376 acc_val: 0.5867 time: 0.0041s\n",
            "Epoch: 0093 loss_train: 1.2149 acc_train: 0.6286 loss_val: 1.3308 acc_val: 0.5967 time: 0.0060s\n",
            "Epoch: 0094 loss_train: 1.2069 acc_train: 0.6071 loss_val: 1.3243 acc_val: 0.5967 time: 0.0042s\n",
            "Epoch: 0095 loss_train: 1.1905 acc_train: 0.6143 loss_val: 1.3182 acc_val: 0.6100 time: 0.0041s\n",
            "Epoch: 0096 loss_train: 1.1509 acc_train: 0.6571 loss_val: 1.3119 acc_val: 0.6200 time: 0.0041s\n",
            "Epoch: 0097 loss_train: 1.1923 acc_train: 0.6357 loss_val: 1.3057 acc_val: 0.6267 time: 0.0044s\n",
            "Epoch: 0098 loss_train: 1.1521 acc_train: 0.6500 loss_val: 1.2992 acc_val: 0.6267 time: 0.0066s\n",
            "Epoch: 0099 loss_train: 1.1811 acc_train: 0.6286 loss_val: 1.2928 acc_val: 0.6233 time: 0.0055s\n",
            "Epoch: 0100 loss_train: 1.1455 acc_train: 0.6286 loss_val: 1.2865 acc_val: 0.6300 time: 0.0047s\n",
            "Epoch: 0101 loss_train: 1.1650 acc_train: 0.6857 loss_val: 1.2800 acc_val: 0.6300 time: 0.0042s\n",
            "Epoch: 0102 loss_train: 1.0691 acc_train: 0.6643 loss_val: 1.2740 acc_val: 0.6333 time: 0.0043s\n",
            "Epoch: 0103 loss_train: 1.1428 acc_train: 0.6571 loss_val: 1.2680 acc_val: 0.6367 time: 0.0043s\n",
            "Epoch: 0104 loss_train: 1.1190 acc_train: 0.6500 loss_val: 1.2617 acc_val: 0.6333 time: 0.0041s\n",
            "Epoch: 0105 loss_train: 1.1407 acc_train: 0.6571 loss_val: 1.2554 acc_val: 0.6367 time: 0.0042s\n",
            "Epoch: 0106 loss_train: 1.1315 acc_train: 0.6429 loss_val: 1.2491 acc_val: 0.6400 time: 0.0084s\n",
            "Epoch: 0107 loss_train: 1.0780 acc_train: 0.6714 loss_val: 1.2434 acc_val: 0.6433 time: 0.0051s\n",
            "Epoch: 0108 loss_train: 1.0640 acc_train: 0.6929 loss_val: 1.2377 acc_val: 0.6433 time: 0.0042s\n",
            "Epoch: 0109 loss_train: 1.1313 acc_train: 0.6214 loss_val: 1.2323 acc_val: 0.6500 time: 0.0041s\n",
            "Epoch: 0110 loss_train: 1.0756 acc_train: 0.6929 loss_val: 1.2269 acc_val: 0.6500 time: 0.0041s\n",
            "Epoch: 0111 loss_train: 1.0869 acc_train: 0.6714 loss_val: 1.2216 acc_val: 0.6500 time: 0.0041s\n",
            "Epoch: 0112 loss_train: 1.0436 acc_train: 0.7286 loss_val: 1.2168 acc_val: 0.6567 time: 0.0040s\n",
            "Epoch: 0113 loss_train: 1.0797 acc_train: 0.6429 loss_val: 1.2120 acc_val: 0.6633 time: 0.0040s\n",
            "Epoch: 0114 loss_train: 1.0964 acc_train: 0.6000 loss_val: 1.2072 acc_val: 0.6633 time: 0.0041s\n",
            "Epoch: 0115 loss_train: 1.0274 acc_train: 0.7286 loss_val: 1.2021 acc_val: 0.6733 time: 0.0043s\n",
            "Epoch: 0116 loss_train: 1.0052 acc_train: 0.7214 loss_val: 1.1968 acc_val: 0.6700 time: 0.0041s\n",
            "Epoch: 0117 loss_train: 1.0772 acc_train: 0.6643 loss_val: 1.1914 acc_val: 0.6733 time: 0.0041s\n",
            "Epoch: 0118 loss_train: 1.0452 acc_train: 0.6929 loss_val: 1.1862 acc_val: 0.6767 time: 0.0042s\n",
            "Epoch: 0119 loss_train: 0.9876 acc_train: 0.7357 loss_val: 1.1813 acc_val: 0.6800 time: 0.0041s\n",
            "Epoch: 0120 loss_train: 0.9984 acc_train: 0.7429 loss_val: 1.1766 acc_val: 0.6900 time: 0.0041s\n",
            "Epoch: 0121 loss_train: 1.0296 acc_train: 0.6786 loss_val: 1.1722 acc_val: 0.6900 time: 0.0041s\n",
            "Epoch: 0122 loss_train: 1.0693 acc_train: 0.6929 loss_val: 1.1682 acc_val: 0.6967 time: 0.0041s\n",
            "Epoch: 0123 loss_train: 1.0019 acc_train: 0.7286 loss_val: 1.1646 acc_val: 0.7133 time: 0.0040s\n",
            "Epoch: 0124 loss_train: 0.9624 acc_train: 0.7500 loss_val: 1.1610 acc_val: 0.7133 time: 0.0041s\n",
            "Epoch: 0125 loss_train: 0.9458 acc_train: 0.7429 loss_val: 1.1571 acc_val: 0.7100 time: 0.0041s\n",
            "Epoch: 0126 loss_train: 1.0232 acc_train: 0.7000 loss_val: 1.1533 acc_val: 0.7067 time: 0.0041s\n",
            "Epoch: 0127 loss_train: 0.9170 acc_train: 0.7500 loss_val: 1.1490 acc_val: 0.7100 time: 0.0042s\n",
            "Epoch: 0128 loss_train: 0.9986 acc_train: 0.7214 loss_val: 1.1442 acc_val: 0.7100 time: 0.0041s\n",
            "Epoch: 0129 loss_train: 0.9426 acc_train: 0.7357 loss_val: 1.1395 acc_val: 0.7200 time: 0.0043s\n",
            "Epoch: 0130 loss_train: 1.0060 acc_train: 0.7357 loss_val: 1.1345 acc_val: 0.7200 time: 0.0040s\n",
            "Epoch: 0131 loss_train: 0.9613 acc_train: 0.7071 loss_val: 1.1296 acc_val: 0.7167 time: 0.0040s\n",
            "Epoch: 0132 loss_train: 1.0133 acc_train: 0.7000 loss_val: 1.1249 acc_val: 0.7200 time: 0.0041s\n",
            "Epoch: 0133 loss_train: 0.9713 acc_train: 0.7071 loss_val: 1.1204 acc_val: 0.7233 time: 0.0041s\n",
            "Epoch: 0134 loss_train: 0.9428 acc_train: 0.7286 loss_val: 1.1162 acc_val: 0.7233 time: 0.0042s\n",
            "Epoch: 0135 loss_train: 0.9383 acc_train: 0.7786 loss_val: 1.1119 acc_val: 0.7233 time: 0.0041s\n",
            "Epoch: 0136 loss_train: 0.9723 acc_train: 0.7071 loss_val: 1.1080 acc_val: 0.7233 time: 0.0042s\n",
            "Epoch: 0137 loss_train: 0.9229 acc_train: 0.7214 loss_val: 1.1045 acc_val: 0.7233 time: 0.0041s\n",
            "Epoch: 0138 loss_train: 0.9629 acc_train: 0.6929 loss_val: 1.1015 acc_val: 0.7267 time: 0.0042s\n",
            "Epoch: 0139 loss_train: 0.8955 acc_train: 0.7714 loss_val: 1.0982 acc_val: 0.7267 time: 0.0041s\n",
            "Epoch: 0140 loss_train: 0.9772 acc_train: 0.7286 loss_val: 1.0950 acc_val: 0.7233 time: 0.0041s\n",
            "Epoch: 0141 loss_train: 0.9015 acc_train: 0.7429 loss_val: 1.0914 acc_val: 0.7233 time: 0.0040s\n",
            "Epoch: 0142 loss_train: 0.9053 acc_train: 0.7500 loss_val: 1.0882 acc_val: 0.7233 time: 0.0042s\n",
            "Epoch: 0143 loss_train: 0.9465 acc_train: 0.7286 loss_val: 1.0853 acc_val: 0.7267 time: 0.0041s\n",
            "Epoch: 0144 loss_train: 0.9177 acc_train: 0.7357 loss_val: 1.0822 acc_val: 0.7267 time: 0.0041s\n",
            "Epoch: 0145 loss_train: 0.8726 acc_train: 0.7929 loss_val: 1.0785 acc_val: 0.7267 time: 0.0041s\n",
            "Epoch: 0146 loss_train: 0.9639 acc_train: 0.7286 loss_val: 1.0741 acc_val: 0.7267 time: 0.0041s\n",
            "Epoch: 0147 loss_train: 0.8868 acc_train: 0.7571 loss_val: 1.0697 acc_val: 0.7300 time: 0.0040s\n",
            "Epoch: 0148 loss_train: 0.9094 acc_train: 0.7357 loss_val: 1.0652 acc_val: 0.7267 time: 0.0040s\n",
            "Epoch: 0149 loss_train: 0.9374 acc_train: 0.7643 loss_val: 1.0609 acc_val: 0.7300 time: 0.0041s\n",
            "Epoch: 0150 loss_train: 0.9293 acc_train: 0.7500 loss_val: 1.0572 acc_val: 0.7300 time: 0.0041s\n",
            "Epoch: 0151 loss_train: 0.8882 acc_train: 0.7571 loss_val: 1.0541 acc_val: 0.7267 time: 0.0041s\n",
            "Epoch: 0152 loss_train: 0.9390 acc_train: 0.7571 loss_val: 1.0514 acc_val: 0.7333 time: 0.0040s\n",
            "Epoch: 0153 loss_train: 0.8700 acc_train: 0.7643 loss_val: 1.0484 acc_val: 0.7367 time: 0.0041s\n",
            "Epoch: 0154 loss_train: 0.9257 acc_train: 0.7429 loss_val: 1.0459 acc_val: 0.7400 time: 0.0039s\n",
            "Epoch: 0155 loss_train: 0.8905 acc_train: 0.7571 loss_val: 1.0434 acc_val: 0.7400 time: 0.0040s\n",
            "Epoch: 0156 loss_train: 0.8557 acc_train: 0.7643 loss_val: 1.0417 acc_val: 0.7467 time: 0.0042s\n",
            "Epoch: 0157 loss_train: 0.9023 acc_train: 0.7429 loss_val: 1.0400 acc_val: 0.7567 time: 0.0040s\n",
            "Epoch: 0158 loss_train: 0.8843 acc_train: 0.7357 loss_val: 1.0372 acc_val: 0.7567 time: 0.0040s\n",
            "Epoch: 0159 loss_train: 0.8348 acc_train: 0.7857 loss_val: 1.0340 acc_val: 0.7467 time: 0.0039s\n",
            "Epoch: 0160 loss_train: 0.8548 acc_train: 0.7929 loss_val: 1.0298 acc_val: 0.7467 time: 0.0040s\n",
            "Epoch: 0161 loss_train: 0.8343 acc_train: 0.7571 loss_val: 1.0256 acc_val: 0.7433 time: 0.0040s\n",
            "Epoch: 0162 loss_train: 0.8206 acc_train: 0.8143 loss_val: 1.0212 acc_val: 0.7433 time: 0.0040s\n",
            "Epoch: 0163 loss_train: 0.8807 acc_train: 0.7429 loss_val: 1.0173 acc_val: 0.7400 time: 0.0043s\n",
            "Epoch: 0164 loss_train: 0.8288 acc_train: 0.8000 loss_val: 1.0139 acc_val: 0.7433 time: 0.0040s\n",
            "Epoch: 0165 loss_train: 0.8543 acc_train: 0.7643 loss_val: 1.0107 acc_val: 0.7500 time: 0.0040s\n",
            "Epoch: 0166 loss_train: 0.8228 acc_train: 0.7929 loss_val: 1.0074 acc_val: 0.7567 time: 0.0040s\n",
            "Epoch: 0167 loss_train: 0.8379 acc_train: 0.7714 loss_val: 1.0037 acc_val: 0.7633 time: 0.0040s\n",
            "Epoch: 0168 loss_train: 0.8068 acc_train: 0.7643 loss_val: 0.9996 acc_val: 0.7667 time: 0.0040s\n",
            "Epoch: 0169 loss_train: 0.8517 acc_train: 0.8143 loss_val: 0.9944 acc_val: 0.7700 time: 0.0040s\n",
            "Epoch: 0170 loss_train: 0.8695 acc_train: 0.7643 loss_val: 0.9896 acc_val: 0.7733 time: 0.0040s\n",
            "Epoch: 0171 loss_train: 0.7801 acc_train: 0.8500 loss_val: 0.9844 acc_val: 0.7733 time: 0.0041s\n",
            "Epoch: 0172 loss_train: 0.8199 acc_train: 0.7714 loss_val: 0.9806 acc_val: 0.7733 time: 0.0041s\n",
            "Epoch: 0173 loss_train: 0.7943 acc_train: 0.8000 loss_val: 0.9771 acc_val: 0.7733 time: 0.0040s\n",
            "Epoch: 0174 loss_train: 0.8441 acc_train: 0.8214 loss_val: 0.9735 acc_val: 0.7733 time: 0.0042s\n",
            "Epoch: 0175 loss_train: 0.7643 acc_train: 0.8214 loss_val: 0.9699 acc_val: 0.7767 time: 0.0040s\n",
            "Epoch: 0176 loss_train: 0.8202 acc_train: 0.8143 loss_val: 0.9666 acc_val: 0.7833 time: 0.0040s\n",
            "Epoch: 0177 loss_train: 0.8530 acc_train: 0.7786 loss_val: 0.9634 acc_val: 0.7833 time: 0.0039s\n",
            "Epoch: 0178 loss_train: 0.7908 acc_train: 0.7929 loss_val: 0.9608 acc_val: 0.7800 time: 0.0051s\n",
            "Epoch: 0179 loss_train: 0.7405 acc_train: 0.8214 loss_val: 0.9580 acc_val: 0.7767 time: 0.0060s\n",
            "Epoch: 0180 loss_train: 0.7649 acc_train: 0.8143 loss_val: 0.9550 acc_val: 0.7767 time: 0.0040s\n",
            "Epoch: 0181 loss_train: 0.8002 acc_train: 0.8143 loss_val: 0.9508 acc_val: 0.7767 time: 0.0042s\n",
            "Epoch: 0182 loss_train: 0.7761 acc_train: 0.8000 loss_val: 0.9467 acc_val: 0.7833 time: 0.0045s\n",
            "Epoch: 0183 loss_train: 0.7999 acc_train: 0.7929 loss_val: 0.9431 acc_val: 0.7867 time: 0.0041s\n",
            "Epoch: 0184 loss_train: 0.7409 acc_train: 0.8643 loss_val: 0.9393 acc_val: 0.7900 time: 0.0041s\n",
            "Epoch: 0185 loss_train: 0.7193 acc_train: 0.8500 loss_val: 0.9351 acc_val: 0.7900 time: 0.0040s\n",
            "Epoch: 0186 loss_train: 0.7422 acc_train: 0.8357 loss_val: 0.9308 acc_val: 0.7900 time: 0.0040s\n",
            "Epoch: 0187 loss_train: 0.8122 acc_train: 0.7929 loss_val: 0.9269 acc_val: 0.7900 time: 0.0040s\n",
            "Epoch: 0188 loss_train: 0.7636 acc_train: 0.8357 loss_val: 0.9239 acc_val: 0.7900 time: 0.0040s\n",
            "Epoch: 0189 loss_train: 0.7988 acc_train: 0.7857 loss_val: 0.9212 acc_val: 0.7933 time: 0.0040s\n",
            "Epoch: 0190 loss_train: 0.6780 acc_train: 0.8000 loss_val: 0.9198 acc_val: 0.7933 time: 0.0053s\n",
            "Epoch: 0191 loss_train: 0.7815 acc_train: 0.8214 loss_val: 0.9190 acc_val: 0.7867 time: 0.0048s\n",
            "Epoch: 0192 loss_train: 0.7356 acc_train: 0.8286 loss_val: 0.9176 acc_val: 0.7833 time: 0.0041s\n",
            "Epoch: 0193 loss_train: 0.7410 acc_train: 0.8143 loss_val: 0.9169 acc_val: 0.7867 time: 0.0041s\n",
            "Epoch: 0194 loss_train: 0.7308 acc_train: 0.8214 loss_val: 0.9144 acc_val: 0.7833 time: 0.0040s\n",
            "Epoch: 0195 loss_train: 0.7563 acc_train: 0.8071 loss_val: 0.9120 acc_val: 0.7867 time: 0.0040s\n",
            "Epoch: 0196 loss_train: 0.7666 acc_train: 0.8214 loss_val: 0.9085 acc_val: 0.7867 time: 0.0040s\n",
            "Epoch: 0197 loss_train: 0.7255 acc_train: 0.8286 loss_val: 0.9044 acc_val: 0.7867 time: 0.0040s\n",
            "Epoch: 0198 loss_train: 0.7018 acc_train: 0.8429 loss_val: 0.9003 acc_val: 0.7867 time: 0.0041s\n",
            "Epoch: 0199 loss_train: 0.7413 acc_train: 0.8286 loss_val: 0.8965 acc_val: 0.7900 time: 0.0045s\n",
            "Epoch: 0200 loss_train: 0.7682 acc_train: 0.8214 loss_val: 0.8929 acc_val: 0.7833 time: 0.0041s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 0.8978s\n",
            "Test set results: loss= 0.9719 accuracy= 0.7790\n",
            "------------------------------------------ hidden 8 ^ ------------------------------------------\n",
            "Loading cora dataset...\n",
            "Epoch: 0001 loss_train: 1.9408 acc_train: 0.1500 loss_val: 1.9138 acc_val: 0.1267 time: 0.0108s\n",
            "Epoch: 0002 loss_train: 1.9163 acc_train: 0.1500 loss_val: 1.8924 acc_val: 0.1300 time: 0.0052s\n",
            "Epoch: 0003 loss_train: 1.8993 acc_train: 0.1786 loss_val: 1.8730 acc_val: 0.3700 time: 0.0048s\n",
            "Epoch: 0004 loss_train: 1.8804 acc_train: 0.3500 loss_val: 1.8551 acc_val: 0.3500 time: 0.0051s\n",
            "Epoch: 0005 loss_train: 1.8656 acc_train: 0.3071 loss_val: 1.8391 acc_val: 0.3500 time: 0.0052s\n",
            "Epoch: 0006 loss_train: 1.8483 acc_train: 0.2929 loss_val: 1.8243 acc_val: 0.3500 time: 0.0051s\n",
            "Epoch: 0007 loss_train: 1.8366 acc_train: 0.2929 loss_val: 1.8107 acc_val: 0.3500 time: 0.0048s\n",
            "Epoch: 0008 loss_train: 1.8261 acc_train: 0.2929 loss_val: 1.7983 acc_val: 0.3500 time: 0.0051s\n",
            "Epoch: 0009 loss_train: 1.8061 acc_train: 0.2929 loss_val: 1.7863 acc_val: 0.3500 time: 0.0050s\n",
            "Epoch: 0010 loss_train: 1.8050 acc_train: 0.2929 loss_val: 1.7752 acc_val: 0.3500 time: 0.0050s\n",
            "Epoch: 0011 loss_train: 1.7792 acc_train: 0.2929 loss_val: 1.7647 acc_val: 0.3500 time: 0.0051s\n",
            "Epoch: 0012 loss_train: 1.7595 acc_train: 0.2929 loss_val: 1.7545 acc_val: 0.3500 time: 0.0051s\n",
            "Epoch: 0013 loss_train: 1.7532 acc_train: 0.2929 loss_val: 1.7446 acc_val: 0.3500 time: 0.0050s\n",
            "Epoch: 0014 loss_train: 1.7335 acc_train: 0.2929 loss_val: 1.7352 acc_val: 0.3500 time: 0.0050s\n",
            "Epoch: 0015 loss_train: 1.7284 acc_train: 0.3000 loss_val: 1.7264 acc_val: 0.3500 time: 0.0050s\n",
            "Epoch: 0016 loss_train: 1.7099 acc_train: 0.3071 loss_val: 1.7182 acc_val: 0.3500 time: 0.0051s\n",
            "Epoch: 0017 loss_train: 1.6966 acc_train: 0.3000 loss_val: 1.7103 acc_val: 0.3500 time: 0.0050s\n",
            "Epoch: 0018 loss_train: 1.6729 acc_train: 0.3143 loss_val: 1.7023 acc_val: 0.3533 time: 0.0051s\n",
            "Epoch: 0019 loss_train: 1.6713 acc_train: 0.3714 loss_val: 1.6942 acc_val: 0.3633 time: 0.0050s\n",
            "Epoch: 0020 loss_train: 1.6510 acc_train: 0.3786 loss_val: 1.6855 acc_val: 0.3633 time: 0.0051s\n",
            "Epoch: 0021 loss_train: 1.6483 acc_train: 0.3857 loss_val: 1.6764 acc_val: 0.3667 time: 0.0051s\n",
            "Epoch: 0022 loss_train: 1.6291 acc_train: 0.4071 loss_val: 1.6667 acc_val: 0.3733 time: 0.0050s\n",
            "Epoch: 0023 loss_train: 1.6218 acc_train: 0.4429 loss_val: 1.6564 acc_val: 0.3933 time: 0.0061s\n",
            "Epoch: 0024 loss_train: 1.6158 acc_train: 0.4143 loss_val: 1.6456 acc_val: 0.4033 time: 0.0055s\n",
            "Epoch: 0025 loss_train: 1.5707 acc_train: 0.4571 loss_val: 1.6343 acc_val: 0.4100 time: 0.0051s\n",
            "Epoch: 0026 loss_train: 1.5634 acc_train: 0.4571 loss_val: 1.6223 acc_val: 0.4267 time: 0.0052s\n",
            "Epoch: 0027 loss_train: 1.5294 acc_train: 0.5143 loss_val: 1.6097 acc_val: 0.4367 time: 0.0054s\n",
            "Epoch: 0028 loss_train: 1.5253 acc_train: 0.5143 loss_val: 1.5965 acc_val: 0.4500 time: 0.0049s\n",
            "Epoch: 0029 loss_train: 1.5136 acc_train: 0.5143 loss_val: 1.5824 acc_val: 0.4800 time: 0.0050s\n",
            "Epoch: 0030 loss_train: 1.4847 acc_train: 0.5214 loss_val: 1.5680 acc_val: 0.5100 time: 0.0048s\n",
            "Epoch: 0031 loss_train: 1.4580 acc_train: 0.5214 loss_val: 1.5534 acc_val: 0.5333 time: 0.0048s\n",
            "Epoch: 0032 loss_train: 1.4473 acc_train: 0.6000 loss_val: 1.5382 acc_val: 0.5467 time: 0.0048s\n",
            "Epoch: 0033 loss_train: 1.4403 acc_train: 0.5714 loss_val: 1.5224 acc_val: 0.5567 time: 0.0048s\n",
            "Epoch: 0034 loss_train: 1.4106 acc_train: 0.5500 loss_val: 1.5064 acc_val: 0.5667 time: 0.0049s\n",
            "Epoch: 0035 loss_train: 1.3685 acc_train: 0.5643 loss_val: 1.4897 acc_val: 0.5767 time: 0.0049s\n",
            "Epoch: 0036 loss_train: 1.3606 acc_train: 0.6214 loss_val: 1.4726 acc_val: 0.5833 time: 0.0049s\n",
            "Epoch: 0037 loss_train: 1.3340 acc_train: 0.6071 loss_val: 1.4553 acc_val: 0.6033 time: 0.0049s\n",
            "Epoch: 0038 loss_train: 1.3182 acc_train: 0.6286 loss_val: 1.4377 acc_val: 0.6133 time: 0.0049s\n",
            "Epoch: 0039 loss_train: 1.2736 acc_train: 0.6500 loss_val: 1.4201 acc_val: 0.6167 time: 0.0048s\n",
            "Epoch: 0040 loss_train: 1.2727 acc_train: 0.6571 loss_val: 1.4023 acc_val: 0.6333 time: 0.0048s\n",
            "Epoch: 0041 loss_train: 1.2331 acc_train: 0.7000 loss_val: 1.3845 acc_val: 0.6367 time: 0.0045s\n",
            "Epoch: 0042 loss_train: 1.2021 acc_train: 0.7071 loss_val: 1.3666 acc_val: 0.6500 time: 0.0046s\n",
            "Epoch: 0043 loss_train: 1.2255 acc_train: 0.7000 loss_val: 1.3483 acc_val: 0.6533 time: 0.0046s\n",
            "Epoch: 0044 loss_train: 1.1381 acc_train: 0.7286 loss_val: 1.3302 acc_val: 0.6633 time: 0.0062s\n",
            "Epoch: 0045 loss_train: 1.1568 acc_train: 0.7357 loss_val: 1.3118 acc_val: 0.6633 time: 0.0060s\n",
            "Epoch: 0046 loss_train: 1.1081 acc_train: 0.7643 loss_val: 1.2924 acc_val: 0.6700 time: 0.0050s\n",
            "Epoch: 0047 loss_train: 1.0696 acc_train: 0.8071 loss_val: 1.2723 acc_val: 0.6800 time: 0.0046s\n",
            "Epoch: 0048 loss_train: 1.0472 acc_train: 0.7929 loss_val: 1.2524 acc_val: 0.6967 time: 0.0047s\n",
            "Epoch: 0049 loss_train: 1.0591 acc_train: 0.7929 loss_val: 1.2326 acc_val: 0.6967 time: 0.0046s\n",
            "Epoch: 0050 loss_train: 0.9950 acc_train: 0.8143 loss_val: 1.2133 acc_val: 0.7033 time: 0.0044s\n",
            "Epoch: 0051 loss_train: 0.9943 acc_train: 0.8214 loss_val: 1.1953 acc_val: 0.7067 time: 0.0046s\n",
            "Epoch: 0052 loss_train: 0.9775 acc_train: 0.7929 loss_val: 1.1778 acc_val: 0.7233 time: 0.0045s\n",
            "Epoch: 0053 loss_train: 0.9518 acc_train: 0.7929 loss_val: 1.1610 acc_val: 0.7267 time: 0.0048s\n",
            "Epoch: 0054 loss_train: 0.9363 acc_train: 0.8071 loss_val: 1.1448 acc_val: 0.7333 time: 0.0047s\n",
            "Epoch: 0055 loss_train: 0.9046 acc_train: 0.8214 loss_val: 1.1295 acc_val: 0.7367 time: 0.0046s\n",
            "Epoch: 0056 loss_train: 0.8870 acc_train: 0.8214 loss_val: 1.1135 acc_val: 0.7433 time: 0.0045s\n",
            "Epoch: 0057 loss_train: 0.8403 acc_train: 0.8500 loss_val: 1.0978 acc_val: 0.7633 time: 0.0045s\n",
            "Epoch: 0058 loss_train: 0.8503 acc_train: 0.8714 loss_val: 1.0823 acc_val: 0.7767 time: 0.0045s\n",
            "Epoch: 0059 loss_train: 0.8225 acc_train: 0.8571 loss_val: 1.0670 acc_val: 0.7867 time: 0.0046s\n",
            "Epoch: 0060 loss_train: 0.8164 acc_train: 0.8786 loss_val: 1.0519 acc_val: 0.7933 time: 0.0046s\n",
            "Epoch: 0061 loss_train: 0.8040 acc_train: 0.8857 loss_val: 1.0368 acc_val: 0.7900 time: 0.0046s\n",
            "Epoch: 0062 loss_train: 0.7571 acc_train: 0.8929 loss_val: 1.0221 acc_val: 0.7900 time: 0.0047s\n",
            "Epoch: 0063 loss_train: 0.7819 acc_train: 0.8786 loss_val: 1.0082 acc_val: 0.7900 time: 0.0046s\n",
            "Epoch: 0064 loss_train: 0.7711 acc_train: 0.8357 loss_val: 0.9951 acc_val: 0.7933 time: 0.0046s\n",
            "Epoch: 0065 loss_train: 0.7496 acc_train: 0.8714 loss_val: 0.9821 acc_val: 0.7967 time: 0.0047s\n",
            "Epoch: 0066 loss_train: 0.7176 acc_train: 0.8714 loss_val: 0.9702 acc_val: 0.7967 time: 0.0046s\n",
            "Epoch: 0067 loss_train: 0.6825 acc_train: 0.9000 loss_val: 0.9590 acc_val: 0.7933 time: 0.0048s\n",
            "Epoch: 0068 loss_train: 0.6953 acc_train: 0.8929 loss_val: 0.9490 acc_val: 0.7933 time: 0.0046s\n",
            "Epoch: 0069 loss_train: 0.6963 acc_train: 0.9000 loss_val: 0.9390 acc_val: 0.7933 time: 0.0046s\n",
            "Epoch: 0070 loss_train: 0.6873 acc_train: 0.8857 loss_val: 0.9296 acc_val: 0.7933 time: 0.0047s\n",
            "Epoch: 0071 loss_train: 0.6597 acc_train: 0.8786 loss_val: 0.9207 acc_val: 0.7967 time: 0.0046s\n",
            "Epoch: 0072 loss_train: 0.6111 acc_train: 0.9071 loss_val: 0.9132 acc_val: 0.7933 time: 0.0045s\n",
            "Epoch: 0073 loss_train: 0.6307 acc_train: 0.9000 loss_val: 0.9053 acc_val: 0.7867 time: 0.0046s\n",
            "Epoch: 0074 loss_train: 0.6271 acc_train: 0.8786 loss_val: 0.8954 acc_val: 0.7967 time: 0.0045s\n",
            "Epoch: 0075 loss_train: 0.6137 acc_train: 0.9071 loss_val: 0.8860 acc_val: 0.8000 time: 0.0046s\n",
            "Epoch: 0076 loss_train: 0.5754 acc_train: 0.9214 loss_val: 0.8774 acc_val: 0.8033 time: 0.0046s\n",
            "Epoch: 0077 loss_train: 0.6167 acc_train: 0.8929 loss_val: 0.8694 acc_val: 0.8067 time: 0.0046s\n",
            "Epoch: 0078 loss_train: 0.5645 acc_train: 0.9214 loss_val: 0.8629 acc_val: 0.8067 time: 0.0047s\n",
            "Epoch: 0079 loss_train: 0.5617 acc_train: 0.9143 loss_val: 0.8576 acc_val: 0.8067 time: 0.0047s\n",
            "Epoch: 0080 loss_train: 0.5801 acc_train: 0.8714 loss_val: 0.8522 acc_val: 0.8067 time: 0.0045s\n",
            "Epoch: 0081 loss_train: 0.5691 acc_train: 0.9000 loss_val: 0.8476 acc_val: 0.8000 time: 0.0045s\n",
            "Epoch: 0082 loss_train: 0.5323 acc_train: 0.9071 loss_val: 0.8436 acc_val: 0.7933 time: 0.0047s\n",
            "Epoch: 0083 loss_train: 0.5384 acc_train: 0.9071 loss_val: 0.8405 acc_val: 0.7867 time: 0.0042s\n",
            "Epoch: 0084 loss_train: 0.5159 acc_train: 0.9286 loss_val: 0.8369 acc_val: 0.7833 time: 0.0042s\n",
            "Epoch: 0085 loss_train: 0.5162 acc_train: 0.9143 loss_val: 0.8326 acc_val: 0.7867 time: 0.0042s\n",
            "Epoch: 0086 loss_train: 0.5304 acc_train: 0.9071 loss_val: 0.8273 acc_val: 0.7900 time: 0.0043s\n",
            "Epoch: 0087 loss_train: 0.4968 acc_train: 0.9214 loss_val: 0.8209 acc_val: 0.7900 time: 0.0044s\n",
            "Epoch: 0088 loss_train: 0.5361 acc_train: 0.9143 loss_val: 0.8150 acc_val: 0.7933 time: 0.0043s\n",
            "Epoch: 0089 loss_train: 0.5271 acc_train: 0.8929 loss_val: 0.8081 acc_val: 0.7933 time: 0.0072s\n",
            "Epoch: 0090 loss_train: 0.5450 acc_train: 0.9143 loss_val: 0.8023 acc_val: 0.7933 time: 0.0051s\n",
            "Epoch: 0091 loss_train: 0.5125 acc_train: 0.9286 loss_val: 0.7968 acc_val: 0.8100 time: 0.0044s\n",
            "Epoch: 0092 loss_train: 0.4754 acc_train: 0.9357 loss_val: 0.7922 acc_val: 0.8100 time: 0.0046s\n",
            "Epoch: 0093 loss_train: 0.5206 acc_train: 0.8929 loss_val: 0.7883 acc_val: 0.8100 time: 0.0045s\n",
            "Epoch: 0094 loss_train: 0.5048 acc_train: 0.9214 loss_val: 0.7859 acc_val: 0.8133 time: 0.0044s\n",
            "Epoch: 0095 loss_train: 0.4699 acc_train: 0.9357 loss_val: 0.7844 acc_val: 0.8000 time: 0.0043s\n",
            "Epoch: 0096 loss_train: 0.4508 acc_train: 0.9286 loss_val: 0.7842 acc_val: 0.8033 time: 0.0044s\n",
            "Epoch: 0097 loss_train: 0.4405 acc_train: 0.9286 loss_val: 0.7845 acc_val: 0.8067 time: 0.0043s\n",
            "Epoch: 0098 loss_train: 0.4545 acc_train: 0.9429 loss_val: 0.7845 acc_val: 0.7967 time: 0.0042s\n",
            "Epoch: 0099 loss_train: 0.4626 acc_train: 0.9429 loss_val: 0.7828 acc_val: 0.7900 time: 0.0042s\n",
            "Epoch: 0100 loss_train: 0.4614 acc_train: 0.9286 loss_val: 0.7775 acc_val: 0.7933 time: 0.0042s\n",
            "Epoch: 0101 loss_train: 0.4433 acc_train: 0.9357 loss_val: 0.7706 acc_val: 0.8000 time: 0.0043s\n",
            "Epoch: 0102 loss_train: 0.4428 acc_train: 0.9286 loss_val: 0.7635 acc_val: 0.8033 time: 0.0043s\n",
            "Epoch: 0103 loss_train: 0.4491 acc_train: 0.9143 loss_val: 0.7578 acc_val: 0.8067 time: 0.0042s\n",
            "Epoch: 0104 loss_train: 0.4400 acc_train: 0.9571 loss_val: 0.7540 acc_val: 0.8133 time: 0.0054s\n",
            "Epoch: 0105 loss_train: 0.4502 acc_train: 0.9286 loss_val: 0.7512 acc_val: 0.8067 time: 0.0051s\n",
            "Epoch: 0106 loss_train: 0.4588 acc_train: 0.9286 loss_val: 0.7489 acc_val: 0.8100 time: 0.0043s\n",
            "Epoch: 0107 loss_train: 0.4487 acc_train: 0.9000 loss_val: 0.7478 acc_val: 0.8000 time: 0.0042s\n",
            "Epoch: 0108 loss_train: 0.4447 acc_train: 0.9143 loss_val: 0.7473 acc_val: 0.8000 time: 0.0043s\n",
            "Epoch: 0109 loss_train: 0.4073 acc_train: 0.9500 loss_val: 0.7475 acc_val: 0.8067 time: 0.0047s\n",
            "Epoch: 0110 loss_train: 0.4422 acc_train: 0.9286 loss_val: 0.7459 acc_val: 0.8033 time: 0.0045s\n",
            "Epoch: 0111 loss_train: 0.4019 acc_train: 0.9714 loss_val: 0.7433 acc_val: 0.8067 time: 0.0042s\n",
            "Epoch: 0112 loss_train: 0.4348 acc_train: 0.9286 loss_val: 0.7394 acc_val: 0.8100 time: 0.0042s\n",
            "Epoch: 0113 loss_train: 0.3970 acc_train: 0.9429 loss_val: 0.7368 acc_val: 0.8067 time: 0.0043s\n",
            "Epoch: 0114 loss_train: 0.4278 acc_train: 0.9429 loss_val: 0.7338 acc_val: 0.8100 time: 0.0042s\n",
            "Epoch: 0115 loss_train: 0.4181 acc_train: 0.9571 loss_val: 0.7305 acc_val: 0.8067 time: 0.0042s\n",
            "Epoch: 0116 loss_train: 0.3953 acc_train: 0.9500 loss_val: 0.7277 acc_val: 0.8100 time: 0.0042s\n",
            "Epoch: 0117 loss_train: 0.4069 acc_train: 0.9429 loss_val: 0.7257 acc_val: 0.8100 time: 0.0042s\n",
            "Epoch: 0118 loss_train: 0.4290 acc_train: 0.9286 loss_val: 0.7230 acc_val: 0.8067 time: 0.0044s\n",
            "Epoch: 0119 loss_train: 0.3661 acc_train: 0.9500 loss_val: 0.7202 acc_val: 0.8067 time: 0.0043s\n",
            "Epoch: 0120 loss_train: 0.3940 acc_train: 0.9500 loss_val: 0.7198 acc_val: 0.8033 time: 0.0045s\n",
            "Epoch: 0121 loss_train: 0.4215 acc_train: 0.9500 loss_val: 0.7209 acc_val: 0.8033 time: 0.0043s\n",
            "Epoch: 0122 loss_train: 0.3705 acc_train: 0.9500 loss_val: 0.7222 acc_val: 0.7967 time: 0.0042s\n",
            "Epoch: 0123 loss_train: 0.4035 acc_train: 0.9571 loss_val: 0.7216 acc_val: 0.8000 time: 0.0044s\n",
            "Epoch: 0124 loss_train: 0.3863 acc_train: 0.9429 loss_val: 0.7176 acc_val: 0.8000 time: 0.0042s\n",
            "Epoch: 0125 loss_train: 0.3710 acc_train: 0.9571 loss_val: 0.7125 acc_val: 0.8067 time: 0.0042s\n",
            "Epoch: 0126 loss_train: 0.3798 acc_train: 0.9714 loss_val: 0.7078 acc_val: 0.8100 time: 0.0043s\n",
            "Epoch: 0127 loss_train: 0.3810 acc_train: 0.9286 loss_val: 0.7045 acc_val: 0.8167 time: 0.0045s\n",
            "Epoch: 0128 loss_train: 0.3829 acc_train: 0.9357 loss_val: 0.7024 acc_val: 0.8167 time: 0.0042s\n",
            "Epoch: 0129 loss_train: 0.3852 acc_train: 0.9286 loss_val: 0.7019 acc_val: 0.8167 time: 0.0042s\n",
            "Epoch: 0130 loss_train: 0.3689 acc_train: 0.9500 loss_val: 0.7022 acc_val: 0.8200 time: 0.0043s\n",
            "Epoch: 0131 loss_train: 0.3457 acc_train: 0.9571 loss_val: 0.7044 acc_val: 0.8133 time: 0.0042s\n",
            "Epoch: 0132 loss_train: 0.3692 acc_train: 0.9429 loss_val: 0.7081 acc_val: 0.8100 time: 0.0041s\n",
            "Epoch: 0133 loss_train: 0.3403 acc_train: 0.9571 loss_val: 0.7127 acc_val: 0.8100 time: 0.0043s\n",
            "Epoch: 0134 loss_train: 0.3698 acc_train: 0.9429 loss_val: 0.7142 acc_val: 0.8033 time: 0.0043s\n",
            "Epoch: 0135 loss_train: 0.3419 acc_train: 0.9786 loss_val: 0.7124 acc_val: 0.8033 time: 0.0042s\n",
            "Epoch: 0136 loss_train: 0.3243 acc_train: 0.9714 loss_val: 0.7082 acc_val: 0.8033 time: 0.0042s\n",
            "Epoch: 0137 loss_train: 0.3437 acc_train: 0.9643 loss_val: 0.7035 acc_val: 0.8033 time: 0.0042s\n",
            "Epoch: 0138 loss_train: 0.3583 acc_train: 0.9357 loss_val: 0.6983 acc_val: 0.8033 time: 0.0042s\n",
            "Epoch: 0139 loss_train: 0.3449 acc_train: 0.9429 loss_val: 0.6949 acc_val: 0.8067 time: 0.0041s\n",
            "Epoch: 0140 loss_train: 0.3517 acc_train: 0.9643 loss_val: 0.6920 acc_val: 0.8067 time: 0.0043s\n",
            "Epoch: 0141 loss_train: 0.3343 acc_train: 0.9714 loss_val: 0.6898 acc_val: 0.8100 time: 0.0042s\n",
            "Epoch: 0142 loss_train: 0.3259 acc_train: 0.9714 loss_val: 0.6891 acc_val: 0.8067 time: 0.0042s\n",
            "Epoch: 0143 loss_train: 0.3724 acc_train: 0.9500 loss_val: 0.6905 acc_val: 0.8100 time: 0.0042s\n",
            "Epoch: 0144 loss_train: 0.3717 acc_train: 0.9500 loss_val: 0.6951 acc_val: 0.8067 time: 0.0045s\n",
            "Epoch: 0145 loss_train: 0.3354 acc_train: 0.9643 loss_val: 0.6975 acc_val: 0.8100 time: 0.0053s\n",
            "Epoch: 0146 loss_train: 0.3117 acc_train: 0.9786 loss_val: 0.6986 acc_val: 0.8100 time: 0.0043s\n",
            "Epoch: 0147 loss_train: 0.3141 acc_train: 0.9714 loss_val: 0.6982 acc_val: 0.8067 time: 0.0042s\n",
            "Epoch: 0148 loss_train: 0.3192 acc_train: 0.9786 loss_val: 0.6962 acc_val: 0.8100 time: 0.0043s\n",
            "Epoch: 0149 loss_train: 0.3294 acc_train: 0.9857 loss_val: 0.6893 acc_val: 0.8100 time: 0.0041s\n",
            "Epoch: 0150 loss_train: 0.3298 acc_train: 0.9643 loss_val: 0.6824 acc_val: 0.8067 time: 0.0041s\n",
            "Epoch: 0151 loss_train: 0.3593 acc_train: 0.9786 loss_val: 0.6795 acc_val: 0.8133 time: 0.0053s\n",
            "Epoch: 0152 loss_train: 0.3276 acc_train: 0.9643 loss_val: 0.6787 acc_val: 0.8167 time: 0.0056s\n",
            "Epoch: 0153 loss_train: 0.3353 acc_train: 0.9571 loss_val: 0.6776 acc_val: 0.8167 time: 0.0059s\n",
            "Epoch: 0154 loss_train: 0.3230 acc_train: 0.9571 loss_val: 0.6778 acc_val: 0.8133 time: 0.0044s\n",
            "Epoch: 0155 loss_train: 0.3180 acc_train: 0.9643 loss_val: 0.6797 acc_val: 0.8067 time: 0.0042s\n",
            "Epoch: 0156 loss_train: 0.3450 acc_train: 0.9643 loss_val: 0.6814 acc_val: 0.8033 time: 0.0059s\n",
            "Epoch: 0157 loss_train: 0.3038 acc_train: 0.9714 loss_val: 0.6831 acc_val: 0.8000 time: 0.0055s\n",
            "Epoch: 0158 loss_train: 0.3061 acc_train: 0.9643 loss_val: 0.6843 acc_val: 0.8067 time: 0.0042s\n",
            "Epoch: 0159 loss_train: 0.3097 acc_train: 0.9429 loss_val: 0.6817 acc_val: 0.8067 time: 0.0042s\n",
            "Epoch: 0160 loss_train: 0.3152 acc_train: 0.9643 loss_val: 0.6773 acc_val: 0.8067 time: 0.0041s\n",
            "Epoch: 0161 loss_train: 0.3341 acc_train: 0.9643 loss_val: 0.6742 acc_val: 0.8000 time: 0.0041s\n",
            "Epoch: 0162 loss_train: 0.2828 acc_train: 0.9714 loss_val: 0.6727 acc_val: 0.8067 time: 0.0042s\n",
            "Epoch: 0163 loss_train: 0.3151 acc_train: 0.9571 loss_val: 0.6715 acc_val: 0.8067 time: 0.0041s\n",
            "Epoch: 0164 loss_train: 0.3377 acc_train: 0.9500 loss_val: 0.6714 acc_val: 0.8033 time: 0.0041s\n",
            "Epoch: 0165 loss_train: 0.3220 acc_train: 0.9500 loss_val: 0.6721 acc_val: 0.8100 time: 0.0044s\n",
            "Epoch: 0166 loss_train: 0.3040 acc_train: 0.9643 loss_val: 0.6749 acc_val: 0.8100 time: 0.0042s\n",
            "Epoch: 0167 loss_train: 0.3094 acc_train: 0.9571 loss_val: 0.6769 acc_val: 0.8100 time: 0.0041s\n",
            "Epoch: 0168 loss_train: 0.2903 acc_train: 0.9714 loss_val: 0.6785 acc_val: 0.8067 time: 0.0042s\n",
            "Epoch: 0169 loss_train: 0.2827 acc_train: 0.9786 loss_val: 0.6754 acc_val: 0.8067 time: 0.0042s\n",
            "Epoch: 0170 loss_train: 0.3005 acc_train: 0.9643 loss_val: 0.6702 acc_val: 0.8100 time: 0.0043s\n",
            "Epoch: 0171 loss_train: 0.3169 acc_train: 0.9571 loss_val: 0.6658 acc_val: 0.8033 time: 0.0042s\n",
            "Epoch: 0172 loss_train: 0.2901 acc_train: 0.9571 loss_val: 0.6654 acc_val: 0.8000 time: 0.0045s\n",
            "Epoch: 0173 loss_train: 0.2765 acc_train: 0.9786 loss_val: 0.6676 acc_val: 0.8000 time: 0.0043s\n",
            "Epoch: 0174 loss_train: 0.3193 acc_train: 0.9571 loss_val: 0.6684 acc_val: 0.8000 time: 0.0041s\n",
            "Epoch: 0175 loss_train: 0.3017 acc_train: 0.9571 loss_val: 0.6697 acc_val: 0.7933 time: 0.0041s\n",
            "Epoch: 0176 loss_train: 0.2877 acc_train: 0.9429 loss_val: 0.6706 acc_val: 0.7967 time: 0.0041s\n",
            "Epoch: 0177 loss_train: 0.3299 acc_train: 0.9357 loss_val: 0.6700 acc_val: 0.8000 time: 0.0042s\n",
            "Epoch: 0178 loss_train: 0.2678 acc_train: 0.9857 loss_val: 0.6676 acc_val: 0.8033 time: 0.0042s\n",
            "Epoch: 0179 loss_train: 0.2846 acc_train: 0.9857 loss_val: 0.6657 acc_val: 0.8100 time: 0.0043s\n",
            "Epoch: 0180 loss_train: 0.2896 acc_train: 0.9643 loss_val: 0.6646 acc_val: 0.8100 time: 0.0042s\n",
            "Epoch: 0181 loss_train: 0.2965 acc_train: 0.9714 loss_val: 0.6638 acc_val: 0.8133 time: 0.0043s\n",
            "Epoch: 0182 loss_train: 0.2787 acc_train: 0.9643 loss_val: 0.6634 acc_val: 0.8100 time: 0.0042s\n",
            "Epoch: 0183 loss_train: 0.2725 acc_train: 0.9714 loss_val: 0.6639 acc_val: 0.8100 time: 0.0042s\n",
            "Epoch: 0184 loss_train: 0.2817 acc_train: 0.9714 loss_val: 0.6642 acc_val: 0.8133 time: 0.0042s\n",
            "Epoch: 0185 loss_train: 0.2558 acc_train: 0.9643 loss_val: 0.6651 acc_val: 0.8133 time: 0.0042s\n",
            "Epoch: 0186 loss_train: 0.3008 acc_train: 0.9786 loss_val: 0.6646 acc_val: 0.8100 time: 0.0041s\n",
            "Epoch: 0187 loss_train: 0.2831 acc_train: 0.9786 loss_val: 0.6631 acc_val: 0.8133 time: 0.0041s\n",
            "Epoch: 0188 loss_train: 0.2786 acc_train: 0.9786 loss_val: 0.6629 acc_val: 0.8100 time: 0.0042s\n",
            "Epoch: 0189 loss_train: 0.2740 acc_train: 0.9786 loss_val: 0.6640 acc_val: 0.8067 time: 0.0041s\n",
            "Epoch: 0190 loss_train: 0.2920 acc_train: 0.9857 loss_val: 0.6654 acc_val: 0.8033 time: 0.0041s\n",
            "Epoch: 0191 loss_train: 0.2841 acc_train: 0.9786 loss_val: 0.6648 acc_val: 0.8033 time: 0.0043s\n",
            "Epoch: 0192 loss_train: 0.2969 acc_train: 0.9786 loss_val: 0.6656 acc_val: 0.8033 time: 0.0041s\n",
            "Epoch: 0193 loss_train: 0.3003 acc_train: 0.9714 loss_val: 0.6656 acc_val: 0.8067 time: 0.0041s\n",
            "Epoch: 0194 loss_train: 0.2700 acc_train: 0.9714 loss_val: 0.6646 acc_val: 0.8100 time: 0.0042s\n",
            "Epoch: 0195 loss_train: 0.2776 acc_train: 0.9643 loss_val: 0.6646 acc_val: 0.8133 time: 0.0041s\n",
            "Epoch: 0196 loss_train: 0.2796 acc_train: 0.9786 loss_val: 0.6632 acc_val: 0.8100 time: 0.0055s\n",
            "Epoch: 0197 loss_train: 0.2754 acc_train: 0.9786 loss_val: 0.6587 acc_val: 0.8133 time: 0.0048s\n",
            "Epoch: 0198 loss_train: 0.2697 acc_train: 0.9786 loss_val: 0.6535 acc_val: 0.8133 time: 0.0044s\n",
            "Epoch: 0199 loss_train: 0.2657 acc_train: 0.9643 loss_val: 0.6504 acc_val: 0.8100 time: 0.0043s\n",
            "Epoch: 0200 loss_train: 0.2527 acc_train: 0.9929 loss_val: 0.6497 acc_val: 0.8133 time: 0.0041s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 0.9326s\n",
            "Test set results: loss= 0.6434 accuracy= 0.8340\n",
            "------------------------------------------ hidden 32 ^ ------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Your code goes here\n",
        "\n",
        "! python train.py --lr 0.1\n",
        "print(\"------------------------------------------ learning rate 0.1 ^ ------------------------------------------\")\n",
        "! python train.py --lr 0.01\n",
        "print(\"------------------------------------------ learning rate 0.01 ^ ------------------------------------------\")\n",
        "! python train.py --lr 0.001\n",
        "print(\"------------------------------------------ learning rate 0.001 ^ ------------------------------------------\")\n",
        "\n",
        "! python train.py --epochs 100\n",
        "print(\"------------------------------------------ epoch 100 ^ ------------------------------------------\")\n",
        "! python train.py --epochs 300\n",
        "print(\"------------------------------------------ epoch 300 ^ ------------------------------------------\")\n",
        "\n",
        "! python train.py --hidden 8\n",
        "print(\"------------------------------------------ hidden 8 ^ ------------------------------------------\")\n",
        "! python train.py --hidden 32\n",
        "print(\"------------------------------------------ hidden 32 ^ ------------------------------------------\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nchjcNMP16r"
      },
      "source": [
        "# 2. (20 pts) Answer the following questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0yGpTTaP16s"
      },
      "source": [
        "## Q1 (To finish, 10 pts): Read [utils file](https://github.com/tkipf/pygcn/blob/master/pygcn/utils.py) and explain which function in this file computes the adjacency matrix?\n",
        "\n",
        "Answer: \n",
        "\n",
        "Building adjacency happened in load_data()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yw3ZcT0P16s"
      },
      "source": [
        "## Q2 (To finish, 10 pts): Read [layer file](https://github.com/tkipf/pygcn/blob/master/pygcn/layers.py) and [model file](https://github.com/tkipf/pygcn/blob/master/pygcn/models.py). Draw a computation graph or use plain language to describe the structure of this GCN. Particularly, how many graph convolutional layers (i.e., how many hops) are in this GCN structure?\n",
        "    \n",
        "Answer: \n",
        "\n",
        "The model has 2 graph convolutional layers. Input goes through the first GCN and relu, then dropout 50%(by default) of the matrix, and the new matrix goes into the second GCN.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJKzHQb-P16s"
      },
      "outputs": [],
      "source": [
        "# convert this file \"HW5_GNN_to_Canvas.ipynb\" to \"HW5_GNN_to_Canvas.html\"\n",
        "!jupyter nbconvert --to html HW5_GNN_to_Canvas.ipynb \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llTE_vq3P16s"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "HW5_GNN_to_Canvas.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
